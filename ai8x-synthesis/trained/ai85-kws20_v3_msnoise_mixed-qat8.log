2024-06-05 23:34:20,367 - Log file for this run: /home/merveeyuboglu/Github/ai8x-training-merve/ai8x-training/logs/v3_original/2024.06.05-233420/2024.06.05-233420.log
2024-06-05 23:34:23,490 - Optimizer Type: <class 'torch.optim.adam.Adam'>
2024-06-05 23:34:23,491 - Optimizer Args: {'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0, 'amsgrad': False}
2024-06-05 23:34:46,020 - Reading compression schedule from: policies/schedule_kws20.yaml
2024-06-05 23:34:46,050 - Dataset sizes:
	training=311705
	validation=34633
	test=11005
2024-06-05 23:34:46,050 - 

2024-06-05 23:34:46,050 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-05 23:34:53,451 - Epoch: [0][  100/ 1218]    Overall Loss 3.042956    Objective Loss 3.042956                                        LR 0.001000    Time 0.073983    
2024-06-05 23:34:58,911 - Epoch: [0][  200/ 1218]    Overall Loss 3.003488    Objective Loss 3.003488                                        LR 0.001000    Time 0.064278    
2024-06-05 23:35:04,110 - Epoch: [0][  300/ 1218]    Overall Loss 2.935542    Objective Loss 2.935542                                        LR 0.001000    Time 0.060172    
2024-06-05 23:35:09,962 - Epoch: [0][  400/ 1218]    Overall Loss 2.859718    Objective Loss 2.859718                                        LR 0.001000    Time 0.059753    
2024-06-05 23:35:15,314 - Epoch: [0][  500/ 1218]    Overall Loss 2.759763    Objective Loss 2.759763                                        LR 0.001000    Time 0.058501    
2024-06-05 23:35:20,943 - Epoch: [0][  600/ 1218]    Overall Loss 2.659985    Objective Loss 2.659985                                        LR 0.001000    Time 0.058129    
2024-06-05 23:35:26,432 - Epoch: [0][  700/ 1218]    Overall Loss 2.573046    Objective Loss 2.573046                                        LR 0.001000    Time 0.057662    
2024-06-05 23:35:31,867 - Epoch: [0][  800/ 1218]    Overall Loss 2.498611    Objective Loss 2.498611                                        LR 0.001000    Time 0.057245    
2024-06-05 23:35:37,315 - Epoch: [0][  900/ 1218]    Overall Loss 2.434340    Objective Loss 2.434340                                        LR 0.001000    Time 0.056935    
2024-06-05 23:35:42,633 - Epoch: [0][ 1000/ 1218]    Overall Loss 2.376071    Objective Loss 2.376071                                        LR 0.001000    Time 0.056557    
2024-06-05 23:35:48,459 - Epoch: [0][ 1100/ 1218]    Overall Loss 2.324137    Objective Loss 2.324137                                        LR 0.001000    Time 0.056710    
2024-06-05 23:35:53,913 - Epoch: [0][ 1200/ 1218]    Overall Loss 2.275560    Objective Loss 2.275560                                        LR 0.001000    Time 0.056527    
2024-06-05 23:35:54,852 - Epoch: [0][ 1218/ 1218]    Overall Loss 2.266748    Objective Loss 2.266748    Top1 42.298289    Top5 78.973105    LR 0.001000    Time 0.056462    
2024-06-05 23:35:55,066 - --- validate (epoch=0)-----------
2024-06-05 23:35:55,066 - 34633 samples (256 per mini-batch)
2024-06-05 23:36:01,771 - Epoch: [0][  100/  136]    Loss 1.684082    Top1 39.074219    Top5 75.679688    
2024-06-05 23:36:03,576 - Epoch: [0][  136/  136]    Loss 1.683202    Top1 39.104322    Top5 75.832299    
2024-06-05 23:36:03,750 - ==> Top1: 39.104    Top5: 75.832    Loss: 1.683

2024-06-05 23:36:03,751 - ==> Confusion:
[[ 561    1    8    9   28    2    5    7   42  146    3    1    4    3   16   38   18    7    7    3   22]
 [   5  512   18    9   29   23    2   27   39    4   73   12   24   22   46    7   39    3  133   23   13]
 [  23    6  351   19   22   16  208  111    1   16   41   12    7    4    3   25   21    1   19   46   18]
 [  11   20   28  389   14   28   28   67   26   26  146    7   35    6   22   14   23   33   59   11   23]
 [  76   37   28    1  533   18    3    8   27   71   32    5    4   11   16   78   65    5   13    5   18]
 [   4  142   22   71   23  216   27   67   14   13   66   67   67   54    8   22   19   22   47   56   16]
 [   7    4  136    7   13    9  590   69    3    2   21   41   12    3    0   35    8    5   15   78   28]
 [   6   15   45   48    4   21   37  433    8   11   59   21   41    3    7    5    7    5  178  107   16]
 [  16   31    0   17    3    5    1    8  579   67    9    3   34    8  121    3   13   13   48    8   15]
 [ 235    6   10   12   32    8    3    6  118  418    2    2    4   18   41   19   18    8   19    1   21]
 [  11   79   47   62   10   19   15   70   41    8  421    6   21    2   12    4   14    6  179   22   15]
 [   1    1    2    5    0    9    4    9    2    0    2  516  142   13    3   77    8  126    6   71   14]
 [   1    6    1    9    1    4    6    7    8    0    4  281  399    6    6   11   22  150   23   41    9]
 [  10   65   12    7   33   88    5   26   15   25    8   84   45  310   60   26   46   15   12   95   14]
 [  29  103    2   31   25    4    1   11  267   80    9    3   18   12  393    1   30   16   48    4   11]
 [  10    2   14    2   15    6   21    3    0    1    1   86   20    0    0  761   24   65    4   14   17]
 [  23   36    3    3   25   12    2    3   14    2    4   35   40   10   14   55  726   17   10   23   15]
 [   7    4    0    6    1    2    8    4   20    3    2  166  129    2    8   76   12  527    5   11   12]
 [   3   47    8   27    4    6    7   66   45    5  112    7   23    0   19    2    7    2  623   27   18]
 [   1   10    5    2    3    5   28   56    3    3    2  114   28    7    0   19   14   11   31  724   22]
 [ 599  630  180  243  360  233  271  465  419  451  258  719  686  277  469  683 1355  460  584 1029 3561]]

2024-06-05 23:36:03,755 - ==> Best [Top1: 39.104   Top5: 75.832   Sparsity:0.00   Params: 169472 on epoch: 0]
2024-06-05 23:36:03,755 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/checkpoint.pth.tar
2024-06-05 23:36:03,764 - 

2024-06-05 23:36:03,764 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-05 23:36:10,749 - Epoch: [1][  100/ 1218]    Overall Loss 1.696817    Objective Loss 1.696817                                        LR 0.001000    Time 0.069826    
2024-06-05 23:36:16,409 - Epoch: [1][  200/ 1218]    Overall Loss 1.677138    Objective Loss 1.677138                                        LR 0.001000    Time 0.063198    
2024-06-05 23:36:21,648 - Epoch: [1][  300/ 1218]    Overall Loss 1.656663    Objective Loss 1.656663                                        LR 0.001000    Time 0.059587    
2024-06-05 23:36:27,454 - Epoch: [1][  400/ 1218]    Overall Loss 1.645921    Objective Loss 1.645921                                        LR 0.001000    Time 0.059199    
2024-06-05 23:36:33,278 - Epoch: [1][  500/ 1218]    Overall Loss 1.630747    Objective Loss 1.630747                                        LR 0.001000    Time 0.059003    
2024-06-05 23:36:38,946 - Epoch: [1][  600/ 1218]    Overall Loss 1.613834    Objective Loss 1.613834                                        LR 0.001000    Time 0.058601    
2024-06-05 23:36:44,296 - Epoch: [1][  700/ 1218]    Overall Loss 1.598885    Objective Loss 1.598885                                        LR 0.001000    Time 0.057869    
2024-06-05 23:36:49,982 - Epoch: [1][  800/ 1218]    Overall Loss 1.585974    Objective Loss 1.585974                                        LR 0.001000    Time 0.057740    
2024-06-05 23:36:55,533 - Epoch: [1][  900/ 1218]    Overall Loss 1.571461    Objective Loss 1.571461                                        LR 0.001000    Time 0.057489    
2024-06-05 23:37:01,028 - Epoch: [1][ 1000/ 1218]    Overall Loss 1.561006    Objective Loss 1.561006                                        LR 0.001000    Time 0.057232    
2024-06-05 23:37:06,805 - Epoch: [1][ 1100/ 1218]    Overall Loss 1.549882    Objective Loss 1.549882                                        LR 0.001000    Time 0.057279    
2024-06-05 23:37:12,420 - Epoch: [1][ 1200/ 1218]    Overall Loss 1.539073    Objective Loss 1.539073                                        LR 0.001000    Time 0.057183    
2024-06-05 23:37:13,400 - Epoch: [1][ 1218/ 1218]    Overall Loss 1.537618    Objective Loss 1.537618    Top1 45.965770    Top5 82.640587    LR 0.001000    Time 0.057142    
2024-06-05 23:37:13,659 - --- validate (epoch=1)-----------
2024-06-05 23:37:13,659 - 34633 samples (256 per mini-batch)
2024-06-05 23:37:20,363 - Epoch: [1][  100/  136]    Loss 1.370697    Top1 47.078125    Top5 80.101562    
2024-06-05 23:37:22,343 - Epoch: [1][  136/  136]    Loss 1.365546    Top1 47.137124    Top5 80.247163    
2024-06-05 23:37:22,581 - ==> Top1: 47.137    Top5: 80.247    Loss: 1.366

2024-06-05 23:37:22,582 - ==> Confusion:
[[ 574    2   11    4   28    3    1    3   35  204    2    0    2   11   13    6    4    6    9    0   13]
 [   3  533    8    6   46   43    7   16   46    6   99    4    8   22   73    2   11    2  100   20    8]
 [  21    8  399   10   33   19  182  105    7   27   52    5    1   29    3    6    3    5   21   24   10]
 [   9    3   30  559   17   35   21   65   23   14   76    0    9    3   52    4    9   12   60    4   11]
 [  51   30   28    4  705   15    1    2   13   44   11    2    1   26   49    8   22    1   25    6   10]
 [   2   67   31   31   26  400   17   95   17   20   44   15   28  123   30    1    7    5   49   24   11]
 [   6    3   89    8   10   20  759   39    2    5   30    8    5    3    3   17    1    8   15   41   14]
 [   5    6   43   11    9   26   22  587   16   18   35    8   18   19    5    2    1    4  191   43    8]
 [  16    5    0    7    9    7    0    3  692   69   14    2    9   23   74    1    0    3   60    2    6]
 [ 187    1    8    2   20    1    1   11  109  544    1    1    2   51   22    1    4    4   18    3   10]
 [  10   29   41   37   22   35   18   53   35    5  564    0    9    6   15    0    2    0  169    4   10]
 [   2    5    1    1    2   39    4   18    7    4    2  447  194   91    4   37    7   74   20   43    9]
 [   3    3    0   19    2   16    5   13   12    2    7   98  548   36   11    9    7  154   29   16    5]
 [  11    9    6    2   22   32    0   23   51   37    4    8   21  680   18    3    8    7   17   37    5]
 [  15   19    0   38   46    5    0    5  107   46    5    0    5   19  707    2    4    7   53    2   13]
 [  19    2   16    1   45   14   24    0    1    5    3   76   15   22    1  702   29   59   10   10   12]
 [  27   23    4   10   35   27    5    5   18    4   12    9   30   33   16    8  748    6   16   12   24]
 [   7    1    1   11    3    6    1    4   13    3    1   40  142   31   13   14    6  677   11    4   16]
 [   0   15    8   31    1   13    7   77   38    5   48    1   10    5   38    0    1    0  741   11    8]
 [   3    7    8    1    4   18   35   52    8    0    0   43   29   42    2   11    9    5   26  774   11]
 [ 686  378  206  290  593  433  198  482  497  502  290  272  570  853  797  264  580  371  821  864 3985]]

2024-06-05 23:37:22,584 - ==> Best [Top1: 47.137   Top5: 80.247   Sparsity:0.00   Params: 169472 on epoch: 1]
2024-06-05 23:37:22,584 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/checkpoint.pth.tar
2024-06-05 23:37:22,600 - 

2024-06-05 23:37:22,600 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-05 23:37:29,979 - Epoch: [2][  100/ 1218]    Overall Loss 1.383134    Objective Loss 1.383134                                        LR 0.001000    Time 0.073758    
2024-06-05 23:37:35,651 - Epoch: [2][  200/ 1218]    Overall Loss 1.382185    Objective Loss 1.382185                                        LR 0.001000    Time 0.065228    
2024-06-05 23:37:41,403 - Epoch: [2][  300/ 1218]    Overall Loss 1.377378    Objective Loss 1.377378                                        LR 0.001000    Time 0.062649    
2024-06-05 23:37:46,719 - Epoch: [2][  400/ 1218]    Overall Loss 1.373865    Objective Loss 1.373865                                        LR 0.001000    Time 0.060270    
2024-06-05 23:37:52,385 - Epoch: [2][  500/ 1218]    Overall Loss 1.370012    Objective Loss 1.370012                                        LR 0.001000    Time 0.059543    
2024-06-05 23:37:57,713 - Epoch: [2][  600/ 1218]    Overall Loss 1.362420    Objective Loss 1.362420                                        LR 0.001000    Time 0.058494    
2024-06-05 23:38:03,290 - Epoch: [2][  700/ 1218]    Overall Loss 1.355820    Objective Loss 1.355820                                        LR 0.001000    Time 0.058101    
2024-06-05 23:38:09,089 - Epoch: [2][  800/ 1218]    Overall Loss 1.348000    Objective Loss 1.348000                                        LR 0.001000    Time 0.058084    
2024-06-05 23:38:14,980 - Epoch: [2][  900/ 1218]    Overall Loss 1.341342    Objective Loss 1.341342                                        LR 0.001000    Time 0.058174    
2024-06-05 23:38:20,590 - Epoch: [2][ 1000/ 1218]    Overall Loss 1.335000    Objective Loss 1.335000                                        LR 0.001000    Time 0.057964    
2024-06-05 23:38:26,271 - Epoch: [2][ 1100/ 1218]    Overall Loss 1.329896    Objective Loss 1.329896                                        LR 0.001000    Time 0.057857    
2024-06-05 23:38:32,125 - Epoch: [2][ 1200/ 1218]    Overall Loss 1.323035    Objective Loss 1.323035                                        LR 0.001000    Time 0.057911    
2024-06-05 23:38:33,175 - Epoch: [2][ 1218/ 1218]    Overall Loss 1.321761    Objective Loss 1.321761    Top1 59.657702    Top5 87.775061    LR 0.001000    Time 0.057916    
2024-06-05 23:38:33,402 - --- validate (epoch=2)-----------
2024-06-05 23:38:33,403 - 34633 samples (256 per mini-batch)
2024-06-05 23:38:39,916 - Epoch: [2][  100/  136]    Loss 1.238490    Top1 52.324219    Top5 87.628906    
2024-06-05 23:38:42,050 - Epoch: [2][  136/  136]    Loss 1.242217    Top1 52.089048    Top5 87.572546    
2024-06-05 23:38:42,303 - ==> Top1: 52.089    Top5: 87.573    Loss: 1.242

2024-06-05 23:38:42,304 - ==> Confusion:
[[ 642    1    7    4   12    1    1    5   20  172    4    3    1   12    8    2    3    2    8   10   13]
 [   4  730    1    2   37   36    7   15   34    5   44    5    5   27   31    0    8    0   45   14   13]
 [  21    8  528   19   27   17   70   99    1   28   44    7    0   21    2    7    3    0   16   28   24]
 [  11   11   23  658   11   29   12   33   12   13   68    2    4   13   37    3    5    9   33    7   22]
 [  70   49   17    3  683   25    4    1   10   61    5    2    0   23   40   16   12    0   11    2   20]
 [   4  114   18   16   24  463    6   58   10   18   15   17    8  167   10    8    3    5   29   28   22]
 [   3   15  122    9    9   22  708   65    2    4   17   11    2   10    3   15    1    4   10   33   21]
 [   3   26   34    7    7   46   15  653    5   18   27   10    3   22    7    0    1    2  133   45   13]
 [  19   12    0    1    5    2    1   10  711   94   11    0    6   49   32    2    0    0   25    6   16]
 [ 210    5    3    2    5    4    0    5   77  592    3    2    0   43   18    1    2    2   10    5   12]
 [  11   65   32   62   14   35    2   43   38    6  609    1    3   16   11    2    3    1   95    5   10]
 [   1    1    3    2    0   17    4   19    4    3    3  646   56  109    0   23    6   16    9   68   21]
 [   3    9    1   19    0   16    4   16   16    1    3  212  413   66   11    7    9   98   30   42   19]
 [  13   17    3    1    8   17    3   19   28   33    2   15    3  768   11    5    4    3    7   29   12]
 [  32   22    1   38   26    3    1   12  106   57    4    0    3   17  690    0    2    8   48    7   21]
 [  12    5    8    3   22    8   29    2    2    7    1   88    1   21    0  741   30   29    5   21   31]
 [  14   44    4    5   32   25    8    3   17    3    3   12    9   37   12   13  748    4   11   22   46]
 [  11    3    0   14    0    2    4    4    6    7    2  128   89   29    5   35    1  620    8   17   20]
 [   5   28    8   31    2   12    1  111   25    7   43    3    1    9   50    0    2    1  692   16   11]
 [   0   18    9    2    0   15    9   70    0    3    3   54    4   53    2    6   14    3    9  800   14]
 [ 678  637  202  256  459  366  155  470  342  557  227  425  255 1012  597  255  406  184  548  956 4945]]

2024-06-05 23:38:42,308 - ==> Best [Top1: 52.089   Top5: 87.573   Sparsity:0.00   Params: 169472 on epoch: 2]
2024-06-05 23:38:42,308 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/checkpoint.pth.tar
2024-06-05 23:38:42,327 - 

2024-06-05 23:38:42,328 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-05 23:38:49,403 - Epoch: [3][  100/ 1218]    Overall Loss 1.232579    Objective Loss 1.232579                                        LR 0.001000    Time 0.070725    
2024-06-05 23:38:55,337 - Epoch: [3][  200/ 1218]    Overall Loss 1.231045    Objective Loss 1.231045                                        LR 0.001000    Time 0.065016    
2024-06-05 23:39:01,483 - Epoch: [3][  300/ 1218]    Overall Loss 1.233501    Objective Loss 1.233501                                        LR 0.001000    Time 0.063823    
2024-06-05 23:39:07,114 - Epoch: [3][  400/ 1218]    Overall Loss 1.225784    Objective Loss 1.225784                                        LR 0.001000    Time 0.061939    
2024-06-05 23:39:12,647 - Epoch: [3][  500/ 1218]    Overall Loss 1.217897    Objective Loss 1.217897                                        LR 0.001000    Time 0.060613    
2024-06-05 23:39:18,553 - Epoch: [3][  600/ 1218]    Overall Loss 1.217144    Objective Loss 1.217144                                        LR 0.001000    Time 0.060350    
2024-06-05 23:39:23,955 - Epoch: [3][  700/ 1218]    Overall Loss 1.212085    Objective Loss 1.212085                                        LR 0.001000    Time 0.059441    
2024-06-05 23:39:29,257 - Epoch: [3][  800/ 1218]    Overall Loss 1.209051    Objective Loss 1.209051                                        LR 0.001000    Time 0.058636    
2024-06-05 23:39:34,746 - Epoch: [3][  900/ 1218]    Overall Loss 1.202467    Objective Loss 1.202467                                        LR 0.001000    Time 0.058217    
2024-06-05 23:39:40,650 - Epoch: [3][ 1000/ 1218]    Overall Loss 1.201948    Objective Loss 1.201948                                        LR 0.001000    Time 0.058297    
2024-06-05 23:39:46,004 - Epoch: [3][ 1100/ 1218]    Overall Loss 1.198189    Objective Loss 1.198189                                        LR 0.001000    Time 0.057862    
2024-06-05 23:39:51,564 - Epoch: [3][ 1200/ 1218]    Overall Loss 1.194462    Objective Loss 1.194462                                        LR 0.001000    Time 0.057672    
2024-06-05 23:39:52,673 - Epoch: [3][ 1218/ 1218]    Overall Loss 1.193953    Objective Loss 1.193953    Top1 55.745721    Top5 87.530562    LR 0.001000    Time 0.057729    
2024-06-05 23:39:52,921 - --- validate (epoch=3)-----------
2024-06-05 23:39:52,921 - 34633 samples (256 per mini-batch)
2024-06-05 23:39:59,543 - Epoch: [3][  100/  136]    Loss 1.075630    Top1 53.878906    Top5 87.410156    
2024-06-05 23:40:01,819 - Epoch: [3][  136/  136]    Loss 1.082352    Top1 53.824387    Top5 87.335778    
2024-06-05 23:40:02,081 - ==> Top1: 53.824    Top5: 87.336    Loss: 1.082

2024-06-05 23:40:02,082 - ==> Confusion:
[[ 659    3   21    2   26    2    0    3   10  137    0    1    2    5   11   10   11    3    6    3   16]
 [   0  744    5    4   35   40   22   10   22    3   24    3    1   10   35    7   22    4   44   21    7]
 [  13    6  632   27   28   12   96   25    3    8   14    9    1   10    5   18   12    2   27   13    9]
 [   9    5   33  729   11   19   15    5   12    2   38    1    9    3   39   10   11   19   37    2    7]
 [  38   36   24    1  764   35    3    1    4   13    9    1    1    8   28   25   28    1   15    4   15]
 [  12  104   16   11   26  561   35   33    9    8   12   29   17   60   10   11   12    8   27   28   14]
 [   6   11   75    5    4    6  869   14    0    1    3    6    6    0    1   26    2    8    8   21   14]
 [   4   22   71   23    9   55   34  589    8    5   16   12    7   11    5    3    3    4  143   40   13]
 [  22   12    2    5    5    2    1    4  683   77   15    3    8   27   73    6   10    3   29    5   10]
 [ 207    0    3    3   37    7    0    6   61  585    2    2    2   18   34    2    4    8    8    1   11]
 [  11   40   44   81   22   26   19   16   25    2  643    3    3    8   13    1    8    1   84    7    7]
 [   2    1    3    1    0   11   11   11    5    2    0  633   74   44    1   64   15   65    9   49   10]
 [   3    4    1   18    0    3    4    4    7    0    2  143  526    9    9   21   24  165   16   26   10]
 [  13   12    6    5   23   60    1    7   18   32    3   25   10  660   23   11   20   14    4   41   13]
 [  24   24    4   42   37    2    0    1   43   17    3    1    8   10  808    1    7    4   48    0   14]
 [   9    4    8    1   10    1   20    1    0    1    0   40    6    1    1  858   32   44    1   14   14]
 [  12   24    8    2   18   17    6    2    9    0    2   13   10   10    3   29  872    4    7    7   17]
 [   6    2    0   12    2    1    1    1    1    0    0   31   76    7    8   37   10  783    3   10   14]
 [   3   22   16   47   10   15    7   33   16    1   22    3   12    1   38    4    3    2  785    8   10]
 [   4   15   10    2    1   15   48   19    4    0    2   51    9   13    4   17   13   15   12  824   10]
 [ 527  541  378  361  584  441  294  241  228  301  208  313  478  482  642  525 1157  359  685  753 4434]]

2024-06-05 23:40:02,084 - ==> Best [Top1: 53.824   Top5: 87.336   Sparsity:0.00   Params: 169472 on epoch: 3]
2024-06-05 23:40:02,084 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/checkpoint.pth.tar
2024-06-05 23:40:02,099 - 

2024-06-05 23:40:02,100 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-05 23:40:09,524 - Epoch: [4][  100/ 1218]    Overall Loss 1.142157    Objective Loss 1.142157                                        LR 0.001000    Time 0.074219    
2024-06-05 23:40:14,991 - Epoch: [4][  200/ 1218]    Overall Loss 1.139217    Objective Loss 1.139217                                        LR 0.001000    Time 0.064429    
2024-06-05 23:40:20,478 - Epoch: [4][  300/ 1218]    Overall Loss 1.129294    Objective Loss 1.129294                                        LR 0.001000    Time 0.061235    
2024-06-05 23:40:26,134 - Epoch: [4][  400/ 1218]    Overall Loss 1.129585    Objective Loss 1.129585                                        LR 0.001000    Time 0.060059    
2024-06-05 23:40:31,604 - Epoch: [4][  500/ 1218]    Overall Loss 1.127242    Objective Loss 1.127242                                        LR 0.001000    Time 0.058983    
2024-06-05 23:40:37,423 - Epoch: [4][  600/ 1218]    Overall Loss 1.121039    Objective Loss 1.121039                                        LR 0.001000    Time 0.058847    
2024-06-05 23:40:43,038 - Epoch: [4][  700/ 1218]    Overall Loss 1.120112    Objective Loss 1.120112                                        LR 0.001000    Time 0.058457    
2024-06-05 23:40:48,837 - Epoch: [4][  800/ 1218]    Overall Loss 1.116257    Objective Loss 1.116257                                        LR 0.001000    Time 0.058396    
2024-06-05 23:40:54,476 - Epoch: [4][  900/ 1218]    Overall Loss 1.111364    Objective Loss 1.111364                                        LR 0.001000    Time 0.058171    
2024-06-05 23:41:00,174 - Epoch: [4][ 1000/ 1218]    Overall Loss 1.105991    Objective Loss 1.105991                                        LR 0.001000    Time 0.058048    
2024-06-05 23:41:05,599 - Epoch: [4][ 1100/ 1218]    Overall Loss 1.101643    Objective Loss 1.101643                                        LR 0.001000    Time 0.057701    
2024-06-05 23:41:11,091 - Epoch: [4][ 1200/ 1218]    Overall Loss 1.098488    Objective Loss 1.098488                                        LR 0.001000    Time 0.057467    
2024-06-05 23:41:12,024 - Epoch: [4][ 1218/ 1218]    Overall Loss 1.098088    Objective Loss 1.098088    Top1 60.146699    Top5 92.420538    LR 0.001000    Time 0.057383    
2024-06-05 23:41:12,235 - --- validate (epoch=4)-----------
2024-06-05 23:41:12,235 - 34633 samples (256 per mini-batch)
2024-06-05 23:41:18,832 - Epoch: [4][  100/  136]    Loss 0.981757    Top1 57.820312    Top5 91.203125    
2024-06-05 23:41:20,885 - Epoch: [4][  136/  136]    Loss 0.988436    Top1 57.751278    Top5 91.124072    
2024-06-05 23:41:21,088 - ==> Top1: 57.751    Top5: 91.124    Loss: 0.988

2024-06-05 23:41:21,090 - ==> Confusion:
[[ 602    6    4    1   14    1    0    6   37  185    0    2    2    8   16    9    8    6    5    3   16]
 [   2  836    5    2   16   26    4   20   16    1   18    5    6    8   18    2   13    3   30   18   14]
 [  17   10  578   27   25   17   70   58    2    7   40    9    0   15    9   17   12    3   15   14   25]
 [   5    6   24  684    3   25    6   12    5    3   74    2   16    8   50    7   11   16   41    5   13]
 [  37   84    5    1  715   29    0    7    7   20    7    4    2   15   41   26   25    1   10    2   16]
 [   3  156    2    7   12  596    9   54    7    8   10   26    9   60   10    6    6    4    9   30   19]
 [   5   30   51    4    6   14  825   18    1    0   11    7    7    3    2   34    6    7    4   38   13]
 [   2   37   12    8    4   57   12  756    9    3   14   18    7   10    6    4    4    3   64   32   15]
 [  15   26    0    0    2    4    0    3  798   26   11    0   18   28   35    2    1    1   16    3   13]
 [  97    4    1    0    9    5    0    7  156  616    1    3    1   40   30    1    3    4    2    1   20]
 [   5   52    5   22   10   20    2   20   40    6  778    3    3   12   13    0    3    0   52    8   10]
 [   5    8    2    0    1   22    2   12    4    0    0  725   70   35    1   32    3   32    4   36   17]
 [   0   14    1    9    1   15    1   10    6    0    0  182  581   14    5   10   11  102   10   12   11]
 [   5   13    1    3    4   39    2   11   44   18    5   19   12  748   20    7    3    4    2   26   15]
 [  17   30    0   12   22    2    0    0   69   12    5    1   12    6  834    2    6    8   45    2   13]
 [   7    9    3    0    7    4   10    0    0    2    0   47    8    7    0  892   23   30    2    5   10]
 [  10   41    4    0   15   11    1    5   10    2    5   21   12   17   11   25  826    4    8   14   30]
 [   3    3    0    3    0    1    0    2    8    0    1   59   87   10    8   27    0  769    3   11   10]
 [   3   42    4   23    2   10    3   69   23    3   14    4   12    4   31    1    1    5  775    6   23]
 [   1   18    0    0    1   15   16   44    3    1    2   46   15   27    3   14   11    6    5  846   14]
 [ 386 1042  161  162  326  423  121  375  403  222  224  367  540  622  601  474  737  276  414  835 5221]]

2024-06-05 23:41:21,093 - ==> Best [Top1: 57.751   Top5: 91.124   Sparsity:0.00   Params: 169472 on epoch: 4]
2024-06-05 23:41:21,093 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/checkpoint.pth.tar
2024-06-05 23:41:21,110 - 

2024-06-05 23:41:21,110 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-05 23:41:28,632 - Epoch: [5][  100/ 1218]    Overall Loss 1.029300    Objective Loss 1.029300                                        LR 0.001000    Time 0.075191    
2024-06-05 23:41:34,482 - Epoch: [5][  200/ 1218]    Overall Loss 1.035220    Objective Loss 1.035220                                        LR 0.001000    Time 0.066834    
2024-06-05 23:41:40,263 - Epoch: [5][  300/ 1218]    Overall Loss 1.031261    Objective Loss 1.031261                                        LR 0.001000    Time 0.063816    
2024-06-05 23:41:45,604 - Epoch: [5][  400/ 1218]    Overall Loss 1.027191    Objective Loss 1.027191                                        LR 0.001000    Time 0.061209    
2024-06-05 23:41:51,695 - Epoch: [5][  500/ 1218]    Overall Loss 1.026403    Objective Loss 1.026403                                        LR 0.001000    Time 0.061144    
2024-06-05 23:41:57,109 - Epoch: [5][  600/ 1218]    Overall Loss 1.025722    Objective Loss 1.025722                                        LR 0.001000    Time 0.059972    
2024-06-05 23:42:02,521 - Epoch: [5][  700/ 1218]    Overall Loss 1.022213    Objective Loss 1.022213                                        LR 0.001000    Time 0.059133    
2024-06-05 23:42:08,228 - Epoch: [5][  800/ 1218]    Overall Loss 1.021978    Objective Loss 1.021978                                        LR 0.001000    Time 0.058872    
2024-06-05 23:42:14,245 - Epoch: [5][  900/ 1218]    Overall Loss 1.020479    Objective Loss 1.020479                                        LR 0.001000    Time 0.059013    
2024-06-05 23:42:19,942 - Epoch: [5][ 1000/ 1218]    Overall Loss 1.016685    Objective Loss 1.016685                                        LR 0.001000    Time 0.058807    
2024-06-05 23:42:25,598 - Epoch: [5][ 1100/ 1218]    Overall Loss 1.013171    Objective Loss 1.013171                                        LR 0.001000    Time 0.058600    
2024-06-05 23:42:31,546 - Epoch: [5][ 1200/ 1218]    Overall Loss 1.010622    Objective Loss 1.010622                                        LR 0.001000    Time 0.058671    
2024-06-05 23:42:32,551 - Epoch: [5][ 1218/ 1218]    Overall Loss 1.010524    Objective Loss 1.010524    Top1 58.190709    Top5 91.931540    LR 0.001000    Time 0.058629    
2024-06-05 23:42:32,799 - --- validate (epoch=5)-----------
2024-06-05 23:42:32,799 - 34633 samples (256 per mini-batch)
2024-06-05 23:42:39,599 - Epoch: [5][  100/  136]    Loss 0.918100    Top1 61.320313    Top5 92.312500    
2024-06-05 23:42:41,613 - Epoch: [5][  136/  136]    Loss 0.913677    Top1 61.568446    Top5 92.348338    
2024-06-05 23:42:41,850 - ==> Top1: 61.568    Top5: 92.348    Loss: 0.914

2024-06-05 23:42:41,851 - ==> Confusion:
[[ 709    3    7    4   32    7    3    7   13   85    0    1    0    5    9    7    4    3    4    0   28]
 [   3  823    3    2   36   44    6   26   11    1    4    3    6    3   13    3   23    2   23   10   18]
 [  27    5  698   17   23   15   43   34    0    4   15    7    1    9    0    9   12    0    5   10   36]
 [   9    7   38  728    8   25    6   10    5    1   37    4    9    3   43    7   13    9   32    4   18]
 [  38   31    9    1  821   30    1    7   11   14    3    1    3    4   16   18   25    1    4    1   15]
 [   7   91    8    9   13  691   10   58   10    5    6   15   10   25   11    9   18    5    6   19   17]
 [   3    8   75    4    9   19  840   15    2    0    1    6    4    4    1   29    8    7    3   24   24]
 [   8   37   29    7    8   75    7  740    8    4    5   11    7    3    2    1    4    4   66   28   23]
 [  19   14    2    2    6    6    0    5  762   66    9    4   13   14   37    1    6    2   15    3   16]
 [ 154    4    1    2   21   11    0    5   65  672    0    1    0   22   18    2    1    3    5    0   14]
 [   9   37   19   23   13   17   10   19   34    5  774    2    5   11    7    2    4    0   52    7   14]
 [   4    3    1    0    2   44    5   12    4    1    0  680   80   15    2   31    9   38   10   51   19]
 [   4    6    1   18    1   15    4    3    8    0    2  117  639   11    7   18   17   75    7   20   22]
 [   6   11    8    3   19   58    1    8   28   24    9   12    4  718   18    6    8   10    2   30   18]
 [  31   17    4   25   33    4    1    3   54   13    3    1    6    5  818    0    9    7   36    1   27]
 [  11    1    9    2    7    9   15    0    1    1    0   40   13    4    0  870   32   20    1    6   24]
 [  14   18    5    1   18   19    4    1   10    1    1   10    6    7    6   15  892    3    1   11   29]
 [  11    2    0    6    2    6    5    2    3    0    0   46   70    5   12   29    6  768    2   11   19]
 [   3   28   14   33    2   10    3   64   12    2   15    4    5    2   39    1    5    2  789    4   21]
 [   6   17    6    1    2   13   26   32    2    0    0   20   12   11    3   12   10    7    6  876   26]
 [ 544  651  311  199  555  623  131  323  262  214  174  315  459  398  459  268  771  150  406  704 6015]]

2024-06-05 23:42:41,854 - ==> Best [Top1: 61.568   Top5: 92.348   Sparsity:0.00   Params: 169472 on epoch: 5]
2024-06-05 23:42:41,854 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/checkpoint.pth.tar
2024-06-05 23:42:41,872 - 

2024-06-05 23:42:41,872 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-05 23:42:49,184 - Epoch: [6][  100/ 1218]    Overall Loss 0.968823    Objective Loss 0.968823                                        LR 0.001000    Time 0.073088    
2024-06-05 23:42:55,218 - Epoch: [6][  200/ 1218]    Overall Loss 0.964598    Objective Loss 0.964598                                        LR 0.001000    Time 0.066697    
2024-06-05 23:43:01,033 - Epoch: [6][  300/ 1218]    Overall Loss 0.962594    Objective Loss 0.962594                                        LR 0.001000    Time 0.063842    
2024-06-05 23:43:06,824 - Epoch: [6][  400/ 1218]    Overall Loss 0.961181    Objective Loss 0.961181                                        LR 0.001000    Time 0.062351    
2024-06-05 23:43:12,456 - Epoch: [6][  500/ 1218]    Overall Loss 0.958700    Objective Loss 0.958700                                        LR 0.001000    Time 0.061142    
2024-06-05 23:43:17,873 - Epoch: [6][  600/ 1218]    Overall Loss 0.957924    Objective Loss 0.957924                                        LR 0.001000    Time 0.059976    
2024-06-05 23:43:23,601 - Epoch: [6][  700/ 1218]    Overall Loss 0.956338    Objective Loss 0.956338                                        LR 0.001000    Time 0.059587    
2024-06-05 23:43:29,247 - Epoch: [6][  800/ 1218]    Overall Loss 0.956166    Objective Loss 0.956166                                        LR 0.001000    Time 0.059193    
2024-06-05 23:43:34,849 - Epoch: [6][  900/ 1218]    Overall Loss 0.955705    Objective Loss 0.955705                                        LR 0.001000    Time 0.058838    
2024-06-05 23:43:40,186 - Epoch: [6][ 1000/ 1218]    Overall Loss 0.951904    Objective Loss 0.951904                                        LR 0.001000    Time 0.058289    
2024-06-05 23:43:45,756 - Epoch: [6][ 1100/ 1218]    Overall Loss 0.950293    Objective Loss 0.950293                                        LR 0.001000    Time 0.058052    
2024-06-05 23:43:51,221 - Epoch: [6][ 1200/ 1218]    Overall Loss 0.949039    Objective Loss 0.949039                                        LR 0.001000    Time 0.057766    
2024-06-05 23:43:52,190 - Epoch: [6][ 1218/ 1218]    Overall Loss 0.948495    Objective Loss 0.948495    Top1 57.457213    Top5 91.442543    LR 0.001000    Time 0.057707    
2024-06-05 23:43:52,450 - --- validate (epoch=6)-----------
2024-06-05 23:43:52,450 - 34633 samples (256 per mini-batch)
2024-06-05 23:43:59,192 - Epoch: [6][  100/  136]    Loss 0.864828    Top1 60.699219    Top5 91.050781    
2024-06-05 23:44:01,029 - Epoch: [6][  136/  136]    Loss 0.860364    Top1 60.809055    Top5 91.037450    
2024-06-05 23:44:01,251 - ==> Top1: 60.809    Top5: 91.037    Loss: 0.860

2024-06-05 23:44:01,252 - ==> Confusion:
[[ 718    2    6    2   28    2    0    9   15   87    3    7    0    5   20    5    2    4    2    3   11]
 [   2  779    5    9   49   69    2   34    6    4   14    7    7    4   24    4    2    3   22    6   11]
 [  18    2  676   43   24    5   40   50    2    6   15   10    6    9    8   16    3    8    8    5   16]
 [   3    3   20  817    8   16    5   13    3    0   26    4    8    6   27    4    4   16   23    1    9]
 [  46   19    6    3  844   29    3    6    3   13    4    3    1   10   14   17    8    1   11    2   11]
 [   6   68    3   16   27  677    7   73    4    4    6   23   17   58   10    8    5    4   12    5   10]
 [   5    9   59   15   10   18  841   24    0    0    4   17    6    3    3   23    4    7    5   19   14]
 [   5   12   22   13   10   55    5  833    4    3    6   19    7    9    3    1    1    5   44    9   11]
 [  22   10    1    3    0    2    0    6  715   64   18    5   10   44   77    0    1    6   12    0    6]
 [ 143    2    0    0   19    5    0   10   67  651    0    1    6   46   28    0    0    5    5    1   12]
 [   3    5   17   41   11   12    3   25   19    5  833    3    3   19   22    1    2    2   28    3    7]
 [   4    2    0    1    2   30    2    9    3    0    1  736   84   23    3   27    2   54    3   12   13]
 [   1    2    2   24    2   12    1   10    1    0    1  101  692   10    6   11    3   89   13    8    6]
 [   7    3    6    4   15   28    3   16    9   16    9   13    9  824    7    5    5    8    2    5    7]
 [  17    9    0   52   27    2    0    5   36   15    7    0    5    9  868    0    3    6   25    0   12]
 [   7    3    5    3    6    3    7    1    0    0    0   41   33    8    0  888    8   40    3    0   10]
 [  11   29    7    6   34   16    2    4    7    1    3   14   10   17   11   25  829   10    2    7   27]
 [   8    0    1    7    0    4    4    3    0    1    0   42   87    9    9   14    1  797    6    4    8]
 [   3   12   11   52    7    5    1   69   10    2   11    7    6    3   52    0    0    4  787    3   13]
 [   1   15    6    2    2   19   14   83    1    0    4   54   23   25    3   20   10    7    3  787    9]
 [ 525  451  268  480  580  451  119  521  175  187  258  393  679  654  727  355  447  311  380  503 5468]]

2024-06-05 23:44:01,255 - ==> Best [Top1: 61.568   Top5: 92.348   Sparsity:0.00   Params: 169472 on epoch: 5]
2024-06-05 23:44:01,255 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/checkpoint.pth.tar
2024-06-05 23:44:01,263 - 

2024-06-05 23:44:01,263 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-05 23:44:09,069 - Epoch: [7][  100/ 1218]    Overall Loss 0.933453    Objective Loss 0.933453                                        LR 0.001000    Time 0.078028    
2024-06-05 23:44:15,260 - Epoch: [7][  200/ 1218]    Overall Loss 0.925426    Objective Loss 0.925426                                        LR 0.001000    Time 0.069955    
2024-06-05 23:44:21,132 - Epoch: [7][  300/ 1218]    Overall Loss 0.925785    Objective Loss 0.925785                                        LR 0.001000    Time 0.066202    
2024-06-05 23:44:26,661 - Epoch: [7][  400/ 1218]    Overall Loss 0.922339    Objective Loss 0.922339                                        LR 0.001000    Time 0.063467    
2024-06-05 23:44:32,489 - Epoch: [7][  500/ 1218]    Overall Loss 0.921615    Objective Loss 0.921615                                        LR 0.001000    Time 0.062426    
2024-06-05 23:44:37,832 - Epoch: [7][  600/ 1218]    Overall Loss 0.916975    Objective Loss 0.916975                                        LR 0.001000    Time 0.060923    
2024-06-05 23:44:43,086 - Epoch: [7][  700/ 1218]    Overall Loss 0.911178    Objective Loss 0.911178                                        LR 0.001000    Time 0.059721    
2024-06-05 23:44:48,356 - Epoch: [7][  800/ 1218]    Overall Loss 0.910099    Objective Loss 0.910099                                        LR 0.001000    Time 0.058840    
2024-06-05 23:44:53,962 - Epoch: [7][  900/ 1218]    Overall Loss 0.909180    Objective Loss 0.909180                                        LR 0.001000    Time 0.058529    
2024-06-05 23:44:59,727 - Epoch: [7][ 1000/ 1218]    Overall Loss 0.906902    Objective Loss 0.906902                                        LR 0.001000    Time 0.058439    
2024-06-05 23:45:05,467 - Epoch: [7][ 1100/ 1218]    Overall Loss 0.906269    Objective Loss 0.906269                                        LR 0.001000    Time 0.058342    
2024-06-05 23:45:11,087 - Epoch: [7][ 1200/ 1218]    Overall Loss 0.905597    Objective Loss 0.905597                                        LR 0.001000    Time 0.058162    
2024-06-05 23:45:12,254 - Epoch: [7][ 1218/ 1218]    Overall Loss 0.905271    Objective Loss 0.905271    Top1 56.479218    Top5 91.687042    LR 0.001000    Time 0.058260    
2024-06-05 23:45:12,450 - --- validate (epoch=7)-----------
2024-06-05 23:45:12,450 - 34633 samples (256 per mini-batch)
2024-06-05 23:45:19,534 - Epoch: [7][  100/  136]    Loss 0.843594    Top1 61.644531    Top5 91.484375    
2024-06-05 23:45:21,217 - Epoch: [7][  136/  136]    Loss 0.839651    Top1 61.724367    Top5 91.591834    
2024-06-05 23:45:21,459 - ==> Top1: 61.724    Top5: 91.592    Loss: 0.840

2024-06-05 23:45:21,460 - ==> Confusion:
[[ 709    5   11    2   20    5    0    2    9  113    1    6    0    6    6    7    2    4    5    1   17]
 [   2  775    4    1   37   77    7   20    8    6    7   14    5    8   20    1    5    9   31    9   17]
 [  23   10  690   22   14   10   47   31    4    5    9   16    0   12   10   11    4    7   13   15   17]
 [  10    3   31  764    3   17    6    8    7    1   30    2   11    5   54    2    4   14   29    6    9]
 [  38   15    5    1  836   25    2    9    6   25    9    7    0   11   24    9   12    7    1    2   10]
 [   4   43    3    8   21  747    6   55    1    5    5   36    7   43    5    5    8    9   16    7    9]
 [   1   13   34    7    3   16  865   18    1    0    9   18    3    3    4   21    3   17    7   25   18]
 [   5   22   24   10    3   76    4  756    6    5    9   21    0   10    5    2    0   11   72   20   16]
 [  16    8    0    1    0    3    0    5  735   88   12    3    4   55   42    1    2    8   13    0    6]
 [  91    4    0    1   11    4    1    2   53  749    1    2    1   40   11    1    2    5    2    1   19]
 [   1   15   16   27   11   12   12   19   27    4  829    2    1   22   15    0    3    3   28    5   12]
 [   1    2    2    0    3   17    5   18    5    0    0  836   19   21    3   10    2   34    6   21    6]
 [   4    1    0    7    0    6    2    6    7    0    2  202  583   10   10    6    4  111   15    9   10]
 [   4    5    3    1    7   19    2    4   12   27    7   36    2  829    6    3    2    9    5   11    7]
 [  16   12    0   22   26    4    0    4   41   19   13    0    5   21  872    0    4    8   21    1    9]
 [   4    8    3    2    5    4    5    1    4    0    1   64   13    4    2  859   11   54    2    8   12]
 [  11   23    2    7   25   22    4    5    7    0    1   25    9    5    4   25  839   12    6   15   25]
 [   2    3    0    1    1    1    1    3    1    0    0   68   35    5   15   12    2  846    5    2    2]
 [   2   18   12   26    7    6    1   37   21    1   12    5    3    1   42    1    2    5  848    3    5]
 [   4    9    5    0    0   19   15   43    2    0    2   60   11   16    0    5    3   14   12  856   12]
 [ 573  499  235  297  474  519  127  312  260  255  260  565  463  687  702  306  334  445  418  647 5554]]

2024-06-05 23:45:21,464 - ==> Best [Top1: 61.724   Top5: 91.592   Sparsity:0.00   Params: 169472 on epoch: 7]
2024-06-05 23:45:21,464 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/checkpoint.pth.tar
2024-06-05 23:45:21,477 - 

2024-06-05 23:45:21,478 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-05 23:45:28,744 - Epoch: [8][  100/ 1218]    Overall Loss 0.883161    Objective Loss 0.883161                                        LR 0.001000    Time 0.072626    
2024-06-05 23:45:34,447 - Epoch: [8][  200/ 1218]    Overall Loss 0.878652    Objective Loss 0.878652                                        LR 0.001000    Time 0.064819    
2024-06-05 23:45:40,162 - Epoch: [8][  300/ 1218]    Overall Loss 0.877105    Objective Loss 0.877105                                        LR 0.001000    Time 0.062254    
2024-06-05 23:45:45,597 - Epoch: [8][  400/ 1218]    Overall Loss 0.875651    Objective Loss 0.875651                                        LR 0.001000    Time 0.060272    
2024-06-05 23:45:51,421 - Epoch: [8][  500/ 1218]    Overall Loss 0.880205    Objective Loss 0.880205                                        LR 0.001000    Time 0.059860    
2024-06-05 23:45:56,832 - Epoch: [8][  600/ 1218]    Overall Loss 0.878831    Objective Loss 0.878831                                        LR 0.001000    Time 0.058897    
2024-06-05 23:46:02,132 - Epoch: [8][  700/ 1218]    Overall Loss 0.879005    Objective Loss 0.879005                                        LR 0.001000    Time 0.058052    
2024-06-05 23:46:07,658 - Epoch: [8][  800/ 1218]    Overall Loss 0.878955    Objective Loss 0.878955                                        LR 0.001000    Time 0.057700    
2024-06-05 23:46:13,056 - Epoch: [8][  900/ 1218]    Overall Loss 0.878775    Objective Loss 0.878775                                        LR 0.001000    Time 0.057283    
2024-06-05 23:46:18,668 - Epoch: [8][ 1000/ 1218]    Overall Loss 0.876431    Objective Loss 0.876431                                        LR 0.001000    Time 0.057164    
2024-06-05 23:46:24,154 - Epoch: [8][ 1100/ 1218]    Overall Loss 0.874758    Objective Loss 0.874758                                        LR 0.001000    Time 0.056953    
2024-06-05 23:46:29,669 - Epoch: [8][ 1200/ 1218]    Overall Loss 0.873230    Objective Loss 0.873230                                        LR 0.001000    Time 0.056800    
2024-06-05 23:46:30,846 - Epoch: [8][ 1218/ 1218]    Overall Loss 0.872922    Objective Loss 0.872922    Top1 57.457213    Top5 90.464548    LR 0.001000    Time 0.056927    
2024-06-05 23:46:31,097 - --- validate (epoch=8)-----------
2024-06-05 23:46:31,097 - 34633 samples (256 per mini-batch)
2024-06-05 23:46:37,934 - Epoch: [8][  100/  136]    Loss 0.794123    Top1 64.695312    Top5 93.441406    
2024-06-05 23:46:39,847 - Epoch: [8][  136/  136]    Loss 0.786812    Top1 64.822568    Top5 93.454220    
2024-06-05 23:46:40,054 - ==> Top1: 64.823    Top5: 93.454    Loss: 0.787

2024-06-05 23:46:40,056 - ==> Confusion:
[[ 701    2    8    0   22    0    1    2   10  129    0    1    1    3    9   11    4    3    4    1   19]
 [   2  905    6    1   24   21    8   10   11    4    4    6    4    1   10    3    9    1   17    4   12]
 [  18    8  709    9   21    6   74   18    3    4   15    8    5    7    1   19    9    1   12    5   18]
 [   4    9   50  741   11    9   14    5    3    5   40    2    7    5   38   11    4    8   29    0   21]
 [  27   42    4    1  857    8    5    2    6   23    5    1    0    5   12   19   13    0    6    1   17]
 [   5  135    6    9   35  606   16   51    6   10    7   24    8   35    5   15   13    5    6   24   22]
 [   2    9   42    3    5    2  929    7    0    3    3    3    3    1    0   24    2    2    5   26   15]
 [   0   45   41    4   10   31   13  748    5    6    7   13    7    7    2    5    2    6   71   32   22]
 [  15   12    2    1    5    2    2    2  787   67   12    1    9   18   23    3    3    6   14    1   17]
 [  73    5    1    0   19    0    0    0   62  780    3    1    0   20    9    2    0    3    4    1   18]
 [   4   30   16   22   10    3   15    6   33    7  835    1    7   14    7    2    0    0   34    1   17]
 [   3    6    7    0    5   12    6   11    3    1    1  662   94   22    1   56    4   38    5   52   22]
 [   2    5    1    8    2    3    3    6    2    1    1   62  726    6    3   32   14   79    7   16   16]
 [   2    7    6    1    8   17    5    4   37   34   14   12    7  789    4   13    8   13    1   10    9]
 [  20   15    1   23   31    3    0    1   67   20   10    1    7    5  817    5    7    9   34    1   21]
 [   4    3    2    0    7    3   15    0    1    1    0   14   15    2    0  941   18   17    2    7   14]
 [  12   36    4    0   17    8    4    1    4    2    0    5    6    6    2   32  880    7    2   10   34]
 [   3    2    6    4    0    0    2    0    3    1    0   14   79    5    6   50    2  805    3    7   13]
 [   1   41   15   28    3    5    5   29   15    1   10    6    5    3   24    3    1    2  843    4   14]
 [   0   17    7    0    0    9   35   11    0    1    1   16    6    4    1   31   12    7    6  909   15]
 [ 430  749  306  166  547  204  214  210  251  277  241  175  555  499  363  573  595  206  347  544 6480]]

2024-06-05 23:46:40,060 - ==> Best [Top1: 64.823   Top5: 93.454   Sparsity:0.00   Params: 169472 on epoch: 8]
2024-06-05 23:46:40,060 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/checkpoint.pth.tar
2024-06-05 23:46:40,079 - 

2024-06-05 23:46:40,079 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-05 23:46:47,419 - Epoch: [9][  100/ 1218]    Overall Loss 0.836259    Objective Loss 0.836259                                        LR 0.001000    Time 0.073363    
2024-06-05 23:46:53,005 - Epoch: [9][  200/ 1218]    Overall Loss 0.850627    Objective Loss 0.850627                                        LR 0.001000    Time 0.064598    
2024-06-05 23:46:58,365 - Epoch: [9][  300/ 1218]    Overall Loss 0.844471    Objective Loss 0.844471                                        LR 0.001000    Time 0.060906    
2024-06-05 23:47:03,639 - Epoch: [9][  400/ 1218]    Overall Loss 0.843991    Objective Loss 0.843991                                        LR 0.001000    Time 0.058857    
2024-06-05 23:47:09,400 - Epoch: [9][  500/ 1218]    Overall Loss 0.842283    Objective Loss 0.842283                                        LR 0.001000    Time 0.058604    
2024-06-05 23:47:14,872 - Epoch: [9][  600/ 1218]    Overall Loss 0.840554    Objective Loss 0.840554                                        LR 0.001000    Time 0.057952    
2024-06-05 23:47:21,098 - Epoch: [9][  700/ 1218]    Overall Loss 0.842703    Objective Loss 0.842703                                        LR 0.001000    Time 0.058564    
2024-06-05 23:47:26,864 - Epoch: [9][  800/ 1218]    Overall Loss 0.845531    Objective Loss 0.845531                                        LR 0.001000    Time 0.058448    
2024-06-05 23:47:32,505 - Epoch: [9][  900/ 1218]    Overall Loss 0.845357    Objective Loss 0.845357                                        LR 0.001000    Time 0.058218    
2024-06-05 23:47:38,138 - Epoch: [9][ 1000/ 1218]    Overall Loss 0.847355    Objective Loss 0.847355                                        LR 0.001000    Time 0.058027    
2024-06-05 23:47:43,770 - Epoch: [9][ 1100/ 1218]    Overall Loss 0.847700    Objective Loss 0.847700                                        LR 0.001000    Time 0.057869    
2024-06-05 23:47:49,244 - Epoch: [9][ 1200/ 1218]    Overall Loss 0.846898    Objective Loss 0.846898                                        LR 0.001000    Time 0.057607    
2024-06-05 23:47:50,198 - Epoch: [9][ 1218/ 1218]    Overall Loss 0.846447    Objective Loss 0.846447    Top1 63.080685    Top5 94.376528    LR 0.001000    Time 0.057538    
2024-06-05 23:47:50,447 - --- validate (epoch=9)-----------
2024-06-05 23:47:50,447 - 34633 samples (256 per mini-batch)
2024-06-05 23:47:57,384 - Epoch: [9][  100/  136]    Loss 0.770739    Top1 63.156250    Top5 91.757812    
2024-06-05 23:47:59,385 - Epoch: [9][  136/  136]    Loss 0.779456    Top1 62.853348    Top5 91.744868    
2024-06-05 23:47:59,617 - ==> Top1: 62.853    Top5: 91.745    Loss: 0.779

2024-06-05 23:47:59,619 - ==> Confusion:
[[ 694    0   18    3   22    4    3    4   12  119    1    6    0    5    9    6    3    0    8    2   12]
 [   3  877    9    1   32   27   13   18    6    4    7   11    2    2    6    1    4    2   28    6    4]
 [  15    4  734   18   15    3   83   16    3    4   12    8    0    7    2   12    1    1    9   11   12]
 [   5    4   46  796    6   12   16    2    4    1   26    6    5    3   30    3    1    4   26    3   17]
 [  31   25    8    1  866   19    8    4    6   20    2    5    0    5   15    8    7    2    8    1   13]
 [   3   77    5    8   32  740   14   35    3   10    1   22    3   42    4    3    6    2   12   14    7]
 [   2    8   24    4    4    9  970   11    1    4    1    5    1    0    0    7    4    3    7   16    5]
 [   3   30   23    7    6   71   22  774    2    4    9   16    2    5    3    1    0    3   62   21   13]
 [  17   11    4    5    4    1    4    4  744   59   19    7    5   31   49    1    5    2   20    5    5]
 [  90    3    4    1   16    2    2    3   76  739    2    5    0   22   13    1    0    2    6    2   12]
 [   6   16   20   24    9    9   21   10   16    7  829    4    3   10   17    1    0    1   44    5   12]
 [   3    1    6    2    1   30   12    6    1    1    2  800   25   18    1   31    2   12    7   43    7]
 [   0    8    3   12    2   19   10    6    5    0    1  171  636    8    3    9    4   53   16   18   11]
 [   4    2    6    1    7   28    3    3   18   25    9   27    3  811   20    3    3    4    0   19    5]
 [  19    8    4   37   21    8    1    0   39   16   11    2    4    9  850    1    2    8   38    5   15]
 [   1    5    3    2    8    1   32    1    2    0    1   48    9    2    1  898    8   20    6   11    7]
 [  10   41   15    5   21   16   17    0    5    0    3   21    2    6    5   15  841    5    4   16   24]
 [   3    1    3    6    2    2   10    2    3    2    1   72   42    8    4   20    1  804    6    4    9]
 [   3   21   13   35    6   14    3   37   11    2    9    6    2    2   31    0    0    2  845    7    9]
 [   2   15   12    2    6   22   43   23    4    0    3   35    6    9    1   10    4    3    3  875   10]
 [ 462  627  512  359  497  471  459  280  199  257  239  510  436  514  471  362  303  163  476  690 5645]]

2024-06-05 23:47:59,622 - ==> Best [Top1: 64.823   Top5: 93.454   Sparsity:0.00   Params: 169472 on epoch: 8]
2024-06-05 23:47:59,622 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/checkpoint.pth.tar
2024-06-05 23:47:59,634 - 

2024-06-05 23:47:59,634 - Initiating quantization aware training (QAT)...
2024-06-05 23:47:59,662 - 

2024-06-05 23:47:59,662 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-05 23:48:06,583 - Epoch: [10][  100/ 1218]    Overall Loss 0.948570    Objective Loss 0.948570                                        LR 0.001000    Time 0.069177    
2024-06-05 23:48:12,111 - Epoch: [10][  200/ 1218]    Overall Loss 0.916199    Objective Loss 0.916199                                        LR 0.001000    Time 0.062215    
2024-06-05 23:48:17,924 - Epoch: [10][  300/ 1218]    Overall Loss 0.900998    Objective Loss 0.900998                                        LR 0.001000    Time 0.060845    
2024-06-05 23:48:23,552 - Epoch: [10][  400/ 1218]    Overall Loss 0.891494    Objective Loss 0.891494                                        LR 0.001000    Time 0.059696    
2024-06-05 23:48:29,145 - Epoch: [10][  500/ 1218]    Overall Loss 0.880652    Objective Loss 0.880652                                        LR 0.001000    Time 0.058940    
2024-06-05 23:48:34,697 - Epoch: [10][  600/ 1218]    Overall Loss 0.874385    Objective Loss 0.874385                                        LR 0.001000    Time 0.058365    
2024-06-05 23:48:40,368 - Epoch: [10][  700/ 1218]    Overall Loss 0.866307    Objective Loss 0.866307                                        LR 0.001000    Time 0.058125    
2024-06-05 23:48:46,078 - Epoch: [10][  800/ 1218]    Overall Loss 0.861764    Objective Loss 0.861764                                        LR 0.001000    Time 0.057995    
2024-06-05 23:48:51,551 - Epoch: [10][  900/ 1218]    Overall Loss 0.856715    Objective Loss 0.856715                                        LR 0.001000    Time 0.057629    
2024-06-05 23:48:56,857 - Epoch: [10][ 1000/ 1218]    Overall Loss 0.853022    Objective Loss 0.853022                                        LR 0.001000    Time 0.057170    
2024-06-05 23:49:02,151 - Epoch: [10][ 1100/ 1218]    Overall Loss 0.851324    Objective Loss 0.851324                                        LR 0.001000    Time 0.056783    
2024-06-05 23:49:07,886 - Epoch: [10][ 1200/ 1218]    Overall Loss 0.848887    Objective Loss 0.848887                                        LR 0.001000    Time 0.056828    
2024-06-05 23:49:08,861 - Epoch: [10][ 1218/ 1218]    Overall Loss 0.848527    Objective Loss 0.848527    Top1 63.814181    Top5 93.643032    LR 0.001000    Time 0.056788    
2024-06-05 23:49:09,068 - --- validate (epoch=10)-----------
2024-06-05 23:49:09,069 - 34633 samples (256 per mini-batch)
2024-06-05 23:49:16,000 - Epoch: [10][  100/  136]    Loss 0.779541    Top1 63.351562    Top5 92.019531    
2024-06-05 23:49:18,113 - Epoch: [10][  136/  136]    Loss 0.774088    Top1 63.566541    Top5 92.183755    
2024-06-05 23:49:18,364 - ==> Top1: 63.567    Top5: 92.184    Loss: 0.774

2024-06-05 23:49:18,366 - ==> Confusion:
[[ 766    1   14    3   15    1    2    3    9   71    2    2    1    6   11    2    1    3    5    2   11]
 [   6  828    8    4   27   48    6   26    9    3    5    9    0    2   18    2   11    4   28    8   11]
 [  23    4  777   18   11    9   28   16    1    4    8   10    6    7    7    7    6    4   11    6    7]
 [   5    1   48  794    5    9    3    7    6    3   22    3    9    6   45    0    5    9   32    0    4]
 [  55   13    6    1  841   15    1    2    3   16    2    4    1   11   27    6   14    3   17    1   15]
 [  10   48   14   12   27  688    4   62    4    7    2   16   18   59   13    5   10    8   18    9    9]
 [   7    9   70    2    2    4  882   11    2    0    6    7    7    2    3   14    6    8    7   26   11]
 [   9   23   27    7    4   44    8  808    2    5    4   15    7    4    3    1    2    4   70   24    6]
 [  29    7    5    0    3    2    0    4  748   77   12    5   10   29   38    1    6    2   16    1    7]
 [ 143    2    6    1    8    5    3    6   47  696    3    1    0   35   18    2    2    7    3    1   12]
 [   7   10   28   25    5    9    8   13   21    4  832    5    5   22   21    0    3    0   34    1   11]
 [   7    0    7    1    2   16    1    7    3    1    0  753   80   23    2   14    4   57    9   17    7]
 [   4    1    3    8    1    3    2    1    3    0    1   73  727   10    7   11    3  101   17    9   10]
 [  10    0    4    0    3   19    3    6   17   18   10   14    7  837   14    4    6    7    4   13    5]
 [  28    5    2   24   11    3    0    1   44   11    5    2    5    8  899    0    1    7   31    0   11]
 [   2    4   18    2   11    2   11    0    2    0    0   41   34    3    1  860   15   42    3    6    9]
 [  16   16   12    5   16   10    4    3    9    3    0   17   14   10    6   21  868    3    9    5   25]
 [   1    1    3   10    0    0    2    3    3    1    0   26   49    7    9   11    4  858    7    4    6]
 [   4    5   14   25    3    6    0   37   19    0    7    7    6    1   31    1    5    2  874    2    9]
 [   1   11   10    0    2   11   16   27    3    0    1   43   17   11    1   13    7    6   12  888    8]
 [ 594  411  432  313  384  361  109  345  220  188  236  321  706  602  706  275  496  304  554  584 5791]]

2024-06-05 23:49:18,368 - ==> Best [Top1: 63.567   Top5: 92.184   Sparsity:0.00   Params: 169472 on epoch: 10]
2024-06-05 23:49:18,369 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-05 23:49:18,382 - 

2024-06-05 23:49:18,382 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-05 23:49:25,786 - Epoch: [11][  100/ 1218]    Overall Loss 0.828811    Objective Loss 0.828811                                        LR 0.001000    Time 0.074009    
2024-06-05 23:49:31,633 - Epoch: [11][  200/ 1218]    Overall Loss 0.825838    Objective Loss 0.825838                                        LR 0.001000    Time 0.066226    
2024-06-05 23:49:37,422 - Epoch: [11][  300/ 1218]    Overall Loss 0.827845    Objective Loss 0.827845                                        LR 0.001000    Time 0.063439    
2024-06-05 23:49:43,325 - Epoch: [11][  400/ 1218]    Overall Loss 0.826212    Objective Loss 0.826212                                        LR 0.001000    Time 0.062332    
2024-06-05 23:49:49,512 - Epoch: [11][  500/ 1218]    Overall Loss 0.825395    Objective Loss 0.825395                                        LR 0.001000    Time 0.062235    
2024-06-05 23:49:54,964 - Epoch: [11][  600/ 1218]    Overall Loss 0.824115    Objective Loss 0.824115                                        LR 0.001000    Time 0.060944    
2024-06-05 23:50:00,843 - Epoch: [11][  700/ 1218]    Overall Loss 0.823680    Objective Loss 0.823680                                        LR 0.001000    Time 0.060633    
2024-06-05 23:50:06,397 - Epoch: [11][  800/ 1218]    Overall Loss 0.820349    Objective Loss 0.820349                                        LR 0.001000    Time 0.059994    
2024-06-05 23:50:12,137 - Epoch: [11][  900/ 1218]    Overall Loss 0.820460    Objective Loss 0.820460                                        LR 0.001000    Time 0.059702    
2024-06-05 23:50:17,648 - Epoch: [11][ 1000/ 1218]    Overall Loss 0.821651    Objective Loss 0.821651                                        LR 0.001000    Time 0.059241    
2024-06-05 23:50:23,157 - Epoch: [11][ 1100/ 1218]    Overall Loss 0.823260    Objective Loss 0.823260                                        LR 0.001000    Time 0.058861    
2024-06-05 23:50:28,489 - Epoch: [11][ 1200/ 1218]    Overall Loss 0.823803    Objective Loss 0.823803                                        LR 0.001000    Time 0.058398    
2024-06-05 23:50:29,396 - Epoch: [11][ 1218/ 1218]    Overall Loss 0.823552    Objective Loss 0.823552    Top1 64.792176    Top5 93.643032    LR 0.001000    Time 0.058278    
2024-06-05 23:50:29,635 - --- validate (epoch=11)-----------
2024-06-05 23:50:29,635 - 34633 samples (256 per mini-batch)
2024-06-05 23:50:36,356 - Epoch: [11][  100/  136]    Loss 0.785404    Top1 64.664063    Top5 93.136719    
2024-06-05 23:50:38,439 - Epoch: [11][  136/  136]    Loss 0.779094    Top1 64.923628    Top5 93.110617    
2024-06-05 23:50:38,691 - ==> Top1: 64.924    Top5: 93.111    Loss: 0.779

2024-06-05 23:50:38,692 - ==> Confusion:
[[ 760    0   11    2   18    0    1    2   15   66    3    2    1    8   14    7    8    2    5    0    6]
 [   5  849    7    5   39   18    6   20   13    3   13    7    6    4   12    3   11    2   20    3   17]
 [  16    3  773   12    8    0   37   20    0    7   16    4    1    5    8   20    8    2   16    5    9]
 [   8    2   38  782    6    8    6    4    6    0   34    3    9    5   54    8    5    5   24    0    9]
 [  49   13   12    0  847   12    3    2   10   25    7    2    0    6   15   12   15    4    5    1   14]
 [  11   70    7   11   32  658    4   59    7    6   11   28   16   54    5    6   11    3   13   16   15]
 [   4    5   84    7    6    8  868   10    2    1    9    5    4    0    0   27    3    3    4   16   20]
 [   8   25   35    5    9   37    9  774    4    1   21   16    8    7    4    2    3    8   72   15   14]
 [  18    4    2    1    1    1    0    1  779   50   25    1   10   32   40    2    4    1   21    0    9]
 [ 142    1    4    0    8    1    0    1   87  689    2    0    1   37   14    3    1    3    1    0    6]
 [   5    8   19   17    9    4   10    5   24    2  886    1    5   19   10    2    3    0   23    4    8]
 [   7    5    5    0    4   12    2   12    2    2    0  757   56   25    2   46   12   15    7   28   12]
 [   2    1    3   12    4    3    2    3    1    0    6  100  731    9    5   24    8   41   17    7   16]
 [   7    2    4    0    9   14    1    2   19   17   22   14    7  819    7   11    9    5    2   15   15]
 [  24    8    2   28   19    1    0    1   52   12   11    1    9    9  873    1    2    4   25    1   15]
 [   3    2    3    0    6    0    7    0    1    0    0   19   14    2    1  977   13    5    3    2    8]
 [  10   16   11    4    9    8    2    1   10    1    2    9    8    5    2   28  907    3    3    4   29]
 [   4    0    0    9    2    1    0    1    2    1    0   51   87    9   10   63   10  735    3    3   14]
 [   3    6   14   25   11    9    3   24   20    0   20    4    7    2   44    1    5    2  846    2   10]
 [   1   10   14    3    4    4   12   38    4    0    7   42   16   16    2   24   17    5   12  832   25]
 [ 497  387  487  290  469  213  121  215  243  162  447  264  585  568  516  531  582  135  447  430 6343]]

2024-06-05 23:50:38,696 - ==> Best [Top1: 64.924   Top5: 93.111   Sparsity:0.00   Params: 169472 on epoch: 11]
2024-06-05 23:50:38,696 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-05 23:50:38,707 - 

2024-06-05 23:50:38,707 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-05 23:50:45,909 - Epoch: [12][  100/ 1218]    Overall Loss 0.812295    Objective Loss 0.812295                                        LR 0.001000    Time 0.071997    
2024-06-05 23:50:51,728 - Epoch: [12][  200/ 1218]    Overall Loss 0.816178    Objective Loss 0.816178                                        LR 0.001000    Time 0.065079    
2024-06-05 23:50:57,640 - Epoch: [12][  300/ 1218]    Overall Loss 0.817819    Objective Loss 0.817819                                        LR 0.001000    Time 0.063084    
2024-06-05 23:51:03,387 - Epoch: [12][  400/ 1218]    Overall Loss 0.814736    Objective Loss 0.814736                                        LR 0.001000    Time 0.061676    
2024-06-05 23:51:09,078 - Epoch: [12][  500/ 1218]    Overall Loss 0.813344    Objective Loss 0.813344                                        LR 0.001000    Time 0.060717    
2024-06-05 23:51:14,445 - Epoch: [12][  600/ 1218]    Overall Loss 0.812193    Objective Loss 0.812193                                        LR 0.001000    Time 0.059539    
2024-06-05 23:51:19,929 - Epoch: [12][  700/ 1218]    Overall Loss 0.810758    Objective Loss 0.810758                                        LR 0.001000    Time 0.058864    
2024-06-05 23:51:25,989 - Epoch: [12][  800/ 1218]    Overall Loss 0.812591    Objective Loss 0.812591                                        LR 0.001000    Time 0.059079    
2024-06-05 23:51:31,529 - Epoch: [12][  900/ 1218]    Overall Loss 0.810752    Objective Loss 0.810752                                        LR 0.001000    Time 0.058667    
2024-06-05 23:51:37,322 - Epoch: [12][ 1000/ 1218]    Overall Loss 0.810499    Objective Loss 0.810499                                        LR 0.001000    Time 0.058590    
2024-06-05 23:51:42,668 - Epoch: [12][ 1100/ 1218]    Overall Loss 0.810473    Objective Loss 0.810473                                        LR 0.001000    Time 0.058122    
2024-06-05 23:51:47,965 - Epoch: [12][ 1200/ 1218]    Overall Loss 0.810436    Objective Loss 0.810436                                        LR 0.001000    Time 0.057690    
2024-06-05 23:51:48,870 - Epoch: [12][ 1218/ 1218]    Overall Loss 0.810009    Objective Loss 0.810009    Top1 66.503667    Top5 94.132029    LR 0.001000    Time 0.057580    
2024-06-05 23:51:49,124 - --- validate (epoch=12)-----------
2024-06-05 23:51:49,125 - 34633 samples (256 per mini-batch)
2024-06-05 23:51:55,460 - Epoch: [12][  100/  136]    Loss 0.742678    Top1 66.265625    Top5 94.082031    
2024-06-05 23:51:57,368 - Epoch: [12][  136/  136]    Loss 0.741896    Top1 66.263390    Top5 94.008605    
2024-06-05 23:51:57,592 - ==> Top1: 66.263    Top5: 94.009    Loss: 0.742

2024-06-05 23:51:57,593 - ==> Confusion:
[[ 757    4    6    0   26    1    0    9    3   84    0    2    0    2    7    5    1    1    6    3   14]
 [   2  830    4    3   52   37   10   53    3    4    2    4    1    1    6    1    6    2   14    9   19]
 [  19    3  753   10   11    5   44   28    3    6    5    1    3    8    3   11   11    2    8   12   24]
 [   8    9   44  785    8    8    6   11    5    4   25    4    6    5   30    3    6    7   24    1   17]
 [  32   20    3    1  892   11    2    8    3   13    0    1    1    5    8   10   15    2    5    4   18]
 [   6   53    7    2   37  723   14   82    2    6    3   14    8   26    5    5    4    4    7   21   14]
 [   1    5   49    2    4    3  926   13    2    2    4    4    2    3    0   19    4    3    4   25   11]
 [   5   15   27    1    7   31    7  860    1    4    4    9    4    2    3    0    2    4   49   29   13]
 [  14   11    3    3    3    3    0   10  727  113   12    1    4   25   25    1    5    2   16    9   15]
 [ 141    4    2    0   29    5    1   12   34  720    0    1    0   22   16    2    3    1    1    1    6]
 [   5    9   29   30   12   13    8   20   25    3  820    1    2   25   16    1    4    0   20    3   18]
 [   4    2    6    0    1   18    4   17    3    2    0  747   35   28    1   22    6   32    7   64   12]
 [   2    3    2    8    0   18    2    7    4    0    2   98  681   10    6   10    7   74   14   27   20]
 [   4    4    5    1   11   29    2   12   17   26    7    9    5  805   12    4    5    2    2   18   21]
 [  19    9    5   26   39    5    0    5   36   26    3    1    2    5  849    0    3    5   41    6   13]
 [   3    4    7    1    5    0    5    5    1    0    0   23    8    2    0  934   13   26    0   12   17]
 [   8   18    7    1   12   10    5    2    8    1    0    7    2    8    4   21  905    2    2   18   31]
 [   3    2    0    8    2    9    1    5    1    4    0   30   45   10    8   27    3  819    7   12    9]
 [   3   13   17   32    8    6    0   77   12    3    5    1    5    1   26    0    2    0  823    6   18]
 [   0    5   12    2    1   11   17   33    1    0    0   17    7    7    3    9    6    6    5  932   14]
 [ 481  420  395  184  570  356  149  467  169  200  189  224  468  467  351  360  493  205  316  808 6660]]

2024-06-05 23:51:57,595 - ==> Best [Top1: 66.263   Top5: 94.009   Sparsity:0.00   Params: 169472 on epoch: 12]
2024-06-05 23:51:57,595 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-05 23:51:57,607 - 

2024-06-05 23:51:57,607 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-05 23:52:04,587 - Epoch: [13][  100/ 1218]    Overall Loss 0.800106    Objective Loss 0.800106                                        LR 0.001000    Time 0.069765    
2024-06-05 23:52:09,889 - Epoch: [13][  200/ 1218]    Overall Loss 0.799761    Objective Loss 0.799761                                        LR 0.001000    Time 0.061383    
2024-06-05 23:52:15,254 - Epoch: [13][  300/ 1218]    Overall Loss 0.805042    Objective Loss 0.805042                                        LR 0.001000    Time 0.058795    
2024-06-05 23:52:20,440 - Epoch: [13][  400/ 1218]    Overall Loss 0.800450    Objective Loss 0.800450                                        LR 0.001000    Time 0.057055    
2024-06-05 23:52:25,766 - Epoch: [13][  500/ 1218]    Overall Loss 0.799843    Objective Loss 0.799843                                        LR 0.001000    Time 0.056292    
2024-06-05 23:52:31,282 - Epoch: [13][  600/ 1218]    Overall Loss 0.800219    Objective Loss 0.800219                                        LR 0.001000    Time 0.056098    
2024-06-05 23:52:36,888 - Epoch: [13][  700/ 1218]    Overall Loss 0.798563    Objective Loss 0.798563                                        LR 0.001000    Time 0.056090    
2024-06-05 23:52:42,290 - Epoch: [13][  800/ 1218]    Overall Loss 0.795452    Objective Loss 0.795452                                        LR 0.001000    Time 0.055828    
2024-06-05 23:52:47,741 - Epoch: [13][  900/ 1218]    Overall Loss 0.792822    Objective Loss 0.792822                                        LR 0.001000    Time 0.055679    
2024-06-05 23:52:53,019 - Epoch: [13][ 1000/ 1218]    Overall Loss 0.793769    Objective Loss 0.793769                                        LR 0.001000    Time 0.055387    
2024-06-05 23:52:58,241 - Epoch: [13][ 1100/ 1218]    Overall Loss 0.794118    Objective Loss 0.794118                                        LR 0.001000    Time 0.055096    
2024-06-05 23:53:03,601 - Epoch: [13][ 1200/ 1218]    Overall Loss 0.794564    Objective Loss 0.794564                                        LR 0.001000    Time 0.054970    
2024-06-05 23:53:04,475 - Epoch: [13][ 1218/ 1218]    Overall Loss 0.793826    Objective Loss 0.793826    Top1 66.503667    Top5 94.132029    LR 0.001000    Time 0.054874    
2024-06-05 23:53:04,695 - --- validate (epoch=13)-----------
2024-06-05 23:53:04,695 - 34633 samples (256 per mini-batch)
2024-06-05 23:53:11,178 - Epoch: [13][  100/  136]    Loss 0.731426    Top1 66.957031    Top5 93.714844    
2024-06-05 23:53:13,088 - Epoch: [13][  136/  136]    Loss 0.733944    Top1 66.835099    Top5 93.731412    
2024-06-05 23:53:13,303 - ==> Top1: 66.835    Top5: 93.731    Loss: 0.734

2024-06-05 23:53:13,305 - ==> Confusion:
[[ 744    0    7    2   26    0    1    2   12   84    1    3    1    3    6    4    8    1    8    1   17]
 [   4  843    4    3   42   27    8   22   10    4    6    6    1    3    4    5   16    4   24    7   20]
 [  18    1  785   16   17    3   35   19    2    4    7    5    3    1    0   11    9    2    9    8   15]
 [   6    2   40  803   11    9    3    3    1    1   29    1    7    1   22    1   11    6   35    3   21]
 [  33   15    5    2  917    6    3    4    3   10    2    4    0    1    6    6   14    2   11    1    9]
 [   7   63    8    2   45  690   10   51    6    5    6   29    8   24    4    5   25    4   15   18   18]
 [   5    6   77    4    5    2  900   14    1    2    9    5    2    0    0   12    5    5    6   15   11]
 [  10   13   17    3   12   28    9  811    6    2    7   18    3    1    2    0    5    2   88   26   14]
 [  15    9    2    4    4    1    0    2  780   71   19    1    5   18   23    1    7    3   21    1   15]
 [ 127    1    2    3   16    2    1    6   69  715    3    0    0   16   13    0    2    3    6    1   15]
 [   7    7   23   24   10    1   10    6   22    2  868    0    2    9    8    0    4    0   46    4   11]
 [   7    1    0    1    4   16    5   16    3    3    2  786   29   14    2   24   10   26    7   42   13]
 [   2    3    3    9    1    4    1    8    7    0    5  123  695    6    3   11   20   58   14   14    8]
 [   7    1    7    2   17   12    3    5   34   22   23   22    5  741   11    9   12   11    3   32   22]
 [  27    6    3   30   38    2    0    4   46   12   10    3    2    1  824    1    7    7   49    0   26]
 [   8    0   12    0   10    0   14    0    1    3    1   28   16    2    0  914   20   16    2    3   16]
 [   9   11    6    2   12   11    2    3    2    3    4   10    2    5    3   17  926    4    6   11   23]
 [   7    4    1    2    3    2    3    4    2    6    0   36   45    5    6   30    9  810    8    4   18]
 [   2   13   11   18    7    2    2   39   17    0    6    4    2    3   21    0    5    0  890    1   15]
 [   0   12   13    0    2    9   14   21    1    1    1   43    6    1    0    6   11    3   13  913   18]
 [ 523  320  363  211  600  206  166  280  232  173  319  323  491  320  345  288  748  157  483  592 6792]]

2024-06-05 23:53:13,307 - ==> Best [Top1: 66.835   Top5: 93.731   Sparsity:0.00   Params: 169472 on epoch: 13]
2024-06-05 23:53:13,307 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-05 23:53:13,321 - 

2024-06-05 23:53:13,321 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-05 23:53:20,190 - Epoch: [14][  100/ 1218]    Overall Loss 0.775838    Objective Loss 0.775838                                        LR 0.001000    Time 0.068665    
2024-06-05 23:53:25,566 - Epoch: [14][  200/ 1218]    Overall Loss 0.783923    Objective Loss 0.783923                                        LR 0.001000    Time 0.061168    
2024-06-05 23:53:30,769 - Epoch: [14][  300/ 1218]    Overall Loss 0.783689    Objective Loss 0.783689                                        LR 0.001000    Time 0.058114    
2024-06-05 23:53:35,766 - Epoch: [14][  400/ 1218]    Overall Loss 0.784505    Objective Loss 0.784505                                        LR 0.001000    Time 0.056071    
2024-06-05 23:53:40,946 - Epoch: [14][  500/ 1218]    Overall Loss 0.782195    Objective Loss 0.782195                                        LR 0.001000    Time 0.055210    
2024-06-05 23:53:45,908 - Epoch: [14][  600/ 1218]    Overall Loss 0.783341    Objective Loss 0.783341                                        LR 0.001000    Time 0.054276    
2024-06-05 23:53:51,123 - Epoch: [14][  700/ 1218]    Overall Loss 0.785278    Objective Loss 0.785278                                        LR 0.001000    Time 0.053969    
2024-06-05 23:53:56,296 - Epoch: [14][  800/ 1218]    Overall Loss 0.784529    Objective Loss 0.784529                                        LR 0.001000    Time 0.053686    
2024-06-05 23:54:01,444 - Epoch: [14][  900/ 1218]    Overall Loss 0.783843    Objective Loss 0.783843                                        LR 0.001000    Time 0.053438    
2024-06-05 23:54:06,558 - Epoch: [14][ 1000/ 1218]    Overall Loss 0.783516    Objective Loss 0.783516                                        LR 0.001000    Time 0.053206    
2024-06-05 23:54:11,975 - Epoch: [14][ 1100/ 1218]    Overall Loss 0.783267    Objective Loss 0.783267                                        LR 0.001000    Time 0.053291    
2024-06-05 23:54:17,395 - Epoch: [14][ 1200/ 1218]    Overall Loss 0.782278    Objective Loss 0.782278                                        LR 0.001000    Time 0.053365    
2024-06-05 23:54:18,424 - Epoch: [14][ 1218/ 1218]    Overall Loss 0.782289    Objective Loss 0.782289    Top1 64.792176    Top5 91.442543    LR 0.001000    Time 0.053420    
2024-06-05 23:54:18,652 - --- validate (epoch=14)-----------
2024-06-05 23:54:18,652 - 34633 samples (256 per mini-batch)
2024-06-05 23:54:24,880 - Epoch: [14][  100/  136]    Loss 0.719188    Top1 66.585937    Top5 93.242188    
2024-06-05 23:54:26,820 - Epoch: [14][  136/  136]    Loss 0.717787    Top1 66.693616    Top5 93.249213    
2024-06-05 23:54:27,050 - ==> Top1: 66.694    Top5: 93.249    Loss: 0.718

2024-06-05 23:54:27,051 - ==> Confusion:
[[ 719    2    4    0   26    5    3    3   13  112    5    0    2    4    6    1    1    1    3    1   20]
 [   0  848    5    5   48   37    6   16    6    3   13    2    4    6    6    6   16    1   13    6   16]
 [  16    4  728   19   16    5   44   20    2    5   28    8    4   11    7    8    3    2   15    9   16]
 [   3    4   32  778    5   15    3    3    3    5   50    3    8    5   38    5    3    9   36    0    8]
 [  27   13    2    0  923   13    0    2    4   16    4    6    0    5    7   10    9    0    3    1    9]
 [   4   72    2    6   33  737   10   62    4    5    7   11   10   33    5    3    6    3    4   11   15]
 [   2    5   41    6    2    5  929   15    3    1   17    0    4    1    4   14    4    1    3   13   16]
 [   2   22   20    5    4   62   11  822    3    4   22   17    3   11    0    1    2    4   38   20    4]
 [  12    5    1    0    6    3    2    2  785   67   26    0   11   32   23    3    2    2   10    2    8]
 [  86    2    0    1   18    2    0    3   84  730    6    2    1   35   13    3    1    3    2    1    8]
 [   3    4    9    9    5    9    2   10   17    0  934    2    3   19   10    1    4    0   12    3    8]
 [   4    6    4    0    4   36    5    8    1    1    2  716   74   30    1   32    5   26    2   42   12]
 [   3    3    0   13    4   14    6    4    5    1    3   92  731   21    5    5    9   52    5   10    9]
 [   5    5    3    2   10   31    2    7   16   15   17    9    8  832    6    6    4    3    2    7   11]
 [  10    6    4   16   33    1    1    0   47   16   20    2   13   10  884    3    2    4   12    1   13]
 [   4    5    6    3   13    3   16    1    2    0    2   18   16    6    2  914   13   22    2    6   12]
 [   6   22   10    4   19   16    6    3    2    0    2    5    5    9    4   22  902    4    4    9   18]
 [   4    3    0    8    0    9    7    4    3    6    0   36   58   23    7   20    2  798    2    6    9]
 [   5   16    6   27    6   10    2   40   15    4   27    5   11    2   40    2    1    1  826    2   10]
 [   1   16    3    1    4   17   28   22    7    1    5   19    9   14    0   13    7    1    6  903   11]
 [ 335  452  305  231  619  383  228  316  213  174  425  207  595  633  391  318  439  151  266  592 6659]]

2024-06-05 23:54:27,052 - ==> Best [Top1: 66.835   Top5: 93.731   Sparsity:0.00   Params: 169472 on epoch: 13]
2024-06-05 23:54:27,052 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-05 23:54:27,061 - 

2024-06-05 23:54:27,062 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-05 23:54:33,951 - Epoch: [15][  100/ 1218]    Overall Loss 0.779250    Objective Loss 0.779250                                        LR 0.001000    Time 0.068863    
2024-06-05 23:54:39,452 - Epoch: [15][  200/ 1218]    Overall Loss 0.775300    Objective Loss 0.775300                                        LR 0.001000    Time 0.061924    
2024-06-05 23:54:44,755 - Epoch: [15][  300/ 1218]    Overall Loss 0.767837    Objective Loss 0.767837                                        LR 0.001000    Time 0.058951    
2024-06-05 23:54:49,735 - Epoch: [15][  400/ 1218]    Overall Loss 0.769291    Objective Loss 0.769291                                        LR 0.001000    Time 0.056658    
2024-06-05 23:54:55,137 - Epoch: [15][  500/ 1218]    Overall Loss 0.768814    Objective Loss 0.768814                                        LR 0.001000    Time 0.056126    
2024-06-05 23:55:00,235 - Epoch: [15][  600/ 1218]    Overall Loss 0.765739    Objective Loss 0.765739                                        LR 0.001000    Time 0.055264    
2024-06-05 23:55:05,805 - Epoch: [15][  700/ 1218]    Overall Loss 0.765299    Objective Loss 0.765299                                        LR 0.001000    Time 0.055322    
2024-06-05 23:55:11,130 - Epoch: [15][  800/ 1218]    Overall Loss 0.763583    Objective Loss 0.763583                                        LR 0.001000    Time 0.055061    
2024-06-05 23:55:16,295 - Epoch: [15][  900/ 1218]    Overall Loss 0.764726    Objective Loss 0.764726                                        LR 0.001000    Time 0.054678    
2024-06-05 23:55:21,297 - Epoch: [15][ 1000/ 1218]    Overall Loss 0.763548    Objective Loss 0.763548                                        LR 0.001000    Time 0.054210    
2024-06-05 23:55:26,648 - Epoch: [15][ 1100/ 1218]    Overall Loss 0.765628    Objective Loss 0.765628                                        LR 0.001000    Time 0.054144    
2024-06-05 23:55:31,864 - Epoch: [15][ 1200/ 1218]    Overall Loss 0.766060    Objective Loss 0.766060                                        LR 0.001000    Time 0.053977    
2024-06-05 23:55:32,745 - Epoch: [15][ 1218/ 1218]    Overall Loss 0.765665    Objective Loss 0.765665    Top1 66.259169    Top5 95.110024    LR 0.001000    Time 0.053902    
2024-06-05 23:55:32,961 - --- validate (epoch=15)-----------
2024-06-05 23:55:32,961 - 34633 samples (256 per mini-batch)
2024-06-05 23:55:39,568 - Epoch: [15][  100/  136]    Loss 0.719106    Top1 66.507812    Top5 92.765625    
2024-06-05 23:55:41,440 - Epoch: [15][  136/  136]    Loss 0.713396    Top1 66.832212    Top5 92.821875    
2024-06-05 23:55:41,659 - ==> Top1: 66.832    Top5: 92.822    Loss: 0.713

2024-06-05 23:55:41,660 - ==> Confusion:
[[ 738    0    2    3   14    9    1    5   23   79    4    5    2   10    5    5    5    4    1    1   15]
 [   7  791    2    0   21   86   10   35   11    2   13    5    7    1   20    1   12    5    9   15   10]
 [  17    5  663   30   15   15   81   18    5    5   22    7    8    6    3    7   11    3    9   21   19]
 [   3    0   14  818    8   18   11    5    5    3   32    6   15    4   27    1    2   14   14    3   13]
 [  29   19    3    1  867   29    0    4    6   14    7    4    2    4   17    7   10    1    9    4   17]
 [   9   31    1    6   11  827    6   34    4    6    1   27   14   19    3    2    6    9    3   16    8]
 [   1    3   21    8    1    9  938   12    2    1   13    7    7    1    0   11    6    6    3   24   12]
 [   4   13   14    9    5   69    9  795    2    2    7   31   12    7    4    1    3    3   41   38    8]
 [  15    5    0    3    1   12    0    0  798   48   26    6   13   27   21    0    2    8   10    5    2]
 [ 110    1    2    2   12    4    0    2   73  713    2    2    3   34   15    3    3    8    0    1   11]
 [   4    7    9   14    3   18    6    8   19    1  903    2    7   21    4    1    4    1    8   10   14]
 [   1    0    0    0    0   16    6    5    2    1    0  798   71   13    2   18    3   35    2   31    7]
 [   1    1    2    6    0    5    4    6    0    1    0  106  760    3    2    7    5   52    7   17   10]
 [   3    1    1    1    6   33    1    3   12   19    6   24   13  818   12    4    7   11    2   12   12]
 [  20    6    3   26   13    7    0    0   59   14   12    3    7    6  875    0    3    8   11    5   20]
 [   2    1    2    2    7   10   10    0    1    1    1   39   12    1    0  915   14   27    2   10    9]
 [   3   12    2    3   14   24    8    3    8    0    1   20    8    6    5   10  903    5    5   11   21]
 [   1    0    0    9    0    3    2    0    4    1    1   35   55    3    3   13    2  857    1    7    8]
 [   3    8    9   39    2   10    1   40   16    3   18    6   11    1   35    1    3    9  821    9   13]
 [   1    4    2    2    1   19   12   11    3    1    0   42   10    6    1    6    6   13    3  934   11]
 [ 427  286  203  248  375  545  181  267  223  197  375  406  651  461  475  244  536  262  260  696 6614]]

2024-06-05 23:55:41,662 - ==> Best [Top1: 66.835   Top5: 93.731   Sparsity:0.00   Params: 169472 on epoch: 13]
2024-06-05 23:55:41,662 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-05 23:55:41,673 - 

2024-06-05 23:55:41,673 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-05 23:55:48,660 - Epoch: [16][  100/ 1218]    Overall Loss 0.759123    Objective Loss 0.759123                                        LR 0.001000    Time 0.069837    
2024-06-05 23:55:53,919 - Epoch: [16][  200/ 1218]    Overall Loss 0.767665    Objective Loss 0.767665                                        LR 0.001000    Time 0.061202    
2024-06-05 23:55:59,245 - Epoch: [16][  300/ 1218]    Overall Loss 0.765499    Objective Loss 0.765499                                        LR 0.001000    Time 0.058547    
2024-06-05 23:56:04,506 - Epoch: [16][  400/ 1218]    Overall Loss 0.766904    Objective Loss 0.766904                                        LR 0.001000    Time 0.057057    
2024-06-05 23:56:09,635 - Epoch: [16][  500/ 1218]    Overall Loss 0.763690    Objective Loss 0.763690                                        LR 0.001000    Time 0.055899    
2024-06-05 23:56:15,050 - Epoch: [16][  600/ 1218]    Overall Loss 0.761576    Objective Loss 0.761576                                        LR 0.001000    Time 0.055603    
2024-06-05 23:56:20,367 - Epoch: [16][  700/ 1218]    Overall Loss 0.762182    Objective Loss 0.762182                                        LR 0.001000    Time 0.055252    
2024-06-05 23:56:25,618 - Epoch: [16][  800/ 1218]    Overall Loss 0.762708    Objective Loss 0.762708                                        LR 0.001000    Time 0.054907    
2024-06-05 23:56:30,767 - Epoch: [16][  900/ 1218]    Overall Loss 0.762550    Objective Loss 0.762550                                        LR 0.001000    Time 0.054524    
2024-06-05 23:56:36,127 - Epoch: [16][ 1000/ 1218]    Overall Loss 0.760787    Objective Loss 0.760787                                        LR 0.001000    Time 0.054430    
2024-06-05 23:56:41,419 - Epoch: [16][ 1100/ 1218]    Overall Loss 0.759728    Objective Loss 0.759728                                        LR 0.001000    Time 0.054291    
2024-06-05 23:56:46,555 - Epoch: [16][ 1200/ 1218]    Overall Loss 0.760399    Objective Loss 0.760399                                        LR 0.001000    Time 0.054044    
2024-06-05 23:56:47,670 - Epoch: [16][ 1218/ 1218]    Overall Loss 0.760387    Objective Loss 0.760387    Top1 62.836186    Top5 91.931540    LR 0.001000    Time 0.054161    
2024-06-05 23:56:47,894 - --- validate (epoch=16)-----------
2024-06-05 23:56:47,894 - 34633 samples (256 per mini-batch)
2024-06-05 23:56:54,196 - Epoch: [16][  100/  136]    Loss 0.698729    Top1 66.957031    Top5 93.078125    
2024-06-05 23:56:56,033 - Epoch: [16][  136/  136]    Loss 0.703042    Top1 66.858199    Top5 92.957584    
2024-06-05 23:56:56,295 - ==> Top1: 66.858    Top5: 92.958    Loss: 0.703

2024-06-05 23:56:56,297 - ==> Confusion:
[[ 761    3   13    2   13    3    2    4    2   92    1    5    0    5    5    4    1    0    7    1    7]
 [   3  869    4    1   39   41    6   19    3    5    1    9    3    2   10    1   14    3   17    5    8]
 [  17    4  784   12    7    5   56   20    2    6    6    8    0    7    1    9    4    1    8    5    8]
 [   8    2   43  781    7   14   11    2    2    8   23    4    9    2   27    5    3    8   41    2   14]
 [  36   16   10    1  867   13    8    4    1   25    3    6    0    7   15   12   15    1    8    0    6]
 [   9   49   10    7   19  740    4   44    0    6    2   30    7   51    5    7    4    6   16   17   10]
 [   3    9   51    0    1    3  942    5    0    1    5    8    4    1    1   14    1    4    4   16   13]
 [   2   30   37    2    4   37   13  806    4    3    2   23    8    8    5    2    1    0   51   27   12]
 [  23    7    3    1    5    6    2    1  723  101   10    5    9   31   37    0    2    2   14    5   15]
 [  93    2    5    1    6    0    1    4   33  788    1    1    0   33   11    0    0    6    6    0   10]
 [   5    9   28   19    9    9   11    7   31    4  850    2    5   24   10    0    1    0   26    4   10]
 [   6    4    2    0    2   14    7    4    1    1    1  810   36   21    1   25    7   32    5   20   12]
 [   5    0    4    8    3    1    6    1    3    1    2  107  736   11    3   11    3   54   16    9   11]
 [   8    2    1    1    4   18    1    1   11   27    7   22    6  844    9    4    5    6    4   12    8]
 [  29    9    2   21   23    5    1    3   39   21   10    1    7   11  864    0    3    4   34    1   10]
 [   6    3    4    0    4    1   17    0    0    1    0   26   13    1    3  933    9   23    6    7    9]
 [   6   17   15    1    8   15    3    3    5    3    1   15    4    6    3   21  902    6    7   11   20]
 [   6    3    2    0    2    3    2    2    0    1    1   34   40   10    6   22    2  853    6    2    8]
 [   5   11   15   18    7    5    6   37    5    2    4    8    8    0   22    1    0    2  889    2   11]
 [   1   10    6    0    3    8   18   14    0    2    0   41   14    9    0    8   13    6    3  915   17]
 [ 505  394  479  194  472  321  208  313  186  210  239  368  588  634  399  329  362  249  478  506 6498]]

2024-06-05 23:56:56,298 - ==> Best [Top1: 66.858   Top5: 92.958   Sparsity:0.00   Params: 169472 on epoch: 16]
2024-06-05 23:56:56,299 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-05 23:56:56,312 - 

2024-06-05 23:56:56,312 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-05 23:57:03,053 - Epoch: [17][  100/ 1218]    Overall Loss 0.761328    Objective Loss 0.761328                                        LR 0.001000    Time 0.067372    
2024-06-05 23:57:08,248 - Epoch: [17][  200/ 1218]    Overall Loss 0.760792    Objective Loss 0.760792                                        LR 0.001000    Time 0.059652    
2024-06-05 23:57:13,364 - Epoch: [17][  300/ 1218]    Overall Loss 0.756272    Objective Loss 0.756272                                        LR 0.001000    Time 0.056813    
2024-06-05 23:57:18,586 - Epoch: [17][  400/ 1218]    Overall Loss 0.754050    Objective Loss 0.754050                                        LR 0.001000    Time 0.055657    
2024-06-05 23:57:23,984 - Epoch: [17][  500/ 1218]    Overall Loss 0.754003    Objective Loss 0.754003                                        LR 0.001000    Time 0.055318    
2024-06-05 23:57:28,852 - Epoch: [17][  600/ 1218]    Overall Loss 0.754592    Objective Loss 0.754592                                        LR 0.001000    Time 0.054208    
2024-06-05 23:57:34,096 - Epoch: [17][  700/ 1218]    Overall Loss 0.752936    Objective Loss 0.752936                                        LR 0.001000    Time 0.053952    
2024-06-05 23:57:39,038 - Epoch: [17][  800/ 1218]    Overall Loss 0.755029    Objective Loss 0.755029                                        LR 0.001000    Time 0.053383    
2024-06-05 23:57:44,316 - Epoch: [17][  900/ 1218]    Overall Loss 0.753496    Objective Loss 0.753496                                        LR 0.001000    Time 0.053312    
2024-06-05 23:57:49,417 - Epoch: [17][ 1000/ 1218]    Overall Loss 0.752332    Objective Loss 0.752332                                        LR 0.001000    Time 0.053080    
2024-06-05 23:57:54,687 - Epoch: [17][ 1100/ 1218]    Overall Loss 0.753386    Objective Loss 0.753386                                        LR 0.001000    Time 0.053043    
2024-06-05 23:58:00,222 - Epoch: [17][ 1200/ 1218]    Overall Loss 0.752553    Objective Loss 0.752553                                        LR 0.001000    Time 0.053234    
2024-06-05 23:58:01,092 - Epoch: [17][ 1218/ 1218]    Overall Loss 0.752429    Objective Loss 0.752429    Top1 69.193154    Top5 93.398533    LR 0.001000    Time 0.053160    
2024-06-05 23:58:01,295 - --- validate (epoch=17)-----------
2024-06-05 23:58:01,296 - 34633 samples (256 per mini-batch)
2024-06-05 23:58:07,866 - Epoch: [17][  100/  136]    Loss 0.686101    Top1 67.332031    Top5 92.710938    
2024-06-05 23:58:09,773 - Epoch: [17][  136/  136]    Loss 0.687660    Top1 67.216239    Top5 92.784339    
2024-06-05 23:58:09,978 - ==> Top1: 67.216    Top5: 92.784    Loss: 0.688

2024-06-05 23:58:09,980 - ==> Confusion:
[[ 734    2    2    0   25    1    1    4   21   86    2    4    1    2   17    6    2    1    7    1   12]
 [   2  848    2    4   34   39    4   17   13    3    3   11    3    2   12    2   10    3   34    9    8]
 [  18    9  719   23   16   10   56   16    3    8   11   13    1    3    9   11   11    4   11    3   15]
 [   6    1   24  791    5   14    5    3    9    2   27    5    9    4   45    3    1   12   36    3   11]
 [  26   14    1    2  890   21    0    2    9   25    1    6    1    3   14    8    9    3    7    2   10]
 [   4   50    3    5   20  763    2   62    7    6    3   24   14   18   13    2    5    6   12   12   12]
 [   3    7   36    7    5   10  940    8    3    0    3    9    3    0    0   10    1    5    6   19   11]
 [   6   19   14    5    9   50    5  818    7    3    6   17    6    3    3    0    3    3   71   19   10]
 [  10    2    2    1    1    3    2    5  816   63   11    3    5   13   24    0    5   10   13    2   11]
 [  88    1    3    1   10    5    1    4   79  749    3    3    1   17   19    1    3    4    3    2    4]
 [   5   11   14   15    4   12    6    7   31    2  867    3    4   11   20    0    2    0   35    3   12]
 [   3    3    5    0    3   18    2    9    4    1    0  808   41   10    0   26    7   31    7   27    6]
 [   1    3    5   12    1    2    0    3    2    0    1  137  691    8    3   13    6   63   10   18   16]
 [   6    3    5    0   12   26    0    7   28   29    8   23    7  787   11    6    7    4    2   17   13]
 [   5    9    1   16   15    1    0    1   49   14    7    3    7    8  926    0    4    3   17    1   11]
 [   4    2    2    1    6    2   13    1    3    1    2   32   12    2    2  935   17   14    0    7    8]
 [   5   13    2    3   14   18    2    2    8    0    2   10    7    4    9   13  921    4    6   12   17]
 [   1    0    0    9    5    2    2    4    4    0    0   44   29   10   11   29    1  835    6    8    5]
 [   7   11    5   12    4    4    1   26   19    2    7    8    6    2   34    0    0    3  897    4    6]
 [   2   15    0    1    4   14   22   18    2    1    1   34    5    6    1    6   12    9    7  921    7]
 [ 409  370  195  204  507  405  162  272  327  148  264  414  508  394  608  278  537  195  463  649 6623]]

2024-06-05 23:58:09,982 - ==> Best [Top1: 67.216   Top5: 92.784   Sparsity:0.00   Params: 169472 on epoch: 17]
2024-06-05 23:58:09,982 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-05 23:58:09,998 - 

2024-06-05 23:58:09,998 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-05 23:58:16,965 - Epoch: [18][  100/ 1218]    Overall Loss 0.721695    Objective Loss 0.721695                                        LR 0.001000    Time 0.069635    
2024-06-05 23:58:22,365 - Epoch: [18][  200/ 1218]    Overall Loss 0.733988    Objective Loss 0.733988                                        LR 0.001000    Time 0.061805    
2024-06-05 23:58:27,391 - Epoch: [18][  300/ 1218]    Overall Loss 0.741167    Objective Loss 0.741167                                        LR 0.001000    Time 0.057949    
2024-06-05 23:58:32,800 - Epoch: [18][  400/ 1218]    Overall Loss 0.743657    Objective Loss 0.743657                                        LR 0.001000    Time 0.056977    
2024-06-05 23:58:38,583 - Epoch: [18][  500/ 1218]    Overall Loss 0.745917    Objective Loss 0.745917                                        LR 0.001000    Time 0.057143    
2024-06-05 23:58:43,482 - Epoch: [18][  600/ 1218]    Overall Loss 0.744981    Objective Loss 0.744981                                        LR 0.001000    Time 0.055781    
2024-06-05 23:58:48,990 - Epoch: [18][  700/ 1218]    Overall Loss 0.747449    Objective Loss 0.747449                                        LR 0.001000    Time 0.055677    
2024-06-05 23:58:54,311 - Epoch: [18][  800/ 1218]    Overall Loss 0.750681    Objective Loss 0.750681                                        LR 0.001000    Time 0.055366    
2024-06-05 23:58:59,883 - Epoch: [18][  900/ 1218]    Overall Loss 0.750741    Objective Loss 0.750741                                        LR 0.001000    Time 0.055403    
2024-06-05 23:59:05,248 - Epoch: [18][ 1000/ 1218]    Overall Loss 0.749587    Objective Loss 0.749587                                        LR 0.001000    Time 0.055225    
2024-06-05 23:59:10,439 - Epoch: [18][ 1100/ 1218]    Overall Loss 0.750396    Objective Loss 0.750396                                        LR 0.001000    Time 0.054921    
2024-06-05 23:59:15,893 - Epoch: [18][ 1200/ 1218]    Overall Loss 0.750587    Objective Loss 0.750587                                        LR 0.001000    Time 0.054888    
2024-06-05 23:59:16,880 - Epoch: [18][ 1218/ 1218]    Overall Loss 0.750940    Objective Loss 0.750940    Top1 65.525672    Top5 93.154034    LR 0.001000    Time 0.054886    
2024-06-05 23:59:17,119 - --- validate (epoch=18)-----------
2024-06-05 23:59:17,119 - 34633 samples (256 per mini-batch)
2024-06-05 23:59:23,683 - Epoch: [18][  100/  136]    Loss 0.690015    Top1 67.851562    Top5 93.890625    
2024-06-05 23:59:25,593 - Epoch: [18][  136/  136]    Loss 0.691400    Top1 67.594491    Top5 93.884445    
2024-06-05 23:59:25,807 - ==> Top1: 67.594    Top5: 93.884    Loss: 0.691

2024-06-05 23:59:25,808 - ==> Confusion:
[[ 779    2    1    3   18    9    1    2    5   53    1    4    0    0   19    7    2    2    2    1   20]
 [   2  812    4    0   37   63    7   29    6    3    9    6    3    3   20    7   19    2   11   10   10]
 [  25    3  680   37   12   18   49   35    1    6   15    3    5   10    6   15   10    3    6   12   19]
 [   8    5   16  789    7   25    5    7    4    3   34    2   10    2   49    6    6    6   16    6   10]
 [  41   16    4    1  875   31    0    6    1   14    3    3    0    3   14    8   10    4    5    1   14]
 [   5   34    2    7   14  822    1   47    4    9    0   19    3   22    8    7   11    2    6   11    9]
 [   2    7   33    8    1   13  920   13    2    2    9    0    2    1    0   18    8    8    6   23   10]
 [   9   19    5    8    4   81    9  816    5    3    7   12    6    2    7    3    4    5   40   23    9]
 [  20    5    1    4    3   12    0    1  767   58   12    4   10   24   52    1    0    1   13    4   10]
 [ 134    1    1    2    9   13    0    7   46  722    2    2    0   23   25    1    2    4    0    1    6]
 [   2    5    6   15    4   13    4   12   24    6  892    1    3   22   22    2    2    1   20    3    5]
 [   2    2    0    0    3   34    2   10    0    1    0  771   53   17    1   30    4   34    3   33   11]
 [   2    3    0    7    0    8    2    6    1    0    1   85  741    9    6   13   12   60   13   15   11]
 [   6    1    2    0    5   38    0    5   20   22   15   11    6  834   13    4    2    5    1    8    3]
 [  14    5    0   16   10    8    0    5   35   13    5    1    6    6  943    0    1    5   10    4   11]
 [   5    3    4    3    7    3    9    1    0    0    1   24   14    5    2  919   17   20    1   11   17]
 [   8   11    2    2   22   20    1    2    5    1    2   11    5    6    8   19  911    5    3    7   21]
 [   3    5    1    7    2    3    2    1    2    2    0   27   50    6    9   21    3  842    2    6   11]
 [   4   14    7   32    3   13    3   43   11    0   14    5    5    3   45    0    1    3  842    2    8]
 [   0    7    3    1    2   27   17   12    1    0    2   28    6    7    3   14   11    3    5  927   12]
 [ 478  370  176  239  427  611   95  276  144  153  268  239  574  423  700  333  567  198  257  599 6805]]

2024-06-05 23:59:25,810 - ==> Best [Top1: 67.594   Top5: 93.884   Sparsity:0.00   Params: 169472 on epoch: 18]
2024-06-05 23:59:25,810 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-05 23:59:25,830 - 

2024-06-05 23:59:25,830 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-05 23:59:32,537 - Epoch: [19][  100/ 1218]    Overall Loss 0.748316    Objective Loss 0.748316                                        LR 0.001000    Time 0.067042    
2024-06-05 23:59:37,754 - Epoch: [19][  200/ 1218]    Overall Loss 0.749132    Objective Loss 0.749132                                        LR 0.001000    Time 0.059591    
2024-06-05 23:59:43,025 - Epoch: [19][  300/ 1218]    Overall Loss 0.752413    Objective Loss 0.752413                                        LR 0.001000    Time 0.057290    
2024-06-05 23:59:48,229 - Epoch: [19][  400/ 1218]    Overall Loss 0.750817    Objective Loss 0.750817                                        LR 0.001000    Time 0.055963    
2024-06-05 23:59:53,462 - Epoch: [19][  500/ 1218]    Overall Loss 0.750374    Objective Loss 0.750374                                        LR 0.001000    Time 0.055228    
2024-06-05 23:59:58,371 - Epoch: [19][  600/ 1218]    Overall Loss 0.747326    Objective Loss 0.747326                                        LR 0.001000    Time 0.054200    
2024-06-06 00:00:03,720 - Epoch: [19][  700/ 1218]    Overall Loss 0.746743    Objective Loss 0.746743                                        LR 0.001000    Time 0.054095    
2024-06-06 00:00:08,887 - Epoch: [19][  800/ 1218]    Overall Loss 0.746290    Objective Loss 0.746290                                        LR 0.001000    Time 0.053790    
2024-06-06 00:00:14,105 - Epoch: [19][  900/ 1218]    Overall Loss 0.746704    Objective Loss 0.746704                                        LR 0.001000    Time 0.053608    
2024-06-06 00:00:19,253 - Epoch: [19][ 1000/ 1218]    Overall Loss 0.746998    Objective Loss 0.746998                                        LR 0.001000    Time 0.053393    
2024-06-06 00:00:24,352 - Epoch: [19][ 1100/ 1218]    Overall Loss 0.747064    Objective Loss 0.747064                                        LR 0.001000    Time 0.053172    
2024-06-06 00:00:29,434 - Epoch: [19][ 1200/ 1218]    Overall Loss 0.745866    Objective Loss 0.745866                                        LR 0.001000    Time 0.052974    
2024-06-06 00:00:30,293 - Epoch: [19][ 1218/ 1218]    Overall Loss 0.745681    Objective Loss 0.745681    Top1 64.547677    Top5 92.665037    LR 0.001000    Time 0.052896    
2024-06-06 00:00:30,510 - --- validate (epoch=19)-----------
2024-06-06 00:00:30,510 - 34633 samples (256 per mini-batch)
2024-06-06 00:00:36,972 - Epoch: [19][  100/  136]    Loss 0.689827    Top1 67.484375    Top5 93.464844    
2024-06-06 00:00:38,868 - Epoch: [19][  136/  136]    Loss 0.684629    Top1 67.582941    Top5 93.529293    
2024-06-06 00:00:39,061 - ==> Top1: 67.583    Top5: 93.529    Loss: 0.685

2024-06-06 00:00:39,062 - ==> Confusion:
[[ 699    0    2    3   13    3    0    4   20  137    0    2    0    5    7    2    4    2   10    2   16]
 [   3  891    4    1   19   39    4   27   10    3    4    9    2    1    6    0    6    2   14    8   10]
 [   7    4  749   23    8    3   26   26    2   11   10   10    1    7    7    9   12    5   19   10   21]
 [  11    3   29  795    2    9    2    5   11    8   23    3    9    2   34    2    5    5   49    4    5]
 [  39   16    1    3  883   20    0    4    4   22    3    3    0    3   19    4    6    0   11    4    9]
 [   2   60    3    6   24  747    3   82    8    8    2   25    7   12    3    1    8    7   15   11    9]
 [   0    7   36    6    5    7  914   22    3    1   11    7    1    0    0   15    7    6    8   18   12]
 [   2   22   12    3    3   38    5  843    7    5    3   17    4    2    5    1    1    1   69   21   13]
 [  10    5    1    0    1    3    1    7  774   86   12    4    4   28   31    0    0    3   16    4   12]
 [  60    1    0    3    9    3    0   12   53  793    2    1    0   25   12    1    1    2    5    3   15]
 [   3   11    8   17    7   10    5   11   35    6  871    0    1   15    8    0    4    2   36    3   11]
 [   1    3    4    0    4   14    1    7    3    1    1  827   34   15    1   12    8   29    9   26   11]
 [   0    2    0    6    0    7    2    4    5    0    1  132  707    3    5    9    9   59   11   17   16]
 [   2    4    2    0    8   30    0    8   26   34    8   12    2  821    8    4    4    4    5    8   11]
 [  18   10    2   32   15    0    0    0   45   21    4    1    4    8  879    0    2    8   34    2   13]
 [   1    4    7    4    9    4   13    1    0    6    1   43   12    2    0  888   25   27    3   10    6]
 [   4   13    3    2   21   10    0    3   14    3    1   14    2    7    7   13  917    5    9   12   12]
 [   1    0    0    4    1    1    2    1    7    3    1   35   32    7    4   15    4  875    5    2    5]
 [   2   15    5   14    3    8    1   30   11    1   12    4    5    0   24    1    3    3  907    5    4]
 [   2   13    1    2    3   19   15   26    4    1    4   57   10    9    1    8   11    3   11  881    7]
 [ 347  469  238  208  418  343  119  349  274  260  253  395  479  445  444  200  576  253  541  576 6745]]

2024-06-06 00:00:39,064 - ==> Best [Top1: 67.594   Top5: 93.884   Sparsity:0.00   Params: 169472 on epoch: 18]
2024-06-06 00:00:39,064 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:00:39,072 - 

2024-06-06 00:00:39,072 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:00:46,184 - Epoch: [20][  100/ 1218]    Overall Loss 0.736184    Objective Loss 0.736184                                        LR 0.001000    Time 0.071091    
2024-06-06 00:00:51,259 - Epoch: [20][  200/ 1218]    Overall Loss 0.739811    Objective Loss 0.739811                                        LR 0.001000    Time 0.060911    
2024-06-06 00:00:56,447 - Epoch: [20][  300/ 1218]    Overall Loss 0.738049    Objective Loss 0.738049                                        LR 0.001000    Time 0.057871    
2024-06-06 00:01:01,739 - Epoch: [20][  400/ 1218]    Overall Loss 0.737495    Objective Loss 0.737495                                        LR 0.001000    Time 0.056627    
2024-06-06 00:01:07,010 - Epoch: [20][  500/ 1218]    Overall Loss 0.737479    Objective Loss 0.737479                                        LR 0.001000    Time 0.055839    
2024-06-06 00:01:12,078 - Epoch: [20][  600/ 1218]    Overall Loss 0.737425    Objective Loss 0.737425                                        LR 0.001000    Time 0.054976    
2024-06-06 00:01:17,236 - Epoch: [20][  700/ 1218]    Overall Loss 0.734673    Objective Loss 0.734673                                        LR 0.001000    Time 0.054487    
2024-06-06 00:01:22,274 - Epoch: [20][  800/ 1218]    Overall Loss 0.735056    Objective Loss 0.735056                                        LR 0.001000    Time 0.053971    
2024-06-06 00:01:27,346 - Epoch: [20][  900/ 1218]    Overall Loss 0.735094    Objective Loss 0.735094                                        LR 0.001000    Time 0.053607    
2024-06-06 00:01:32,631 - Epoch: [20][ 1000/ 1218]    Overall Loss 0.735099    Objective Loss 0.735099                                        LR 0.001000    Time 0.053529    
2024-06-06 00:01:37,614 - Epoch: [20][ 1100/ 1218]    Overall Loss 0.734876    Objective Loss 0.734876                                        LR 0.001000    Time 0.053191    
2024-06-06 00:01:42,765 - Epoch: [20][ 1200/ 1218]    Overall Loss 0.736428    Objective Loss 0.736428                                        LR 0.001000    Time 0.053049    
2024-06-06 00:01:43,608 - Epoch: [20][ 1218/ 1218]    Overall Loss 0.736271    Objective Loss 0.736271    Top1 66.992665    Top5 95.110024    LR 0.001000    Time 0.052956    
2024-06-06 00:01:43,811 - --- validate (epoch=20)-----------
2024-06-06 00:01:43,811 - 34633 samples (256 per mini-batch)
2024-06-06 00:01:49,842 - Epoch: [20][  100/  136]    Loss 0.663550    Top1 68.953125    Top5 94.097656    
2024-06-06 00:01:51,790 - Epoch: [20][  136/  136]    Loss 0.665270    Top1 68.966015    Top5 94.049028    
2024-06-06 00:01:51,978 - ==> Top1: 68.966    Top5: 94.049    Loss: 0.665

2024-06-06 00:01:51,980 - ==> Confusion:
[[ 769    0    8    6   10    3    0    2    9   84    0    2    2   10    5    1    5    3    4    0    8]
 [   6  823    5    3   29   63    8   29   10    1    7    7    2    7   10    3    4    2   11   11   22]
 [  14    2  740   21    3    4   57   24    1    8   11   14    4    8    7    7   10    4   11   11    9]
 [   3    3   18  829    4   20    4    5    6    4   30    2   10   11   31    3    2    1   18    3    9]
 [  32   10    6    4  881   17    6    5    3   19    5    6    1    9   14    6    9    2    7    3    9]
 [   5   27    4    8   14  796    4   41    7   10    6   34    9   32    5    3    7    4    5   11   11]
 [   5    7   30    7    2    9  902   10    1    1    5    9    3    3    1   24    2   11    7   37   10]
 [   3   15   13    7    2   66    5  848    1    2   13   15    9    9    4    0    2    3   34   20    6]
 [  16    2    1    0    0    8    0    3  772   76   20    5    4   27   45    2    4    3    9    1    4]
 [ 100    1    4    0    8    5    0    3   47  767    2    3    2   25   21    0    2    1    1    0    9]
 [   4    6    7   13    2    7    4   10   20    2  916    2    2   25    7    2    3    2   20    4    6]
 [   5    2    1    1    1   26    3    3    1    0    2  840   38   15    2   16    4   26    2   16    7]
 [   1    0    2   14    0    9    1    0    8    0    2  107  714    7    6   13   16   61    7   11   16]
 [   7    2    4    2    8   12    1    7   24   33    9   18    5  824    4    4    6    7    5    7   12]
 [  22    2    2   34    6    6    1    2   39   18   10    1    2   14  896    1    3    8   19    0   12]
 [   3    2    7    2    3    4   12    0    1    2    0   29   13    3    2  938   11   21    2    5    6]
 [  11    8    3    6    8   19    2    3    4    3    3   11    5    9    3   18  918    5    4    8   21]
 [   3    0    1    8    0    4    2    5    3    4    1   32   37    5    8   10    5  863    4    2    8]
 [   3    6   12   39    1    7    0   45   14    2   14    8    8    3   29    0    3    3  850    2    9]
 [   2    4    1    3    0   19   10   19    2    0    3   63   18    8    3   10   21    6    5  870   21]
 [ 409  218  334  327  302  357  135  306  200  191  299  357  516  572  438  257  562  209  319  495 7129]]

2024-06-06 00:01:51,981 - ==> Best [Top1: 68.966   Top5: 94.049   Sparsity:0.00   Params: 169472 on epoch: 20]
2024-06-06 00:01:51,981 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:01:52,000 - 

2024-06-06 00:01:52,000 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:01:58,555 - Epoch: [21][  100/ 1218]    Overall Loss 0.744218    Objective Loss 0.744218                                        LR 0.001000    Time 0.065520    
2024-06-06 00:02:03,848 - Epoch: [21][  200/ 1218]    Overall Loss 0.728463    Objective Loss 0.728463                                        LR 0.001000    Time 0.059212    
2024-06-06 00:02:09,124 - Epoch: [21][  300/ 1218]    Overall Loss 0.728182    Objective Loss 0.728182                                        LR 0.001000    Time 0.057054    
2024-06-06 00:02:14,169 - Epoch: [21][  400/ 1218]    Overall Loss 0.728052    Objective Loss 0.728052                                        LR 0.001000    Time 0.055399    
2024-06-06 00:02:19,443 - Epoch: [21][  500/ 1218]    Overall Loss 0.727849    Objective Loss 0.727849                                        LR 0.001000    Time 0.054862    
2024-06-06 00:02:24,704 - Epoch: [21][  600/ 1218]    Overall Loss 0.727504    Objective Loss 0.727504                                        LR 0.001000    Time 0.054482    
2024-06-06 00:02:29,785 - Epoch: [21][  700/ 1218]    Overall Loss 0.728418    Objective Loss 0.728418                                        LR 0.001000    Time 0.053955    
2024-06-06 00:02:34,961 - Epoch: [21][  800/ 1218]    Overall Loss 0.727702    Objective Loss 0.727702                                        LR 0.001000    Time 0.053677    
2024-06-06 00:02:39,975 - Epoch: [21][  900/ 1218]    Overall Loss 0.728970    Objective Loss 0.728970                                        LR 0.001000    Time 0.053282    
2024-06-06 00:02:44,904 - Epoch: [21][ 1000/ 1218]    Overall Loss 0.728833    Objective Loss 0.728833                                        LR 0.001000    Time 0.052880    
2024-06-06 00:02:49,913 - Epoch: [21][ 1100/ 1218]    Overall Loss 0.728425    Objective Loss 0.728425                                        LR 0.001000    Time 0.052624    
2024-06-06 00:02:55,019 - Epoch: [21][ 1200/ 1218]    Overall Loss 0.728253    Objective Loss 0.728253                                        LR 0.001000    Time 0.052492    
2024-06-06 00:02:55,869 - Epoch: [21][ 1218/ 1218]    Overall Loss 0.728161    Objective Loss 0.728161    Top1 64.303178    Top5 95.354523    LR 0.001000    Time 0.052414    
2024-06-06 00:02:56,069 - --- validate (epoch=21)-----------
2024-06-06 00:02:56,069 - 34633 samples (256 per mini-batch)
2024-06-06 00:03:02,279 - Epoch: [21][  100/  136]    Loss 0.665847    Top1 69.742188    Top5 94.804688    
2024-06-06 00:03:04,149 - Epoch: [21][  136/  136]    Loss 0.665400    Top1 69.592585    Top5 94.782433    
2024-06-06 00:03:04,398 - ==> Top1: 69.593    Top5: 94.782    Loss: 0.665

2024-06-06 00:03:04,400 - ==> Confusion:
[[ 733    0   13    2   19    0    1    4   14  100    1    2    0    5    9    3    1    4    5    0   15]
 [   2  826    7    6   30   48    5   20   15    2    6    7    4    5   19    8   10    3   20    4   16]
 [   9    3  803   10    8    2   22   18    2   12   13    4    3    7    3   11    4    4   10    3   19]
 [   4    1   45  803    3    5    3    3    8    4   37    2    8    5   29    4    3    6   19    4   20]
 [  33   15    9    0  891    9    2    1    6   22    5    3    0    8   12   11    7    0    5    1   14]
 [   3   30    5   10   23  753    2   51   12   11    5   16    9   49    3    9    6    6    6   22   12]
 [   3    3   55    7    4    6  891   13    2    5    9    7    5    3    0   18    4    8    4   27   12]
 [   3   16   30    2    8   52    3  808    7    4    8   15    7    9    3    1    2    4   54   15   26]
 [  11    6    0    1    1    0    1    3  774   70   19    5    5   27   46    0    3    4   14    1   11]
 [  89    2    0    0    7    1    2    1   57  775    8    3    0   22   12    3    1    1    4    2   11]
 [   0    2   17   16    3    3    0   12   27    2  907    1    3   26    9    1    0    0   15    2   18]
 [   5    0    3    0    1   14    1    9    6    0    1  772   38   34    2   18    3   55    6   31   12]
 [   2    2    3    8    0    9    1    4    8    1    3   92  722    7    5    5    4   84   12   11   12]
 [   6    1    1    2    4   12    0    3   22   30    8   10    5  852    6    6    4    5    5    3   16]
 [  13    1    5   26   12    1    0    1   37   23   16    3    4    7  909    1    1    6   14    1   17]
 [   4    1    5    0    3    1    7    0    2    1    0   33   12    6    1  921   11   45    4    1    8]
 [   7   10   12    2   19   12    4    1   11    4    2   12    4    8    3   21  890    4    4   19   23]
 [   2    0    0    4    1    1    1    3    3    2    1   19   29    9    8   14    2  891    5    3    7]
 [   5    5   14   21    4    4    2   26   15    1   14    2    2    4   33    2    2    2  881    3   16]
 [   3    6    8    2    1    8   14   14    2    2    1   37    6   16    4   10    8    6    9  914   17]
 [ 398  235  396  192  406  277  120  215  243  253  294  290  481  549  416  332  346  323  328  452 7386]]

2024-06-06 00:03:04,401 - ==> Best [Top1: 69.593   Top5: 94.782   Sparsity:0.00   Params: 169472 on epoch: 21]
2024-06-06 00:03:04,402 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:03:04,417 - 

2024-06-06 00:03:04,417 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:03:11,016 - Epoch: [22][  100/ 1218]    Overall Loss 0.704790    Objective Loss 0.704790                                        LR 0.001000    Time 0.065953    
2024-06-06 00:03:16,236 - Epoch: [22][  200/ 1218]    Overall Loss 0.721720    Objective Loss 0.721720                                        LR 0.001000    Time 0.059056    
2024-06-06 00:03:21,423 - Epoch: [22][  300/ 1218]    Overall Loss 0.721844    Objective Loss 0.721844                                        LR 0.001000    Time 0.056651    
2024-06-06 00:03:26,597 - Epoch: [22][  400/ 1218]    Overall Loss 0.724352    Objective Loss 0.724352                                        LR 0.001000    Time 0.055419    
2024-06-06 00:03:31,971 - Epoch: [22][  500/ 1218]    Overall Loss 0.724075    Objective Loss 0.724075                                        LR 0.001000    Time 0.055078    
2024-06-06 00:03:36,999 - Epoch: [22][  600/ 1218]    Overall Loss 0.726772    Objective Loss 0.726772                                        LR 0.001000    Time 0.054274    
2024-06-06 00:03:42,099 - Epoch: [22][  700/ 1218]    Overall Loss 0.727215    Objective Loss 0.727215                                        LR 0.001000    Time 0.053803    
2024-06-06 00:03:47,167 - Epoch: [22][  800/ 1218]    Overall Loss 0.726421    Objective Loss 0.726421                                        LR 0.001000    Time 0.053410    
2024-06-06 00:03:52,254 - Epoch: [22][  900/ 1218]    Overall Loss 0.724412    Objective Loss 0.724412                                        LR 0.001000    Time 0.053125    
2024-06-06 00:03:57,175 - Epoch: [22][ 1000/ 1218]    Overall Loss 0.723997    Objective Loss 0.723997                                        LR 0.001000    Time 0.052731    
2024-06-06 00:04:02,307 - Epoch: [22][ 1100/ 1218]    Overall Loss 0.723144    Objective Loss 0.723144                                        LR 0.001000    Time 0.052601    
2024-06-06 00:04:07,551 - Epoch: [22][ 1200/ 1218]    Overall Loss 0.722852    Objective Loss 0.722852                                        LR 0.001000    Time 0.052586    
2024-06-06 00:04:08,358 - Epoch: [22][ 1218/ 1218]    Overall Loss 0.723294    Objective Loss 0.723294    Top1 62.591687    Top5 90.953545    LR 0.001000    Time 0.052470    
2024-06-06 00:04:08,585 - --- validate (epoch=22)-----------
2024-06-06 00:04:08,585 - 34633 samples (256 per mini-batch)
2024-06-06 00:04:14,706 - Epoch: [22][  100/  136]    Loss 0.667616    Top1 68.000000    Top5 93.796875    
2024-06-06 00:04:16,484 - Epoch: [22][  136/  136]    Loss 0.664033    Top1 67.935206    Top5 93.887333    
2024-06-06 00:04:16,692 - ==> Top1: 67.935    Top5: 93.887    Loss: 0.664

2024-06-06 00:04:16,693 - ==> Confusion:
[[ 787    2    3    1   12    0    1    3   10   80    0    3    0    3    9    1    6    0    2    1    7]
 [   7  865    5    3   29   29    2   40   15    7    4    2    1    2   16    0    6    4   12    7    7]
 [  17    3  769   23   11    3   39   32    2    7    8    6    2    5    4    7    7    0   12    5    8]
 [   3    5   25  823    8   12    3    9    4    3   20    3    4    3   42    0    1    7   33    1    7]
 [  36   16    3    4  904    8    1    6    4   24    1    6    2    2   10    4    5    1    7    1    9]
 [   8   36    6    6   19  774    6   81    8    8    3   17    8   23    5    1    3    1    9   12    9]
 [   3    6   35    3    4    9  946   24    3    2    3    5    0    1    0   10    5    0    4   16    7]
 [   7   10   13    4    8   34    6  896    5    3    7   16    2    1    3    0    1    0   42   14    5]
 [  21    4    0    4    1    2    0    5  774   81   13    4    5   33   25    0    7    2   12    2    7]
 [ 134    1    0    0    4    2    0    1   50  774    1    1    2   14   11    0    1    1    1    2    1]
 [   5    5   15   26   12   10    6   18   26    5  847    4    0   11   16    0    2    0   41    5   10]
 [   8    2    2    1    0   21    3   21    0    5    0  813   22   20    1   16    2   16    4   45    9]
 [   1    1    1   11    1    6    5   11   11    1    2  121  704   17    3    5    7   48   10   18   11]
 [   8    2    6    0    8   10    4   13   23   38    6   19    2  815    7    3    3    7    1   21    5]
 [  28   11    3   18   19    2    0    3   52   21    5    2    2    5  876    0    5    5   25    3   13]
 [   4    4   10    3    8    4   14    1    1    4    0   32    8    7    1  906   16   23    1    8   11]
 [  13   17    4    1   14   10    0    4   10    4    2   14    7    2    5    8  922    5    1    9   20]
 [   2    2    4    6    0    6   10    5    4    4    0   45   31    8    5   18    7  822    9    8    9]
 [   4   10   13   18    8    6    3   55    9    2    4    3    2    3   18    0    3    1  880    7    9]
 [   1   17    5    1    2   13   28   35    0    2    0   25    4    1    2    5   10    4   11  915    7]
 [ 545  354  344  220  488  359  180  437  257  259  199  332  408  504  406  202  589  138  420  575 6716]]

2024-06-06 00:04:16,694 - ==> Best [Top1: 69.593   Top5: 94.782   Sparsity:0.00   Params: 169472 on epoch: 21]
2024-06-06 00:04:16,695 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:04:16,702 - 

2024-06-06 00:04:16,702 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:04:23,759 - Epoch: [23][  100/ 1218]    Overall Loss 0.710488    Objective Loss 0.710488                                        LR 0.001000    Time 0.070539    
2024-06-06 00:04:28,884 - Epoch: [23][  200/ 1218]    Overall Loss 0.713569    Objective Loss 0.713569                                        LR 0.001000    Time 0.060881    
2024-06-06 00:04:34,008 - Epoch: [23][  300/ 1218]    Overall Loss 0.716124    Objective Loss 0.716124                                        LR 0.001000    Time 0.057660    
2024-06-06 00:04:38,972 - Epoch: [23][  400/ 1218]    Overall Loss 0.719406    Objective Loss 0.719406                                        LR 0.001000    Time 0.055651    
2024-06-06 00:04:44,115 - Epoch: [23][  500/ 1218]    Overall Loss 0.721811    Objective Loss 0.721811                                        LR 0.001000    Time 0.054801    
2024-06-06 00:04:49,215 - Epoch: [23][  600/ 1218]    Overall Loss 0.722298    Objective Loss 0.722298                                        LR 0.001000    Time 0.054165    
2024-06-06 00:04:54,283 - Epoch: [23][  700/ 1218]    Overall Loss 0.724432    Objective Loss 0.724432                                        LR 0.001000    Time 0.053663    
2024-06-06 00:04:59,392 - Epoch: [23][  800/ 1218]    Overall Loss 0.722898    Objective Loss 0.722898                                        LR 0.001000    Time 0.053338    
2024-06-06 00:05:04,418 - Epoch: [23][  900/ 1218]    Overall Loss 0.723162    Objective Loss 0.723162                                        LR 0.001000    Time 0.052993    
2024-06-06 00:05:09,450 - Epoch: [23][ 1000/ 1218]    Overall Loss 0.722506    Objective Loss 0.722506                                        LR 0.001000    Time 0.052724    
2024-06-06 00:05:14,782 - Epoch: [23][ 1100/ 1218]    Overall Loss 0.722475    Objective Loss 0.722475                                        LR 0.001000    Time 0.052776    
2024-06-06 00:05:19,844 - Epoch: [23][ 1200/ 1218]    Overall Loss 0.722819    Objective Loss 0.722819                                        LR 0.001000    Time 0.052594    
2024-06-06 00:05:20,713 - Epoch: [23][ 1218/ 1218]    Overall Loss 0.722820    Objective Loss 0.722820    Top1 69.682152    Top5 94.132029    LR 0.001000    Time 0.052530    
2024-06-06 00:05:20,941 - --- validate (epoch=23)-----------
2024-06-06 00:05:20,942 - 34633 samples (256 per mini-batch)
2024-06-06 00:05:27,012 - Epoch: [23][  100/  136]    Loss 0.663678    Top1 68.437500    Top5 93.542969    
2024-06-06 00:05:28,988 - Epoch: [23][  136/  136]    Loss 0.664150    Top1 68.405856    Top5 93.558167    
2024-06-06 00:05:29,212 - ==> Top1: 68.406    Top5: 93.558    Loss: 0.664

2024-06-06 00:05:29,213 - ==> Confusion:
[[ 742    2    3    2   17    7    0    6    9   95    0    2    3   10   12    2    1    5    3    2    8]
 [   6  831    6    1   22   87    3   28    6    3    4    5    2    3    7    2   15    8    8    5   11]
 [  21    2  754   23    7    5   29   31    3    8   17    3    7    9    6   14    6    4    8    4    9]
 [   5    1   17  796    5   24    5    8    2    6   34    2   13    7   41    2    1   15   23    4    5]
 [  44   24    3    1  867   34    0    2    3   19    2    2    1    8   13    4   14    2    3    1    7]
 [   8   20    2    2   14  842    2   51    3    4    5   13    8   36    3    1    5    6    4    7    7]
 [   4    7   39    0    1   16  943   11    2    0    4    3    6    3    1   11    2   10    2   12    9]
 [   5   10   11    4    3   54    8  862    1    4   11   12    7   11    4    1    2    7   39   11   10]
 [  16    7    0    1    3    5    1    8  766   74   11    2    9   32   35    1    4    4   14    1    8]
 [ 112    2    0    0    7    5    1    4   48  761    3    1    0   27   14    4    2    3    2    2    3]
 [   2    5    9   16    3   14    8   14   19    8  875    5    4   29   12    0    2    2   28    1    8]
 [   4    4    4    0    2   34    5   11    2    1    0  735   72   34    2   14    7   43    3   26    8]
 [   1    1    3    8    1   13    0    6    5    0    1   75  751   19    3    6    4   66    4   16   12]
 [   3    0    0    0    3   22    2    9   12   27    8    7    5  869    4    3    7    6    0    5    9]
 [  13    9    1   14   21   10    0    2   27   14    7    0    5   11  913    2    4   15   24    2    4]
 [   1    2    8    0    2    5    6    1    0    1    1   26   22    3    1  921   20   27    2    6   11]
 [   4    8    6    4   10   15    3    0    6    1    3    7    4   14    0   14  938    4    2    6   23]
 [   5    3    0    2    2    4    1    3    1    3    1   19   35   12    4   11    1  886    2    5    5]
 [   1   14    7   28    4    9    1   45   11    0    8    0    8    1   37    1    0    3  873    2    5]
 [   3    8    7    2    1   38   16   28    1    0    1   31   17   19    0    5   14    9    4  868   16]
 [ 416  396  253  192  295  594  140  329  164  200  285  235  568  616  525  245  501  285  343  452 6898]]

2024-06-06 00:05:29,215 - ==> Best [Top1: 69.593   Top5: 94.782   Sparsity:0.00   Params: 169472 on epoch: 21]
2024-06-06 00:05:29,216 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:05:29,227 - 

2024-06-06 00:05:29,227 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:05:35,924 - Epoch: [24][  100/ 1218]    Overall Loss 0.702953    Objective Loss 0.702953                                        LR 0.001000    Time 0.066935    
2024-06-06 00:05:41,068 - Epoch: [24][  200/ 1218]    Overall Loss 0.705415    Objective Loss 0.705415                                        LR 0.001000    Time 0.059176    
2024-06-06 00:05:46,198 - Epoch: [24][  300/ 1218]    Overall Loss 0.710904    Objective Loss 0.710904                                        LR 0.001000    Time 0.056533    
2024-06-06 00:05:51,415 - Epoch: [24][  400/ 1218]    Overall Loss 0.709359    Objective Loss 0.709359                                        LR 0.001000    Time 0.055420    
2024-06-06 00:05:56,616 - Epoch: [24][  500/ 1218]    Overall Loss 0.708634    Objective Loss 0.708634                                        LR 0.001000    Time 0.054727    
2024-06-06 00:06:01,496 - Epoch: [24][  600/ 1218]    Overall Loss 0.707928    Objective Loss 0.707928                                        LR 0.001000    Time 0.053737    
2024-06-06 00:06:06,571 - Epoch: [24][  700/ 1218]    Overall Loss 0.710542    Objective Loss 0.710542                                        LR 0.001000    Time 0.053307    
2024-06-06 00:06:11,941 - Epoch: [24][  800/ 1218]    Overall Loss 0.711869    Objective Loss 0.711869                                        LR 0.001000    Time 0.053353    
2024-06-06 00:06:17,086 - Epoch: [24][  900/ 1218]    Overall Loss 0.712782    Objective Loss 0.712782                                        LR 0.001000    Time 0.053139    
2024-06-06 00:06:22,461 - Epoch: [24][ 1000/ 1218]    Overall Loss 0.714303    Objective Loss 0.714303                                        LR 0.001000    Time 0.053198    
2024-06-06 00:06:27,383 - Epoch: [24][ 1100/ 1218]    Overall Loss 0.714175    Objective Loss 0.714175                                        LR 0.001000    Time 0.052834    
2024-06-06 00:06:32,536 - Epoch: [24][ 1200/ 1218]    Overall Loss 0.714858    Objective Loss 0.714858                                        LR 0.001000    Time 0.052723    
2024-06-06 00:06:33,423 - Epoch: [24][ 1218/ 1218]    Overall Loss 0.714118    Objective Loss 0.714118    Top1 64.303178    Top5 90.709046    LR 0.001000    Time 0.052671    
2024-06-06 00:06:33,669 - --- validate (epoch=24)-----------
2024-06-06 00:06:33,669 - 34633 samples (256 per mini-batch)
2024-06-06 00:06:39,647 - Epoch: [24][  100/  136]    Loss 0.651861    Top1 68.566406    Top5 93.347656    
2024-06-06 00:06:41,545 - Epoch: [24][  136/  136]    Loss 0.650469    Top1 68.625300    Top5 93.529293    
2024-06-06 00:06:41,728 - ==> Top1: 68.625    Top5: 93.529    Loss: 0.650

2024-06-06 00:06:41,729 - ==> Confusion:
[[ 734    1    6    2   18    4    0    3    8  104    1    4    3    6    2    4    4    4    3    7   13]
 [   2  863    6    2   16   29    6   39   13    1    9    7    4    1    6    2    6    5   31    8    7]
 [   6    2  793   16    8    1   32   27    2   10    5    3    4    4    4   10    5    4   12   12   10]
 [   4    3   34  835    3   15    7   10    5    2   22    2    7    3   17    2    2    5   27    3    8]
 [  35   25   11    0  894   10    1    4    3   20    2    4    2    1    7    7    9    2    7    1    9]
 [   5   56    7    8   20  754    8   66    5    9    0   17   11   21    6    3    5    1   10   21   10]
 [   2    4   27    8    2    6  940    9    2    4    8    8    4    4    1   18    1    4    3   26    5]
 [   4   26   14    4    1   40   10  845    4    8    8   12   10    1    1    0    2    0   49   29    9]
 [  16    5    2    3    3    2    0    6  799   70   19    8    7   17   12    0    1    3   15    4   10]
 [  95    2    1    2    6    1    0    3   66  774    1    1    1   19   13    0    0    4    3    2    7]
 [   3    6   12   14    5    3    4   14   29    2  906    3    6    9    7    2    1    0   23    4   11]
 [   7    1    1    1    2   19    2   14    2    2    1  787   39    9    0   25    4   34    0   54    7]
 [   2    1    3   12    2    7    7    6    2    1    3  112  732    1    1    8    5   54    4   25    7]
 [   6    4    8    3    6   18    4    4   24   21   18   14    2  816    5    4    4    4    5   17   14]
 [  15   11    8   25   12    2    0    1   54   25    8    4    3   10  851    0    1    8   38    3   19]
 [   2    3    5    1    7    5    8    0    1    2    1   28    6    2    0  937   11   30    3    8    6]
 [   2    7   12    4    7    9    5    2    4    2    3   15    9    7    2   22  917    2    6   12   23]
 [   1    0    1    3    2    0    2    7    0    5    0   42   43    5    6   11    1  860    3    9    4]
 [   2    5   12   28    4    6    4   47    9    1   11    4    4    2   22    0    2    1  876    8   10]
 [   2    5    1    1    0   14   20   18    3    2    1   25   10    7    0    9    8    7    4  947    4]
 [ 328  389  408  261  354  261  143  427  242  266  280  360  503  385  301  326  408  243  391  749 6907]]

2024-06-06 00:06:41,731 - ==> Best [Top1: 69.593   Top5: 94.782   Sparsity:0.00   Params: 169472 on epoch: 21]
2024-06-06 00:06:41,731 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:06:41,738 - 

2024-06-06 00:06:41,738 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:06:48,428 - Epoch: [25][  100/ 1218]    Overall Loss 0.712274    Objective Loss 0.712274                                        LR 0.001000    Time 0.066865    
2024-06-06 00:06:53,358 - Epoch: [25][  200/ 1218]    Overall Loss 0.712421    Objective Loss 0.712421                                        LR 0.001000    Time 0.058074    
2024-06-06 00:06:58,353 - Epoch: [25][  300/ 1218]    Overall Loss 0.710845    Objective Loss 0.710845                                        LR 0.001000    Time 0.055356    
2024-06-06 00:07:03,679 - Epoch: [25][  400/ 1218]    Overall Loss 0.713336    Objective Loss 0.713336                                        LR 0.001000    Time 0.054826    
2024-06-06 00:07:08,963 - Epoch: [25][  500/ 1218]    Overall Loss 0.716384    Objective Loss 0.716384                                        LR 0.001000    Time 0.054426    
2024-06-06 00:07:14,081 - Epoch: [25][  600/ 1218]    Overall Loss 0.716399    Objective Loss 0.716399                                        LR 0.001000    Time 0.053880    
2024-06-06 00:07:19,378 - Epoch: [25][  700/ 1218]    Overall Loss 0.716765    Objective Loss 0.716765                                        LR 0.001000    Time 0.053746    
2024-06-06 00:07:24,709 - Epoch: [25][  800/ 1218]    Overall Loss 0.716225    Objective Loss 0.716225                                        LR 0.001000    Time 0.053690    
2024-06-06 00:07:29,962 - Epoch: [25][  900/ 1218]    Overall Loss 0.715091    Objective Loss 0.715091                                        LR 0.001000    Time 0.053558    
2024-06-06 00:07:34,959 - Epoch: [25][ 1000/ 1218]    Overall Loss 0.716708    Objective Loss 0.716708                                        LR 0.001000    Time 0.053197    
2024-06-06 00:07:39,991 - Epoch: [25][ 1100/ 1218]    Overall Loss 0.717633    Objective Loss 0.717633                                        LR 0.001000    Time 0.052933    
2024-06-06 00:07:44,990 - Epoch: [25][ 1200/ 1218]    Overall Loss 0.715977    Objective Loss 0.715977                                        LR 0.001000    Time 0.052686    
2024-06-06 00:07:45,831 - Epoch: [25][ 1218/ 1218]    Overall Loss 0.715379    Objective Loss 0.715379    Top1 71.393643    Top5 94.865526    LR 0.001000    Time 0.052597    
2024-06-06 00:07:46,043 - --- validate (epoch=25)-----------
2024-06-06 00:07:46,044 - 34633 samples (256 per mini-batch)
2024-06-06 00:07:52,289 - Epoch: [25][  100/  136]    Loss 0.659350    Top1 71.109375    Top5 94.847656    
2024-06-06 00:07:54,143 - Epoch: [25][  136/  136]    Loss 0.658997    Top1 70.837063    Top5 94.742009    
2024-06-06 00:07:54,350 - ==> Top1: 70.837    Top5: 94.742    Loss: 0.659

2024-06-06 00:07:54,352 - ==> Confusion:
[[ 754    1    3    0   14    1    1    3   10   98    1    2    2    3   12    2    1    1    3    2   17]
 [   4  858    2    0   37   42    5   12   15    4    4    7    5    5   14    2   14    1   12    2   18]
 [  19    8  730   24   14    7   36   14    2   13    5    4    7   10   11   11   10    2   10    5   28]
 [   0    5   16  817    9   11    6    6    4    6   26    2    8    3   54    1    5    9   15    3   10]
 [  22   13    1    1  898   23    0    1    5   26    0    3    0    9   14    6    9    1    5    0   17]
 [   4   43    0    8   35  772    8   35    9   12    1   23    5   35    7    2   10    5    8    5   16]
 [   1    9   30    3    6   10  930    8    3    2    4    8    3    4    3   14    5    6    4   16   17]
 [   4   29   15    3    6   57    3  819    6    9    7   15   10    6    7    1    3    4   44   16   13]
 [  15    6    0    0    3    2    0    1  820   53    6    4    6   18   45    0    2    3    7    0   11]
 [ 101    3    0    1    8    1    0    0   58  771    0    1    1   25   16    0    2    3    2    2    6]
 [   4   11    7   16    8    5    6    6   41    6  859    4    3   27   23    0    3    0   21    3   11]
 [   4    1    1    0    3   26    3    9    5    3    1  758   72   31    2   17    4   36    5   19   11]
 [   2    3    1    8    2    5    1    5    5    1    0   68  790   11    7   10    7   37    4    5   23]
 [  10    3    2    0    7   22    1    5   26   30    6   15    2  826    9    7    2    6    1    9   12]
 [  11    9    0   22   14    5    0    2   43   16    1    3    0    8  927    1    4    2   11    0   19]
 [   3    1    3    1    5    3    9    1    2    1    0   22   22    2    2  916   20   23    2    5   23]
 [   7   11    5    2   16   13    1    1   15    1    1   10   10    8    2    8  923    3    2    8   25]
 [   3    2    0   10    0    3    2    2    3    5    0   22   67    9    6   16    1  839    7    1    7]
 [   0   15    9   25    9    8    3   35   17    0    7    4    6    2   55    1    2    0  845    1   14]
 [   3    8    5    0    6   12   17   20    6    2    1   40   18   18    3    7   12    6    4  882   18]
 [ 329  280  163  160  441  316   91  254  260  198  194  267  565  539  564  195  476  160  297  384 7799]]

2024-06-06 00:07:54,354 - ==> Best [Top1: 70.837   Top5: 94.742   Sparsity:0.00   Params: 169472 on epoch: 25]
2024-06-06 00:07:54,354 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:07:54,369 - 

2024-06-06 00:07:54,370 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:08:01,056 - Epoch: [26][  100/ 1218]    Overall Loss 0.699011    Objective Loss 0.699011                                        LR 0.001000    Time 0.066838    
2024-06-06 00:08:06,476 - Epoch: [26][  200/ 1218]    Overall Loss 0.705173    Objective Loss 0.705173                                        LR 0.001000    Time 0.060506    
2024-06-06 00:08:11,601 - Epoch: [26][  300/ 1218]    Overall Loss 0.709065    Objective Loss 0.709065                                        LR 0.001000    Time 0.057413    
2024-06-06 00:08:16,860 - Epoch: [26][  400/ 1218]    Overall Loss 0.708396    Objective Loss 0.708396                                        LR 0.001000    Time 0.056200    
2024-06-06 00:08:22,411 - Epoch: [26][  500/ 1218]    Overall Loss 0.708955    Objective Loss 0.708955                                        LR 0.001000    Time 0.056058    
2024-06-06 00:08:27,272 - Epoch: [26][  600/ 1218]    Overall Loss 0.708318    Objective Loss 0.708318                                        LR 0.001000    Time 0.054812    
2024-06-06 00:08:32,342 - Epoch: [26][  700/ 1218]    Overall Loss 0.707302    Objective Loss 0.707302                                        LR 0.001000    Time 0.054222    
2024-06-06 00:08:37,558 - Epoch: [26][  800/ 1218]    Overall Loss 0.707631    Objective Loss 0.707631                                        LR 0.001000    Time 0.053961    
2024-06-06 00:08:42,455 - Epoch: [26][  900/ 1218]    Overall Loss 0.708240    Objective Loss 0.708240                                        LR 0.001000    Time 0.053404    
2024-06-06 00:08:47,733 - Epoch: [26][ 1000/ 1218]    Overall Loss 0.709318    Objective Loss 0.709318                                        LR 0.001000    Time 0.053339    
2024-06-06 00:08:52,708 - Epoch: [26][ 1100/ 1218]    Overall Loss 0.709105    Objective Loss 0.709105                                        LR 0.001000    Time 0.053011    
2024-06-06 00:08:57,780 - Epoch: [26][ 1200/ 1218]    Overall Loss 0.708377    Objective Loss 0.708377                                        LR 0.001000    Time 0.052818    
2024-06-06 00:08:58,727 - Epoch: [26][ 1218/ 1218]    Overall Loss 0.708328    Objective Loss 0.708328    Top1 71.393643    Top5 93.398533    LR 0.001000    Time 0.052814    
2024-06-06 00:08:58,902 - --- validate (epoch=26)-----------
2024-06-06 00:08:58,903 - 34633 samples (256 per mini-batch)
2024-06-06 00:09:05,043 - Epoch: [26][  100/  136]    Loss 0.661408    Top1 68.746094    Top5 93.906250    
2024-06-06 00:09:07,031 - Epoch: [26][  136/  136]    Loss 0.661366    Top1 68.864955    Top5 93.921982    
2024-06-06 00:09:07,272 - ==> Top1: 68.865    Top5: 93.922    Loss: 0.661

2024-06-06 00:09:07,274 - ==> Confusion:
[[ 789    3    4    1   25    1    2    4   13   50    1    2    0    5    6    8    4    2    2    0    9]
 [   5  844    6    3   30   42    5   30    4    1    5    7    2    2    6    5   27    3   19    7   10]
 [  13    2  797   11    7    5   39   12    0    6    3    4    1    5    5    9   18    0    8   11   14]
 [   6    2   35  805    4   15    7    3    4    4   13    3   12    2   25    9   14    7   27    6   13]
 [  42    8    8    0  909   12    2    3    0   11    0    4    2    4    8    9   17    1    6    4    4]
 [   5   37    4   10   22  782    6   47    3    5    0   20   11   18    4    5   22    2   11   20    9]
 [   1    1   44    6    3    7  931    8    0    1    2    7    2    0    0   23    8    3    4   23   12]
 [   3   12   21    2    4   48    5  855    3    2    0   13    6    1    3    2    7    6   52   21   11]
 [  24    2    0    2    7    8    1    4  765   79   14    6    8   23   23    1    7    4   11    2   11]
 [ 160    1    2    0   18    7    1    2   42  704    1    2    0   25    7    1    4    8    3    3   10]
 [   6    3   24   27    7   10    6   13   24    2  834    1    5   16   16    1    5    0   39    6   19]
 [   5    0    1    2    0   11    5    7    0    1    0  795   49    8    1   34   13   21    2   47    9]
 [   3    2    2    5    0    5    2    5    2    0    2  104  743    5    2   18   21   46    5   14    9]
 [   9    2    3    1    8   35    0    7   13   22   10   21    6  801    4   10    9    5    2   19   14]
 [  29    5    4   25   20    8    1    2   41   17    9    2    5   10  872    2   10    6   20    0   10]
 [   5    3    3    0    5    3    5    1    0    0    0   14    9    2    0  963   16   13    3   13    8]
 [   9    6    9    4    7    7    2    3    4    1    0    7    8    6    3   22  939    4    5   15   11]
 [   1    0    5    8    1    4    6    1    4    1    0   31   46   11    6   34    2  821    2    7   14]
 [   1   14   16   24    5    5    1   36   10    2    3    1    5    1   28    3    3    3  876    6   15]
 [   0    5    7    2    2   17   11   12    3    0    1   25    7    5    0   11   12    1    7  950   10]
 [ 450  301  376  177  479  355  167  310  156  152  161  273  499  384  341  404  771  160  340  601 7075]]

2024-06-06 00:09:07,275 - ==> Best [Top1: 70.837   Top5: 94.742   Sparsity:0.00   Params: 169472 on epoch: 25]
2024-06-06 00:09:07,276 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:09:07,286 - 

2024-06-06 00:09:07,286 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:09:14,074 - Epoch: [27][  100/ 1218]    Overall Loss 0.703966    Objective Loss 0.703966                                        LR 0.001000    Time 0.067845    
2024-06-06 00:09:19,396 - Epoch: [27][  200/ 1218]    Overall Loss 0.704752    Objective Loss 0.704752                                        LR 0.001000    Time 0.060524    
2024-06-06 00:09:24,311 - Epoch: [27][  300/ 1218]    Overall Loss 0.699695    Objective Loss 0.699695                                        LR 0.001000    Time 0.056723    
2024-06-06 00:09:29,392 - Epoch: [27][  400/ 1218]    Overall Loss 0.700250    Objective Loss 0.700250                                        LR 0.001000    Time 0.055241    
2024-06-06 00:09:34,531 - Epoch: [27][  500/ 1218]    Overall Loss 0.703794    Objective Loss 0.703794                                        LR 0.001000    Time 0.054465    
2024-06-06 00:09:39,443 - Epoch: [27][  600/ 1218]    Overall Loss 0.701326    Objective Loss 0.701326                                        LR 0.001000    Time 0.053570    
2024-06-06 00:09:44,768 - Epoch: [27][  700/ 1218]    Overall Loss 0.702377    Objective Loss 0.702377                                        LR 0.001000    Time 0.053522    
2024-06-06 00:09:49,694 - Epoch: [27][  800/ 1218]    Overall Loss 0.701405    Objective Loss 0.701405                                        LR 0.001000    Time 0.052986    
2024-06-06 00:09:54,746 - Epoch: [27][  900/ 1218]    Overall Loss 0.702480    Objective Loss 0.702480                                        LR 0.001000    Time 0.052709    
2024-06-06 00:09:59,917 - Epoch: [27][ 1000/ 1218]    Overall Loss 0.703722    Objective Loss 0.703722                                        LR 0.001000    Time 0.052608    
2024-06-06 00:10:05,017 - Epoch: [27][ 1100/ 1218]    Overall Loss 0.703937    Objective Loss 0.703937                                        LR 0.001000    Time 0.052459    
2024-06-06 00:10:10,156 - Epoch: [27][ 1200/ 1218]    Overall Loss 0.702897    Objective Loss 0.702897                                        LR 0.001000    Time 0.052368    
2024-06-06 00:10:10,999 - Epoch: [27][ 1218/ 1218]    Overall Loss 0.703345    Objective Loss 0.703345    Top1 70.171149    Top5 94.132029    LR 0.001000    Time 0.052286    
2024-06-06 00:10:11,185 - --- validate (epoch=27)-----------
2024-06-06 00:10:11,185 - 34633 samples (256 per mini-batch)
2024-06-06 00:10:17,356 - Epoch: [27][  100/  136]    Loss 0.658587    Top1 69.492188    Top5 94.105469    
2024-06-06 00:10:19,182 - Epoch: [27][  136/  136]    Loss 0.657790    Top1 69.197009    Top5 94.204949    
2024-06-06 00:10:19,383 - ==> Top1: 69.197    Top5: 94.205    Loss: 0.658

2024-06-06 00:10:19,384 - ==> Confusion:
[[ 805    1   11    0   15    1    0    2    4   58    0    0    0    7    8    4    2    1    4    4    4]
 [   3  866   10    1   43   22    4   22    4    3    3    4    1    3   16    6    6    5   23    7   11]
 [  17    5  798    2    7    2   47   16    2    9    7    4    1    7    3    8    7    1    4   11   12]
 [  12    3   58  729    5   14   11    7    4    6   30    2   11    3   54    1    6    7   36    4   13]
 [  40   13    7    0  923    8    4    3    3   14    1    4    0    4   12    6    4    0    1    2    5]
 [   8   69    6    3   40  684    8   87    2    9    4   21    2   43    6    6    6    1    5   15   18]
 [   4    5   31    0    7    3  957   11    0    2    4    6    0    2    0   11    3    6    3   22    9]
 [   6   20   25    3    4   18    3  870    1    7    6   13    3    2    3    1    0    2   55   25   10]
 [  29    5    0    1    3    2    1    2  746   93   18    2    2   34   36    1    4    3   10    4    6]
 [ 122    2    1    0   14    1    2    3   41  761    1    0    0   24   13    3    1    2    2    0    8]
 [   4    5   31    8    9    3    8    9   22    6  873    3    0   27    9    0    1    2   29    6    9]
 [   8    5    3    0    2    9    5   14    1    1    0  801   33   23    1   27    6   22    3   34   13]
 [   2    3    5    4    0    7    5   13    5    0    3  111  707    9    7   14    6   51   10   14   19]
 [   8    3    4    0    7   16    3    3   10   30    5   21    1  846    3    6    4    6    2   15    8]
 [  26    4    4   14   18    2    1    2   18   22   11    2    2    9  907    1    2    5   28    5   15]
 [   3    2   12    0    5    1   13    1    4    3    0   21    6    2    0  945   20   15    2    8    3]
 [  18   15    8    2   21    5    8    5    6    5    1   11    0    6    2   13  892    5    7   17   25]
 [   9    2    3    1    0    2    4    2    3    7    0   29   39    9   10   27    2  842    3   10    1]
 [   3   10   19    6    7    3    2   29    8    5    4    2    2    3   33    4    2    1  900    5   10]
 [   3    8    5    0    4    7   17   27    1    1    1   23    9    3    0   12    9    2    8  934   14]
 [ 514  346  487   81  515  167  242  293  161  265  224  294  371  534  364  291  514  160  376  555 7178]]

2024-06-06 00:10:19,386 - ==> Best [Top1: 70.837   Top5: 94.742   Sparsity:0.00   Params: 169472 on epoch: 25]
2024-06-06 00:10:19,386 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:10:19,398 - 

2024-06-06 00:10:19,398 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:10:26,271 - Epoch: [28][  100/ 1218]    Overall Loss 0.717135    Objective Loss 0.717135                                        LR 0.001000    Time 0.068703    
2024-06-06 00:10:31,276 - Epoch: [28][  200/ 1218]    Overall Loss 0.712571    Objective Loss 0.712571                                        LR 0.001000    Time 0.059365    
2024-06-06 00:10:36,343 - Epoch: [28][  300/ 1218]    Overall Loss 0.711047    Objective Loss 0.711047                                        LR 0.001000    Time 0.056457    
2024-06-06 00:10:41,392 - Epoch: [28][  400/ 1218]    Overall Loss 0.710505    Objective Loss 0.710505                                        LR 0.001000    Time 0.054960    
2024-06-06 00:10:46,575 - Epoch: [28][  500/ 1218]    Overall Loss 0.717373    Objective Loss 0.717373                                        LR 0.001000    Time 0.054317    
2024-06-06 00:10:51,766 - Epoch: [28][  600/ 1218]    Overall Loss 0.725337    Objective Loss 0.725337                                        LR 0.001000    Time 0.053913    
2024-06-06 00:10:56,809 - Epoch: [28][  700/ 1218]    Overall Loss 0.733471    Objective Loss 0.733471                                        LR 0.001000    Time 0.053411    
2024-06-06 00:11:01,881 - Epoch: [28][  800/ 1218]    Overall Loss 0.735501    Objective Loss 0.735501                                        LR 0.001000    Time 0.053071    
2024-06-06 00:11:06,972 - Epoch: [28][  900/ 1218]    Overall Loss 0.733057    Objective Loss 0.733057                                        LR 0.001000    Time 0.052829    
2024-06-06 00:11:11,980 - Epoch: [28][ 1000/ 1218]    Overall Loss 0.730064    Objective Loss 0.730064                                        LR 0.001000    Time 0.052551    
2024-06-06 00:11:16,999 - Epoch: [28][ 1100/ 1218]    Overall Loss 0.733840    Objective Loss 0.733840                                        LR 0.001000    Time 0.052335    
2024-06-06 00:11:21,967 - Epoch: [28][ 1200/ 1218]    Overall Loss 0.734758    Objective Loss 0.734758                                        LR 0.001000    Time 0.052112    
2024-06-06 00:11:22,859 - Epoch: [28][ 1218/ 1218]    Overall Loss 0.734831    Objective Loss 0.734831    Top1 64.792176    Top5 93.643032    LR 0.001000    Time 0.052074    
2024-06-06 00:11:23,111 - --- validate (epoch=28)-----------
2024-06-06 00:11:23,111 - 34633 samples (256 per mini-batch)
2024-06-06 00:11:29,181 - Epoch: [28][  100/  136]    Loss 0.682469    Top1 67.226562    Top5 93.398438    
2024-06-06 00:11:31,122 - Epoch: [28][  136/  136]    Loss 0.689753    Top1 67.057431    Top5 93.439783    
2024-06-06 00:11:31,363 - ==> Top1: 67.057    Top5: 93.440    Loss: 0.690

2024-06-06 00:11:31,364 - ==> Confusion:
[[ 775    2    4    1   13    5    1    1   11   83    0    1    2    3   13    5    1    1    1    2    6]
 [   4  891    5    1   30   23    3   19   15    6    7    5    2    2   16    1   10    1   15    2    5]
 [   9    5  803   12    6    7   21   29    1    6   10    3    3    9   13    7    3    1    6    7    9]
 [   5    5   28  798    7    7    4    3    6    1   35    1    7    5   57    2    2   10   23    0   10]
 [  45   18    4    0  881   13    1    2    5   20    3    7    1    6   16    5   10    1    7    1    8]
 [  11   74    5   10   20  725    3   72    7   12    4   16   13   25   12    1    3    3   11    8    8]
 [   4   13   67    1    2    5  905   21    2    2    8    6    2    3    1   10    4    4    3   15    8]
 [   4   26   14    1   10   32    0  872    7    2    6   14    9    1    8    0    0    5   43   10   13]
 [  18   10    0    4    2    1    0    5  824   51   22    4    4    5   38    1    0    1    9    0    3]
 [ 103    1    1    0    5    2    2    3   64  780    1    0    0   16   12    0    0    2    1    2    6]
 [   3    5    7    9    1    1    4   11   30    1  929    1    1   15   11    0    3    0   25    3    4]
 [   3    3    4    1    3   22    2   17    0    2    2  801   44   14    3   24    4   33    4   16    9]
 [   1    1    6   12    0    7    1    7    3    0    4   82  745    8    6    7    3   76   10   10    6]
 [   8    0    2    0    4   12    4    7   51   43   17   13    8  798   12    3    3    6    1    6    3]
 [  16    5    2   18   13    1    0    3   47   12    7    3    1    6  937    0    4    5   13    0    5]
 [   6    2    7    0    5    3    9    1    1    2    0   25   10    5    4  939   10   16    1   10   10]
 [  11   19    6    3   12   14    0    3    7    1    3    7    8    4    3   16  923    5    2    8   17]
 [   5    4    2    4    1    1    0    3    1    1    1   22   31    9    7   16    5  884    0    5    3]
 [   2   12    7   18    5    3    0   30    7    0   14    5    8    0   41    0    3    2  891    7    3]
 [   1    9    7    2    1   15   11   40    3    2    6   39    8   12    1   10    8    6    3  894   10]
 [ 536  461  413  215  461  281  115  344  318  239  401  329  583  473  648  295  439  271  384  497 6229]]

2024-06-06 00:11:31,366 - ==> Best [Top1: 70.837   Top5: 94.742   Sparsity:0.00   Params: 169472 on epoch: 25]
2024-06-06 00:11:31,366 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:11:31,374 - 

2024-06-06 00:11:31,374 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:11:37,963 - Epoch: [29][  100/ 1218]    Overall Loss 0.731191    Objective Loss 0.731191                                        LR 0.001000    Time 0.065871    
2024-06-06 00:11:43,038 - Epoch: [29][  200/ 1218]    Overall Loss 0.727224    Objective Loss 0.727224                                        LR 0.001000    Time 0.058300    
2024-06-06 00:11:48,164 - Epoch: [29][  300/ 1218]    Overall Loss 0.732021    Objective Loss 0.732021                                        LR 0.001000    Time 0.055944    
2024-06-06 00:11:53,501 - Epoch: [29][  400/ 1218]    Overall Loss 0.730041    Objective Loss 0.730041                                        LR 0.001000    Time 0.055294    
2024-06-06 00:11:58,844 - Epoch: [29][  500/ 1218]    Overall Loss 0.723887    Objective Loss 0.723887                                        LR 0.001000    Time 0.054918    
2024-06-06 00:12:03,881 - Epoch: [29][  600/ 1218]    Overall Loss 0.721853    Objective Loss 0.721853                                        LR 0.001000    Time 0.054156    
2024-06-06 00:12:09,025 - Epoch: [29][  700/ 1218]    Overall Loss 0.719148    Objective Loss 0.719148                                        LR 0.001000    Time 0.053764    
2024-06-06 00:12:14,114 - Epoch: [29][  800/ 1218]    Overall Loss 0.718006    Objective Loss 0.718006                                        LR 0.001000    Time 0.053402    
2024-06-06 00:12:19,244 - Epoch: [29][  900/ 1218]    Overall Loss 0.717889    Objective Loss 0.717889                                        LR 0.001000    Time 0.053166    
2024-06-06 00:12:24,449 - Epoch: [29][ 1000/ 1218]    Overall Loss 0.719264    Objective Loss 0.719264                                        LR 0.001000    Time 0.053053    
2024-06-06 00:12:29,563 - Epoch: [29][ 1100/ 1218]    Overall Loss 0.718305    Objective Loss 0.718305                                        LR 0.001000    Time 0.052877    
2024-06-06 00:12:34,677 - Epoch: [29][ 1200/ 1218]    Overall Loss 0.718868    Objective Loss 0.718868                                        LR 0.001000    Time 0.052730    
2024-06-06 00:12:35,551 - Epoch: [29][ 1218/ 1218]    Overall Loss 0.718837    Objective Loss 0.718837    Top1 66.259169    Top5 91.931540    LR 0.001000    Time 0.052667    
2024-06-06 00:12:35,776 - --- validate (epoch=29)-----------
2024-06-06 00:12:35,777 - 34633 samples (256 per mini-batch)
2024-06-06 00:12:41,936 - Epoch: [29][  100/  136]    Loss 0.652279    Top1 69.164062    Top5 94.113281    
2024-06-06 00:12:43,751 - Epoch: [29][  136/  136]    Loss 0.651632    Top1 69.084399    Top5 93.959518    
2024-06-06 00:12:43,945 - ==> Top1: 69.084    Top5: 93.960    Loss: 0.652

2024-06-06 00:12:43,946 - ==> Confusion:
[[ 707    0    9    2   10    4    1    4   12  121    2    3    0    9   10    1    8    5    2    4   17]
 [   1  867    5    2   19   36    7   23    9    3   10    4    1    5   10    5   15    4   11   14   12]
 [   8    3  791   22    3    6   36   10    1    5   10    1    3   11    6    8   10    5    6   16    9]
 [   5    1   27  851    3    9    7    3    5    3   17    1   11    6   18    1    7    7   19    4   11]
 [  35   14    8    3  832   26    1    6    4   27    2    7    0   12   12   12   20    1    6    4   22]
 [   4   45    5    9    9  793    7   40    3    6    6   17    9   43    5    4   10    3    6   11    8]
 [   1    4   39    3    5    1  934   10    1    2    5    5    2    2    2   14    3    2    6   32   13]
 [   3   15   24    2    0   64    9  827    5    2   10   14    5    7    4    1    5    3   45   27    5]
 [  10    5    2    2    3    5    0    4  822   53   17    1    4   26   15    0    7    3   13    3    7]
 [  78    0    8    0    5    4    0    1   59  781    4    1    0   28   12    0    1    5    3    2    9]
 [   1    5    3   22    1    7    7    4   16    4  931    3    2   17    8    1    4    1   21    1    5]
 [   5    0    4    0    2   17    3    4    2    3    4  807   46   20    1   15    8   35    1   27    7]
 [   0    3    3    5    0    5    2    4    5    0    2  101  733    6    5    9   18   55   14   16    9]
 [   5    1    3    2    3   21    1    4   23   22    8   11    3  856    3    2    7    9    1   11    5]
 [  10    5    3   37   12    5    0    1   54   10    9    1    8   13  879    0    4    6   30    4    7]
 [   0    2   10    1    1    2   11    2    1    1    1   27    9    3    1  918   18   31    5   12   10]
 [   3   14    7    4    4   10    5    1    9    3    5    8    5    9    2   16  932    5    5   11   14]
 [   1    0    0    5    1    0    0    2    1    0    0   33   52   12    2   20    4  850    3    6   13]
 [   1    9   15   33    5    4    2   16   16    5   15    2    5    4   14    0    6    3  890    6    7]
 [   3    9    3    0    1   11   17   20    0    3    4   31   10    9    0    7   10    7    4  933    6]
 [ 236  357  377  287  306  336  178  232  226  193  343  293  509  601  316  197  730  250  307  666 6992]]

2024-06-06 00:12:43,947 - ==> Best [Top1: 70.837   Top5: 94.742   Sparsity:0.00   Params: 169472 on epoch: 25]
2024-06-06 00:12:43,948 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:12:43,955 - 

2024-06-06 00:12:43,955 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:12:50,817 - Epoch: [30][  100/ 1218]    Overall Loss 0.723004    Objective Loss 0.723004                                        LR 0.001000    Time 0.068588    
2024-06-06 00:12:56,007 - Epoch: [30][  200/ 1218]    Overall Loss 0.716746    Objective Loss 0.716746                                        LR 0.001000    Time 0.060230    
2024-06-06 00:13:01,108 - Epoch: [30][  300/ 1218]    Overall Loss 0.710555    Objective Loss 0.710555                                        LR 0.001000    Time 0.057148    
2024-06-06 00:13:06,439 - Epoch: [30][  400/ 1218]    Overall Loss 0.710371    Objective Loss 0.710371                                        LR 0.001000    Time 0.056184    
2024-06-06 00:13:11,873 - Epoch: [30][  500/ 1218]    Overall Loss 0.707119    Objective Loss 0.707119                                        LR 0.001000    Time 0.055810    
2024-06-06 00:13:17,201 - Epoch: [30][  600/ 1218]    Overall Loss 0.705934    Objective Loss 0.705934                                        LR 0.001000    Time 0.055384    
2024-06-06 00:13:22,515 - Epoch: [30][  700/ 1218]    Overall Loss 0.705879    Objective Loss 0.705879                                        LR 0.001000    Time 0.055060    
2024-06-06 00:13:27,339 - Epoch: [30][  800/ 1218]    Overall Loss 0.706159    Objective Loss 0.706159                                        LR 0.001000    Time 0.054204    
2024-06-06 00:13:32,379 - Epoch: [30][  900/ 1218]    Overall Loss 0.704540    Objective Loss 0.704540                                        LR 0.001000    Time 0.053780    
2024-06-06 00:13:37,335 - Epoch: [30][ 1000/ 1218]    Overall Loss 0.705947    Objective Loss 0.705947                                        LR 0.001000    Time 0.053355    
2024-06-06 00:13:42,462 - Epoch: [30][ 1100/ 1218]    Overall Loss 0.706262    Objective Loss 0.706262                                        LR 0.001000    Time 0.053164    
2024-06-06 00:13:47,644 - Epoch: [30][ 1200/ 1218]    Overall Loss 0.706012    Objective Loss 0.706012                                        LR 0.001000    Time 0.053049    
2024-06-06 00:13:48,555 - Epoch: [30][ 1218/ 1218]    Overall Loss 0.706580    Objective Loss 0.706580    Top1 71.149144    Top5 95.110024    LR 0.001000    Time 0.053013    
2024-06-06 00:13:48,754 - --- validate (epoch=30)-----------
2024-06-06 00:13:48,755 - 34633 samples (256 per mini-batch)
2024-06-06 00:13:54,840 - Epoch: [30][  100/  136]    Loss 0.649620    Top1 69.828125    Top5 94.574219    
2024-06-06 00:13:56,649 - Epoch: [30][  136/  136]    Loss 0.645633    Top1 69.999711    Top5 94.713135    
2024-06-06 00:13:56,880 - ==> Top1: 70.000    Top5: 94.713    Loss: 0.646

2024-06-06 00:13:56,881 - ==> Confusion:
[[ 777    0    4    2   17    5    1    6    7   73    0    3    1    6    9    3    2    2    3    4    6]
 [   2  842    9    4   40   32   13   38    7    3    5    7    6    1    6    2   16    1   11    7   11]
 [  13    3  799   13    9    3   30   26    0    6   10    6    5    5    4    5    6    2    7    6   12]
 [  10    2   37  825    9   10    2    5    3    1   18    3   11    4   37    2    2    8   12    3   12]
 [  29   16    7    0  911    8    2   10    2   18    1    4    2    5    9    6    5    0    5    0   14]
 [   5   28    3    6   30  770    7   65    4    3    3   26   12   29    5    4    6    1    4   17   15]
 [   2    3   56    6    4    4  933   10    2    3    6    2    8    3    0    6    5    3    3   13   14]
 [   5    5   33    6    2   54   11  857    1    1    5   17   12    4    2    1    1    4   23   25    8]
 [  13    2    1    3    2    4    2    8  791   61   13    2    6   24   38    1    4    4   11    1   11]
 [ 105    0    1    1   18    0    3    4   53  751    1    1    2   29   15    1    3    1    4    2    6]
 [   5    3   18   21    4    4    7   17   23    3  858    2    4   22   30    0    4    0   21    4   14]
 [   4    2    7    2    3    9    3   10    0    1    0  821   39   22    0   16    5   24    2   32    9]
 [   1    0    2    9    1    2    2    5    1    0    0  141  730    5    3    9    4   46    7   12   15]
 [   4    0    9    2    9   11    2    5   12   14   12   12    6  865   10    4    6    1    1    5   11]
 [  14    1    4   19   18    5    1    3   27   14    4    0    6    9  929    2    3    3   20    2   14]
 [   5    0    6    1   14    3   14    3    0    0    0   28   10    4    0  926   23    9    1    4   15]
 [   4    5    9    3   12    4    3    4   10    0    4   12    3    5    4    9  952    0    1    4   24]
 [   6    1    2    2    1    3    4    7    1    3    1   47   44    8    5   23    6  818    1    6   16]
 [   5    9   21   24   10    5    2   56    8    1   14    3   11    3   26    0    3    3  837    5   12]
 [   4    6   11    0    3   12   16   17    2    1    0   28    9    8    0    6   15    2    2  934   12]
 [ 396  267  545  202  489  285  179  391  168  181  200  336  494  480  407  227  515  122  220  511 7317]]

2024-06-06 00:13:56,883 - ==> Best [Top1: 70.837   Top5: 94.742   Sparsity:0.00   Params: 169472 on epoch: 25]
2024-06-06 00:13:56,883 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:13:56,890 - 

2024-06-06 00:13:56,891 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:14:03,656 - Epoch: [31][  100/ 1218]    Overall Loss 0.704845    Objective Loss 0.704845                                        LR 0.001000    Time 0.067630    
2024-06-06 00:14:08,567 - Epoch: [31][  200/ 1218]    Overall Loss 0.696976    Objective Loss 0.696976                                        LR 0.001000    Time 0.058358    
2024-06-06 00:14:13,637 - Epoch: [31][  300/ 1218]    Overall Loss 0.703452    Objective Loss 0.703452                                        LR 0.001000    Time 0.055798    
2024-06-06 00:14:18,941 - Epoch: [31][  400/ 1218]    Overall Loss 0.700812    Objective Loss 0.700812                                        LR 0.001000    Time 0.055103    
2024-06-06 00:14:24,376 - Epoch: [31][  500/ 1218]    Overall Loss 0.701731    Objective Loss 0.701731                                        LR 0.001000    Time 0.054947    
2024-06-06 00:14:29,567 - Epoch: [31][  600/ 1218]    Overall Loss 0.700126    Objective Loss 0.700126                                        LR 0.001000    Time 0.054437    
2024-06-06 00:14:34,705 - Epoch: [31][  700/ 1218]    Overall Loss 0.700688    Objective Loss 0.700688                                        LR 0.001000    Time 0.053998    
2024-06-06 00:14:39,791 - Epoch: [31][  800/ 1218]    Overall Loss 0.700865    Objective Loss 0.700865                                        LR 0.001000    Time 0.053602    
2024-06-06 00:14:44,957 - Epoch: [31][  900/ 1218]    Overall Loss 0.697363    Objective Loss 0.697363                                        LR 0.001000    Time 0.053384    
2024-06-06 00:14:50,241 - Epoch: [31][ 1000/ 1218]    Overall Loss 0.697555    Objective Loss 0.697555                                        LR 0.001000    Time 0.053328    
2024-06-06 00:14:55,373 - Epoch: [31][ 1100/ 1218]    Overall Loss 0.698163    Objective Loss 0.698163                                        LR 0.001000    Time 0.053143    
2024-06-06 00:15:00,467 - Epoch: [31][ 1200/ 1218]    Overall Loss 0.697860    Objective Loss 0.697860                                        LR 0.001000    Time 0.052958    
2024-06-06 00:15:01,426 - Epoch: [31][ 1218/ 1218]    Overall Loss 0.698166    Objective Loss 0.698166    Top1 71.882641    Top5 94.132029    LR 0.001000    Time 0.052962    
2024-06-06 00:15:01,669 - --- validate (epoch=31)-----------
2024-06-06 00:15:01,669 - 34633 samples (256 per mini-batch)
2024-06-06 00:15:07,817 - Epoch: [31][  100/  136]    Loss 0.654022    Top1 70.097656    Top5 94.210938    
2024-06-06 00:15:09,710 - Epoch: [31][  136/  136]    Loss 0.658779    Top1 69.835128    Top5 94.069240    
2024-06-06 00:15:09,915 - ==> Top1: 69.835    Top5: 94.069    Loss: 0.659

2024-06-06 00:15:09,917 - ==> Confusion:
[[ 791    0   16    4    9    2    2    1    7   52    1    2    3    4   11    4    4    2    5    0   11]
 [   4  893    7    3   16   45    5   19    5    3    8    4    1    3   10    3    8    2   13    5    6]
 [   2    7  822   22    6    3   33    6    1    4   12    4    4    5    3    4    4    2    8    8   10]
 [   7    1   41  831    3   10    5    2    4    1   31    0   10    0   22    3    4    7   25    0    9]
 [  42   33   15    2  868   20    2    1    1   13    2    5    1    4   10    9    6    2    8    1    9]
 [   6   48    7    9   16  783    4   47    7   11    4   10   14   33    4    2    1    8    5    9   15]
 [   4   12   45    5    2    5  955    5    2    1   10    3    2    0    1    4    4    3    5   10    8]
 [   8   17   28   11    5   44    6  831    4    0   14    9    7   15    3    0    3    3   44   15   10]
 [  27    5    3    2    2    0    1    5  781   55   22    0    6   20   38    1    4    9   11    3    7]
 [ 151    3    5    1   10    2    1    4   51  715    5    1    2   12   15    0    3    2    5    0   13]
 [   6   15   22   16    2    3    5    7   25    2  887    0    4   16   13    0    1    0   28    1   11]
 [   6    2    7    4    0   14    7   10    1    1    1  758   59   17    2   23    6   45    4   32   12]
 [   1    1    6   12    1    8    1    5    3    0    1   62  761    5    4    8    8   76   10   12   10]
 [   6    6    7    2    7   17    1    4   27   31   15   12   14  822    9    5    2    5    0    2    7]
 [  22    5   15   31   11    1    0    1   38    7    4    3    1    5  905    2    2    4   26    0   15]
 [   4    4   13    3    7    4   15    1    1    2    0   16   13    3    1  916   15   29    2    5   12]
 [  11   17   17    4   10   17    3    2    7    1    2    5   11    8    1   12  899    7    6   11   21]
 [   4    3    3    4    0    3    1    5    1    0    3   14   43    3    5   14    2  877    3    4   13]
 [   6   11   25   26    1    5    2   25    9    1   12    2    8    2   24    0    2    3  879    1   14]
 [   3    5   17    2    0   28   23   32    0    0    5   17   21   12    2    9   13    8    7  875    9]
 [ 402  424  642  300  316  300  176  268  184  120  301  201  585  409  374  178  412  252  335  416 7337]]

2024-06-06 00:15:09,919 - ==> Best [Top1: 70.837   Top5: 94.742   Sparsity:0.00   Params: 169472 on epoch: 25]
2024-06-06 00:15:09,919 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:15:09,930 - 

2024-06-06 00:15:09,930 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:15:16,808 - Epoch: [32][  100/ 1218]    Overall Loss 0.721938    Objective Loss 0.721938                                        LR 0.001000    Time 0.068746    
2024-06-06 00:15:21,711 - Epoch: [32][  200/ 1218]    Overall Loss 0.708591    Objective Loss 0.708591                                        LR 0.001000    Time 0.058877    
2024-06-06 00:15:26,796 - Epoch: [32][  300/ 1218]    Overall Loss 0.701539    Objective Loss 0.701539                                        LR 0.001000    Time 0.056192    
2024-06-06 00:15:31,737 - Epoch: [32][  400/ 1218]    Overall Loss 0.701056    Objective Loss 0.701056                                        LR 0.001000    Time 0.054491    
2024-06-06 00:15:36,762 - Epoch: [32][  500/ 1218]    Overall Loss 0.700380    Objective Loss 0.700380                                        LR 0.001000    Time 0.053638    
2024-06-06 00:15:41,923 - Epoch: [32][  600/ 1218]    Overall Loss 0.703193    Objective Loss 0.703193                                        LR 0.001000    Time 0.053297    
2024-06-06 00:15:46,933 - Epoch: [32][  700/ 1218]    Overall Loss 0.701643    Objective Loss 0.701643                                        LR 0.001000    Time 0.052837    
2024-06-06 00:15:51,878 - Epoch: [32][  800/ 1218]    Overall Loss 0.699966    Objective Loss 0.699966                                        LR 0.001000    Time 0.052410    
2024-06-06 00:15:57,035 - Epoch: [32][  900/ 1218]    Overall Loss 0.700581    Objective Loss 0.700581                                        LR 0.001000    Time 0.052314    
2024-06-06 00:16:02,066 - Epoch: [32][ 1000/ 1218]    Overall Loss 0.699841    Objective Loss 0.699841                                        LR 0.001000    Time 0.052111    
2024-06-06 00:16:07,309 - Epoch: [32][ 1100/ 1218]    Overall Loss 0.698517    Objective Loss 0.698517                                        LR 0.001000    Time 0.052138    
2024-06-06 00:16:12,448 - Epoch: [32][ 1200/ 1218]    Overall Loss 0.698631    Objective Loss 0.698631                                        LR 0.001000    Time 0.052074    
2024-06-06 00:16:13,378 - Epoch: [32][ 1218/ 1218]    Overall Loss 0.699267    Objective Loss 0.699267    Top1 67.237164    Top5 93.398533    LR 0.001000    Time 0.052067    
2024-06-06 00:16:13,585 - --- validate (epoch=32)-----------
2024-06-06 00:16:13,586 - 34633 samples (256 per mini-batch)
2024-06-06 00:16:19,608 - Epoch: [32][  100/  136]    Loss 0.637542    Top1 69.261719    Top5 94.253906    
2024-06-06 00:16:21,494 - Epoch: [32][  136/  136]    Loss 0.642064    Top1 69.500188    Top5 94.158750    
2024-06-06 00:16:21,728 - ==> Top1: 69.500    Top5: 94.159    Loss: 0.642

2024-06-06 00:16:21,730 - ==> Confusion:
[[ 728    2    5    2   49    5    0    5    8   84    0    8    2    6    3    1    1    1    4    5   12]
 [   2  874    6    3   49   26    8   20    4    1    4   11    1    0   12    2    8    0   18    9    5]
 [  16    5  775    7   11    3   58   23    1    5    4    7    3    7    3    8    8    2    6    8   10]
 [   3    5   31  805   10   10    6    7    2    4   26    5   10    4   28    4    5   10   26    6    9]
 [  22    7    3    0  942    9    1    1    5   16    1    4    0    4    7   10   12    1    0    2    7]
 [   3   47    3    3   29  774   10   38    5    4    4   36    4   29    5    2   15    4    9    6   13]
 [   1   12   26    2    4    9  967    8    1    0    1    5    1    1    0   11    6    2    3   15   11]
 [   3   19   11    2    7   50   15  851    1    3    4   12    5    4    1    2    6    3   40   30    8]
 [  13    5    3    1    5    1    0    1  775   72   21    4    6   34   23    1    5    1   17    5    9]
 [  75    1    2    0   18    4    2    3   48  790    2    2    0   26    8    1    2    1    5    1   10]
 [   3   13   12   12    5    8   13    9   21    1  906    2    3   10    8    1    4    0   21    6    6]
 [   5    3    3    1    3   12    3    8    0    2    2  865   19   15    1   15    4   22    3   21    4]
 [   1    2    4    8    1    7    3    6    1    0    2  162  697    4    4    6    6   47   14   11    9]
 [   5    2    0    2   11   26    2   11   14   22   14   27    5  813    6    4    9    4    0   11   13]
 [  14    5    3   26   44    2    1    2   36   16    9    3    6   14  855    0    5   11   33    1   12]
 [   3    2   10    3    7    3   16    2    1    3    1   40   11    4    0  903   14   23    4    3   13]
 [  10   18    7    2   11    6    1    0   10    1    1   16    4    4    3   11  939    1    0    7   20]
 [   3    3    1    4    2    1    1    3    2    1    0   52   38    8    4   17    2  849    5    7    2]
 [   3   13    9   22    8    2    4   36    3    1   10    5    3    1   19    0    4    4  895    7    9]
 [   2    9    3    0    2   14   25    9    0    2    0   45    7    5    0    5    8    3    4  935   10]
 [ 333  337  420  227  573  272  206  314  177  163  222  426  458  430  341  270  516  225  329  561 7132]]

2024-06-06 00:16:21,732 - ==> Best [Top1: 70.837   Top5: 94.742   Sparsity:0.00   Params: 169472 on epoch: 25]
2024-06-06 00:16:21,732 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:16:21,743 - 

2024-06-06 00:16:21,744 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:16:28,414 - Epoch: [33][  100/ 1218]    Overall Loss 0.687404    Objective Loss 0.687404                                        LR 0.001000    Time 0.066670    
2024-06-06 00:16:33,459 - Epoch: [33][  200/ 1218]    Overall Loss 0.682269    Objective Loss 0.682269                                        LR 0.001000    Time 0.058551    
2024-06-06 00:16:38,442 - Epoch: [33][  300/ 1218]    Overall Loss 0.685878    Objective Loss 0.685878                                        LR 0.001000    Time 0.055635    
2024-06-06 00:16:43,846 - Epoch: [33][  400/ 1218]    Overall Loss 0.696001    Objective Loss 0.696001                                        LR 0.001000    Time 0.055230    
2024-06-06 00:16:48,897 - Epoch: [33][  500/ 1218]    Overall Loss 0.698946    Objective Loss 0.698946                                        LR 0.001000    Time 0.054282    
2024-06-06 00:16:54,328 - Epoch: [33][  600/ 1218]    Overall Loss 0.697254    Objective Loss 0.697254                                        LR 0.001000    Time 0.054283    
2024-06-06 00:16:59,403 - Epoch: [33][  700/ 1218]    Overall Loss 0.696647    Objective Loss 0.696647                                        LR 0.001000    Time 0.053776    
2024-06-06 00:17:04,425 - Epoch: [33][  800/ 1218]    Overall Loss 0.696049    Objective Loss 0.696049                                        LR 0.001000    Time 0.053328    
2024-06-06 00:17:09,617 - Epoch: [33][  900/ 1218]    Overall Loss 0.695194    Objective Loss 0.695194                                        LR 0.001000    Time 0.053169    
2024-06-06 00:17:14,939 - Epoch: [33][ 1000/ 1218]    Overall Loss 0.694863    Objective Loss 0.694863                                        LR 0.001000    Time 0.053171    
2024-06-06 00:17:20,413 - Epoch: [33][ 1100/ 1218]    Overall Loss 0.694652    Objective Loss 0.694652                                        LR 0.001000    Time 0.053312    
2024-06-06 00:17:25,597 - Epoch: [33][ 1200/ 1218]    Overall Loss 0.695232    Objective Loss 0.695232                                        LR 0.001000    Time 0.053188    
2024-06-06 00:17:26,583 - Epoch: [33][ 1218/ 1218]    Overall Loss 0.695237    Objective Loss 0.695237    Top1 74.327628    Top5 94.621027    LR 0.001000    Time 0.053210    
2024-06-06 00:17:26,838 - --- validate (epoch=33)-----------
2024-06-06 00:17:26,838 - 34633 samples (256 per mini-batch)
2024-06-06 00:17:32,939 - Epoch: [33][  100/  136]    Loss 0.634350    Top1 71.203125    Top5 94.730469    
2024-06-06 00:17:34,709 - Epoch: [33][  136/  136]    Loss 0.640168    Top1 71.068056    Top5 94.655386    
2024-06-06 00:17:34,922 - ==> Top1: 71.068    Top5: 94.655    Loss: 0.640

2024-06-06 00:17:34,923 - ==> Confusion:
[[ 803    0    2    0    2    3    0    4    6   67    1    1    1    4   10    0    4    4    5    1   13]
 [   5  867    4    4   19   25    7   32    4    0    4   12    0    2   13    2   16    2   23    6   16]
 [  16    2  801   11    3    5   18   25    2    7    6    4    5    9    6   10    7    7    6    4   16]
 [   3    0   36  808    3    6    1   10    3    2   17    2   10    5   41    1    6    6   41    1   14]
 [  51   17    4    1  862    7    1    5    5   17    0    5    2    3   22    9   19    2    7    1   14]
 [   6   52    1    7   15  709    8   90    3   12    2   19   14   30   10    5   20    5   10   12   13]
 [   4    7   36    5    2    2  914   18    1    2    8    7    3    4    1   25   10    3    9   14   11]
 [   3   13   12    4    1   27    6  913    0    3    2   15    5    3    5    1    4    4   39    9    8]
 [  14    3    1    1    2    1    1    5  772   69   21    2    3   21   49    1    4    2   18    1   11]
 [ 127    0    4    1    5    2    1    3   45  753    2    0    3   19   11    2    1    5    7    0   10]
 [   3    5   12   27    5   11   10   17   26    5  842    1    2   19   19    0    5    2   36    2   15]
 [   4    3    2    1    2   13    2   18    2    1    0  820   32   16    1   28    6   31    4   19    6]
 [   1    1    3   11    0    1    4    9    6    2    1  102  728    7    4   13   10   53   18    3   18]
 [   9    4    2    0    4   16    0    8   17   28    6   11   12  827   14    6    5   12    3    4   13]
 [  12    4    5   21    9    2    1    5   26    9    6    4    4    6  923    1    6    7   39    0    8]
 [   5    2    7    0    3    2    8    1    2    4    0   29   11    2    2  937   17   19    0    4   11]
 [   7    7    4    1    6   11    1    3    6    2    1    8    5    7    4   10  956    2    4   12   15]
 [   2    0    0    5    1    1    3    1    1    4    0   36   40    4    9   18    2  863    3    3    9]
 [   2    2    9   23    2    4    0   52    4    0    4    4    7    2   29    1    1    0  898    6    8]
 [   0    8    5    0    0   13   12   43    0    0    0   32   14    9    2   11   12   13    4  898   12]
 [ 400  299  336  205  242  191  126  410  141  200  146  295  373  438  507  254  613  208  419  410 7719]]

2024-06-06 00:17:34,924 - ==> Best [Top1: 71.068   Top5: 94.655   Sparsity:0.00   Params: 169472 on epoch: 33]
2024-06-06 00:17:34,924 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:17:34,940 - 

2024-06-06 00:17:34,940 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:17:41,588 - Epoch: [34][  100/ 1218]    Overall Loss 0.681077    Objective Loss 0.681077                                        LR 0.001000    Time 0.066459    
2024-06-06 00:17:46,813 - Epoch: [34][  200/ 1218]    Overall Loss 0.688260    Objective Loss 0.688260                                        LR 0.001000    Time 0.059339    
2024-06-06 00:17:51,859 - Epoch: [34][  300/ 1218]    Overall Loss 0.690967    Objective Loss 0.690967                                        LR 0.001000    Time 0.056371    
2024-06-06 00:17:57,149 - Epoch: [34][  400/ 1218]    Overall Loss 0.692468    Objective Loss 0.692468                                        LR 0.001000    Time 0.055498    
2024-06-06 00:18:02,515 - Epoch: [34][  500/ 1218]    Overall Loss 0.692358    Objective Loss 0.692358                                        LR 0.001000    Time 0.055127    
2024-06-06 00:18:07,638 - Epoch: [34][  600/ 1218]    Overall Loss 0.690634    Objective Loss 0.690634                                        LR 0.001000    Time 0.054473    
2024-06-06 00:18:12,822 - Epoch: [34][  700/ 1218]    Overall Loss 0.691518    Objective Loss 0.691518                                        LR 0.001000    Time 0.054093    
2024-06-06 00:18:18,020 - Epoch: [34][  800/ 1218]    Overall Loss 0.692195    Objective Loss 0.692195                                        LR 0.001000    Time 0.053827    
2024-06-06 00:18:23,142 - Epoch: [34][  900/ 1218]    Overall Loss 0.690360    Objective Loss 0.690360                                        LR 0.001000    Time 0.053534    
2024-06-06 00:18:28,133 - Epoch: [34][ 1000/ 1218]    Overall Loss 0.691461    Objective Loss 0.691461                                        LR 0.001000    Time 0.053169    
2024-06-06 00:18:33,306 - Epoch: [34][ 1100/ 1218]    Overall Loss 0.692007    Objective Loss 0.692007                                        LR 0.001000    Time 0.053037    
2024-06-06 00:18:38,370 - Epoch: [34][ 1200/ 1218]    Overall Loss 0.693022    Objective Loss 0.693022                                        LR 0.001000    Time 0.052835    
2024-06-06 00:18:39,287 - Epoch: [34][ 1218/ 1218]    Overall Loss 0.693391    Objective Loss 0.693391    Top1 71.149144    Top5 96.577017    LR 0.001000    Time 0.052806    
2024-06-06 00:18:39,528 - --- validate (epoch=34)-----------
2024-06-06 00:18:39,528 - 34633 samples (256 per mini-batch)
2024-06-06 00:18:45,883 - Epoch: [34][  100/  136]    Loss 0.650946    Top1 69.308594    Top5 94.246094    
2024-06-06 00:18:47,680 - Epoch: [34][  136/  136]    Loss 0.654453    Top1 69.329830    Top5 94.346433    
2024-06-06 00:18:47,885 - ==> Top1: 69.330    Top5: 94.346    Loss: 0.654

2024-06-06 00:18:47,887 - ==> Confusion:
[[ 799    2    6    1    9    2    1    3   13   49    0    0    3    7    7    7    2    0    5    2   13]
 [   5  852    2    2   37   35    7   28    7    1    3    2    5    3    4    6   18    4   24    6   12]
 [  19    1  773   20   11    1   45   17    1    4    3    5    4   11    4    8    5    5    8   14   11]
 [   6    5   27  821    4    9    9    4    2    1   24    3   12    5   29    1    8    8   25    5    8]
 [  46    5    7    1  903   15    0    4    2   16    1    2    0    9   10    9   10    0    5    2    7]
 [   8   46    7   10   15  740   15   55    1    8    2   14   10   46    1    7   13    3    6   20   16]
 [   3   12   29    8    5    2  941    8    2    1    5    5    2    2    0   14    7    4    2   21   13]
 [   5    9   19    1    2   48   10  857    4    3    7   10    5    6    2    1    3    1   43   23   18]
 [  26    4    0    2    1    1    2    3  787   73   12    4    6   34   10    3    5    4   12    1   12]
 [ 173    0    1    2    9    1    2    1   39  703    2    0    3   30   10    3    5    2    5    3    7]
 [   4    6   15   17    5    5    6   10   30    1  880    1    1   27   11    0    5    1   24    5   10]
 [   0    4    3    0    0   20    4   14    1    2    1  777   38   36    2   29    8   29    3   34    6]
 [   1    4    4   15    1    9    2    4    1    0    1   68  741   16    3   16    4   55   12   23   15]
 [   6    2    2    0    6   15    0    5   21   25    6    6    7  853    5    4    8    3    1   14   12]
 [  31    5    4   28   21    4    2    0   48    9    4    0    3   10  870    1    5   10   27    1   15]
 [   7    3    6    0    6    3   15    2    0    2    2   11   15    4    0  926   22   26    1    8    7]
 [   5    7    6    3   10    6    0    1    5    3    2   10    5   11    3   25  928    4    5    9   24]
 [   1    0    2    8    1    1    5    1    3    3    0   24   31   17    3   32    5  846    3   12    7]
 [   2    9    7   34    3    7    4   24    6    2    7    5    5    5   17    3    1    1  896   14    6]
 [   1   11    7    0    2   10   26   16    0    0    3   23    7   20    0   11   18    5    4  913   11]
 [ 444  259  309  237  440  302  207  309  195  167  219  236  437  626  313  349  540  201  358  580 7204]]

2024-06-06 00:18:47,888 - ==> Best [Top1: 71.068   Top5: 94.655   Sparsity:0.00   Params: 169472 on epoch: 33]
2024-06-06 00:18:47,888 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:18:47,897 - 

2024-06-06 00:18:47,898 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:18:54,394 - Epoch: [35][  100/ 1218]    Overall Loss 0.694378    Objective Loss 0.694378                                        LR 0.001000    Time 0.064940    
2024-06-06 00:18:59,325 - Epoch: [35][  200/ 1218]    Overall Loss 0.697484    Objective Loss 0.697484                                        LR 0.001000    Time 0.057109    
2024-06-06 00:19:04,240 - Epoch: [35][  300/ 1218]    Overall Loss 0.690576    Objective Loss 0.690576                                        LR 0.001000    Time 0.054449    
2024-06-06 00:19:09,196 - Epoch: [35][  400/ 1218]    Overall Loss 0.690857    Objective Loss 0.690857                                        LR 0.001000    Time 0.053221    
2024-06-06 00:19:14,518 - Epoch: [35][  500/ 1218]    Overall Loss 0.694998    Objective Loss 0.694998                                        LR 0.001000    Time 0.053217    
2024-06-06 00:19:19,546 - Epoch: [35][  600/ 1218]    Overall Loss 0.695223    Objective Loss 0.695223                                        LR 0.001000    Time 0.052723    
2024-06-06 00:19:24,843 - Epoch: [35][  700/ 1218]    Overall Loss 0.694044    Objective Loss 0.694044                                        LR 0.001000    Time 0.052755    
2024-06-06 00:19:29,815 - Epoch: [35][  800/ 1218]    Overall Loss 0.692624    Objective Loss 0.692624                                        LR 0.001000    Time 0.052373    
2024-06-06 00:19:34,989 - Epoch: [35][  900/ 1218]    Overall Loss 0.692155    Objective Loss 0.692155                                        LR 0.001000    Time 0.052300    
2024-06-06 00:19:39,944 - Epoch: [35][ 1000/ 1218]    Overall Loss 0.693880    Objective Loss 0.693880                                        LR 0.001000    Time 0.052023    
2024-06-06 00:19:44,702 - Epoch: [35][ 1100/ 1218]    Overall Loss 0.694096    Objective Loss 0.694096                                        LR 0.001000    Time 0.051617    
2024-06-06 00:19:49,492 - Epoch: [35][ 1200/ 1218]    Overall Loss 0.693722    Objective Loss 0.693722                                        LR 0.001000    Time 0.051305    
2024-06-06 00:19:50,354 - Epoch: [35][ 1218/ 1218]    Overall Loss 0.693499    Objective Loss 0.693499    Top1 70.904645    Top5 95.110024    LR 0.001000    Time 0.051254    
2024-06-06 00:19:50,555 - --- validate (epoch=35)-----------
2024-06-06 00:19:50,555 - 34633 samples (256 per mini-batch)
2024-06-06 00:19:56,793 - Epoch: [35][  100/  136]    Loss 0.630421    Top1 70.140625    Top5 94.179688    
2024-06-06 00:19:58,644 - Epoch: [35][  136/  136]    Loss 0.633081    Top1 70.112321    Top5 94.118326    
2024-06-06 00:19:58,853 - ==> Top1: 70.112    Top5: 94.118    Loss: 0.633

2024-06-06 00:19:58,854 - ==> Confusion:
[[ 764    2    9    2   11    2    1    5   13   80    2    3    2    7    3    1    8    5    0    1   10]
 [   4  902    6    1   17   28    5   19    6    8    4    1    3    3    7    3   10    3   18    6    9]
 [   6    4  814    7    2    5   32    8    4    5    9    5    3   10    5    4   15    1   17    6    8]
 [   8    4   35  802    5    8    4    1    4    2   28    2    8    6   34    2    9   11   36    1    6]
 [  33   18    8    2  878   14    1    6    2   22    3    2    1   11   13    2    6    1   15    2   14]
 [   7   76    3    9   14  748    9   44   10    7    7   17    7   32    6    4    8    2   11   10   12]
 [   2   13   38    3    6    4  957    9    4    2    8    1    1    1    1    8    5    5    3    7    8]
 [   4   35   23    0    0   47    8  824    5    3    9   14    5    1    3    2    3    4   56   20   11]
 [  15    4    2    0    5    4    0    1  842   45   22    2    3   22   15    0    2    1   12    0    5]
 [  89    4    3    1    6    4    0    0  103  728    3    0    1   33   10    0    0    2    7    1    6]
 [   2    7   13   10    2    2    9    7   22    3  914    0    0   18   11    1    4    0   26    3   10]
 [   2    3    5    1    2   15    7   12    6    1    0  744   78   30    5   18    7   42    5   21    7]
 [   0    2    4   10    0    6    3    4    6    0    2   65  760   12    5   10    7   74   10    8    7]
 [   2    2    7    2    1   13    4    2   27   26    6    7    3  871    7    3    3    2    1    5    7]
 [  18    6    4   28    8    0    1    2   72    9   14    1    5   10  857    2    2    5   41    2   11]
 [   1    5    9    1    4    1   17    1    1    4    1   22   12    4    0  908   29   30    1    3   12]
 [   4   18    9    3    9    6    5    2    7    1    3    8    4    8    1   15  940    3    3    9   14]
 [   5    5    0    0    0    0    1    2    2    3    2   21   31   12    3   12    3  890    4    3    6]
 [   3   11   10   11    2    4    1   24   16    0   15    2    8    5   12    3    1    3  917    4    6]
 [   2   16   13    0    4   16   27   22    2    0    0   23   16   11    2   11   17    7    5  882   12]
 [ 371  480  308  194  304  211  186  205  279  186  291  214  518  524  313  255  697  228  407  421 7340]]

2024-06-06 00:19:58,856 - ==> Best [Top1: 71.068   Top5: 94.655   Sparsity:0.00   Params: 169472 on epoch: 33]
2024-06-06 00:19:58,856 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:19:58,865 - 

2024-06-06 00:19:58,865 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:20:05,501 - Epoch: [36][  100/ 1218]    Overall Loss 0.680982    Objective Loss 0.680982                                        LR 0.001000    Time 0.066334    
2024-06-06 00:20:10,326 - Epoch: [36][  200/ 1218]    Overall Loss 0.684564    Objective Loss 0.684564                                        LR 0.001000    Time 0.057281    
2024-06-06 00:20:15,542 - Epoch: [36][  300/ 1218]    Overall Loss 0.689110    Objective Loss 0.689110                                        LR 0.001000    Time 0.055566    
2024-06-06 00:20:20,544 - Epoch: [36][  400/ 1218]    Overall Loss 0.685146    Objective Loss 0.685146                                        LR 0.001000    Time 0.054173    
2024-06-06 00:20:25,601 - Epoch: [36][  500/ 1218]    Overall Loss 0.686195    Objective Loss 0.686195                                        LR 0.001000    Time 0.053448    
2024-06-06 00:20:30,850 - Epoch: [36][  600/ 1218]    Overall Loss 0.685976    Objective Loss 0.685976                                        LR 0.001000    Time 0.053285    
2024-06-06 00:20:35,645 - Epoch: [36][  700/ 1218]    Overall Loss 0.686169    Objective Loss 0.686169                                        LR 0.001000    Time 0.052520    
2024-06-06 00:20:40,795 - Epoch: [36][  800/ 1218]    Overall Loss 0.684168    Objective Loss 0.684168                                        LR 0.001000    Time 0.052389    
2024-06-06 00:20:45,835 - Epoch: [36][  900/ 1218]    Overall Loss 0.682364    Objective Loss 0.682364                                        LR 0.001000    Time 0.052165    
2024-06-06 00:20:51,012 - Epoch: [36][ 1000/ 1218]    Overall Loss 0.681973    Objective Loss 0.681973                                        LR 0.001000    Time 0.052123    
2024-06-06 00:20:56,150 - Epoch: [36][ 1100/ 1218]    Overall Loss 0.682146    Objective Loss 0.682146                                        LR 0.001000    Time 0.052054    
2024-06-06 00:21:01,403 - Epoch: [36][ 1200/ 1218]    Overall Loss 0.682974    Objective Loss 0.682974                                        LR 0.001000    Time 0.052092    
2024-06-06 00:21:02,250 - Epoch: [36][ 1218/ 1218]    Overall Loss 0.683032    Objective Loss 0.683032    Top1 71.149144    Top5 96.332518    LR 0.001000    Time 0.052016    
2024-06-06 00:21:02,480 - --- validate (epoch=36)-----------
2024-06-06 00:21:02,480 - 34633 samples (256 per mini-batch)
2024-06-06 00:21:08,705 - Epoch: [36][  100/  136]    Loss 0.615542    Top1 71.183594    Top5 94.527344    
2024-06-06 00:21:10,568 - Epoch: [36][  136/  136]    Loss 0.620019    Top1 71.099818    Top5 94.424393    
2024-06-06 00:21:10,799 - ==> Top1: 71.100    Top5: 94.424    Loss: 0.620

2024-06-06 00:21:10,801 - ==> Confusion:
[[ 724    1    4    1   12    6    1    1   17  106    1    2    1    7   10    9    6    4    4    0   14]
 [   1  886    5    4   14   37    7   18    8    3    5    4    7    1    6    4    8    6   24    1   14]
 [   3    5  785   31    3    1   39   14    2   12   10    0    3    7   11    9   10    2    7    4   12]
 [   0    7   28  841    1    9    4    2    1    6   16    2   11    5   39    2    1   11   23    1    6]
 [  24   34    2    1  868   18    2    2    3   18    6    4    0    7   15    8   13    6   13    0   10]
 [   4   44    3   11   12  793    2   41    7    5    8   19   20   29    5    3    4    9    9    1   14]
 [   1   10   37    3    2    5  960    5    1    2    4    5    3    2    3    7    3    7    8   11    7]
 [   2   21   17    4    1   48    6  818    5    8   17   12    8    7    5    0    0    6   68   15    9]
 [   8    9    0    1    0    1    1    1  804   55   18    1    9   27   41    0    2    7   12    1    4]
 [  70    0    0    1    7    1    0    1   71  783    0    1    1   28   17    4    1    6    3    1    5]
 [   1    6    4   22    3    6    5    4   22    4  890    0    4   22   17    2    3    4   28    3   14]
 [   1    0    3    0    0   16    8    4    1    4    1  794   66   13    1    8    3   46    8   23   11]
 [   1    3    4    4    1    4    3    4    6    0    0   81  772   13    3    8    2   69    7    6    4]
 [   1    6    1    1    3   14    3    9   18   23   11   16   11  846   10    4    4    5    2    5    8]
 [  12   13    1   16   15    2    0    0   40   12    6    3   11   10  898    2    4    8   39    0    6]
 [   2    3    7    1    5    1    7    0    0    0    0   29   15    7    0  926    9   44    3    4    3]
 [   2   16    7    5    9    6    3    1    7    3    0   13   19    8    3   17  915   10    7    4   17]
 [   0    1    3    2    0    1    1    0    1    2    1   17   40    7    4   10    1  905    1    4    4]
 [   2    7    9   18    0    4    2   15   10    0    6    2    8    1   26    0    0    3  936    2    7]
 [   2   11    5    1    2   14   26   19    1    1    1   54   20   13    2   13    5    9    6  869   14]
 [ 217  405  293  207  245  314  142  231  197  200  241  294  587  538  437  248  382  334  441  368 7611]]

2024-06-06 00:21:10,803 - ==> Best [Top1: 71.100   Top5: 94.424   Sparsity:0.00   Params: 169472 on epoch: 36]
2024-06-06 00:21:10,803 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:21:10,821 - 

2024-06-06 00:21:10,821 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:21:17,414 - Epoch: [37][  100/ 1218]    Overall Loss 0.674037    Objective Loss 0.674037                                        LR 0.001000    Time 0.065893    
2024-06-06 00:21:22,536 - Epoch: [37][  200/ 1218]    Overall Loss 0.678152    Objective Loss 0.678152                                        LR 0.001000    Time 0.058549    
2024-06-06 00:21:27,636 - Epoch: [37][  300/ 1218]    Overall Loss 0.680691    Objective Loss 0.680691                                        LR 0.001000    Time 0.056023    
2024-06-06 00:21:32,563 - Epoch: [37][  400/ 1218]    Overall Loss 0.679857    Objective Loss 0.679857                                        LR 0.001000    Time 0.054330    
2024-06-06 00:21:37,538 - Epoch: [37][  500/ 1218]    Overall Loss 0.681183    Objective Loss 0.681183                                        LR 0.001000    Time 0.053409    
2024-06-06 00:21:42,840 - Epoch: [37][  600/ 1218]    Overall Loss 0.684893    Objective Loss 0.684893                                        LR 0.001000    Time 0.053340    
2024-06-06 00:21:47,920 - Epoch: [37][  700/ 1218]    Overall Loss 0.685557    Objective Loss 0.685557                                        LR 0.001000    Time 0.052974    
2024-06-06 00:21:52,905 - Epoch: [37][  800/ 1218]    Overall Loss 0.685363    Objective Loss 0.685363                                        LR 0.001000    Time 0.052580    
2024-06-06 00:21:58,056 - Epoch: [37][  900/ 1218]    Overall Loss 0.685109    Objective Loss 0.685109                                        LR 0.001000    Time 0.052459    
2024-06-06 00:22:03,128 - Epoch: [37][ 1000/ 1218]    Overall Loss 0.684632    Objective Loss 0.684632                                        LR 0.001000    Time 0.052283    
2024-06-06 00:22:08,321 - Epoch: [37][ 1100/ 1218]    Overall Loss 0.685517    Objective Loss 0.685517                                        LR 0.001000    Time 0.052248    
2024-06-06 00:22:13,558 - Epoch: [37][ 1200/ 1218]    Overall Loss 0.686753    Objective Loss 0.686753                                        LR 0.001000    Time 0.052257    
2024-06-06 00:22:14,433 - Epoch: [37][ 1218/ 1218]    Overall Loss 0.686363    Objective Loss 0.686363    Top1 71.149144    Top5 93.643032    LR 0.001000    Time 0.052202    
2024-06-06 00:22:14,652 - --- validate (epoch=37)-----------
2024-06-06 00:22:14,653 - 34633 samples (256 per mini-batch)
2024-06-06 00:22:20,724 - Epoch: [37][  100/  136]    Loss 0.630193    Top1 68.843750    Top5 93.945312    
2024-06-06 00:22:22,596 - Epoch: [37][  136/  136]    Loss 0.631461    Top1 69.075737    Top5 93.872896    
2024-06-06 00:22:22,829 - ==> Top1: 69.076    Top5: 93.873    Loss: 0.631

2024-06-06 00:22:22,831 - ==> Confusion:
[[ 767    2    3    0   17    9    1    6   10   70    0    8    1    1   11    7    2    2    6    2    6]
 [   1  884    1    0   20   36    6   17    9    2    5   10    1    3    7    7    9    3   25    8    9]
 [  13    7  763   12    4    6   49   22    1    3    3   12    2    8    5    9   11    4    8   15   13]
 [   0    5   32  807    7   17    6    5    3    1   20    6   17    3   30    4    1    5   26    9   12]
 [  32   16    0    2  894   23    1    5    3    8    5    6    1    3   12    5    9    3   13    2   11]
 [   3   42    1    0   18  823    8   34    6    4    3   39    5   15    2    1   11    2    4   16    6]
 [   3   11   18    0    2   11  966   16    0    3    3   14    1    1    1    8    3    2    5   13    5]
 [   6   27   11    4    2   53    9  840    5    0    3   29    4    3    4    0    1    2   51   20    3]
 [   8   12    0    0    2   15    1    2  795   49   23    5    4   23   38    0    2    2   15    1    5]
 [ 100    1    1    1   10    6    3    3  106  689    2    4    0   32   24    1    1    5    4    1    7]
 [   3   16   13   12    2    9    7   20   10    0  887    5    5   17    5    1    5    1   36    4    6]
 [   2    2    1    0    1   17    2    5    2    0    1  892   20    6    2   17    3   11    5   17    5]
 [   2    0    1    2    0    8    2    5    5    0    0  170  725    2    3   10    3   33   12    6    6]
 [   3    8    2    2    3   22    1    4   21   13   11   45    6  826    6    5    4    3    1    6    9]
 [  13    9    4   23   13    7    0    3   41    6    8    3    5    6  922    0    2    5   18    3    7]
 [   3    2    4    0    1    4    8    0    0    0    0   34   11    1    0  962   18   11    2    2    3]
 [   4   16    6    3   10   15    1    1    7    1    1   22    5    5    1   13  935    1    6   11    8]
 [   2    3    0    4    1    7    3    2    3    1    0   81   51    4    3   20    3  799    4    3   11]
 [   2   13    9   15   10    6    2   35   12    0   10    8    4    1   18    1    3    2  893    5    9]
 [   2    8    0    2    0   11   12   28    1    0    0   78    8    2    1    9   10    3    4  894   15]
 [ 341  473  299  145  327  442  206  304  209  109  248  647  433  418  385  333  574  158  415  506 6960]]

2024-06-06 00:22:22,832 - ==> Best [Top1: 71.100   Top5: 94.424   Sparsity:0.00   Params: 169472 on epoch: 36]
2024-06-06 00:22:22,832 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:22:22,840 - 

2024-06-06 00:22:22,840 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:22:29,548 - Epoch: [38][  100/ 1218]    Overall Loss 0.683062    Objective Loss 0.683062                                        LR 0.001000    Time 0.067048    
2024-06-06 00:22:34,766 - Epoch: [38][  200/ 1218]    Overall Loss 0.675276    Objective Loss 0.675276                                        LR 0.001000    Time 0.059603    
2024-06-06 00:22:39,857 - Epoch: [38][  300/ 1218]    Overall Loss 0.679037    Objective Loss 0.679037                                        LR 0.001000    Time 0.056698    
2024-06-06 00:22:45,033 - Epoch: [38][  400/ 1218]    Overall Loss 0.679451    Objective Loss 0.679451                                        LR 0.001000    Time 0.055457    
2024-06-06 00:22:50,168 - Epoch: [38][  500/ 1218]    Overall Loss 0.679052    Objective Loss 0.679052                                        LR 0.001000    Time 0.054631    
2024-06-06 00:22:55,553 - Epoch: [38][  600/ 1218]    Overall Loss 0.678703    Objective Loss 0.678703                                        LR 0.001000    Time 0.054497    
2024-06-06 00:23:00,885 - Epoch: [38][  700/ 1218]    Overall Loss 0.679430    Objective Loss 0.679430                                        LR 0.001000    Time 0.054325    
2024-06-06 00:23:06,154 - Epoch: [38][  800/ 1218]    Overall Loss 0.678301    Objective Loss 0.678301                                        LR 0.001000    Time 0.054119    
2024-06-06 00:23:11,633 - Epoch: [38][  900/ 1218]    Overall Loss 0.679413    Objective Loss 0.679413                                        LR 0.001000    Time 0.054183    
2024-06-06 00:23:16,838 - Epoch: [38][ 1000/ 1218]    Overall Loss 0.681336    Objective Loss 0.681336                                        LR 0.001000    Time 0.053965    
2024-06-06 00:23:22,047 - Epoch: [38][ 1100/ 1218]    Overall Loss 0.681186    Objective Loss 0.681186                                        LR 0.001000    Time 0.053793    
2024-06-06 00:23:27,204 - Epoch: [38][ 1200/ 1218]    Overall Loss 0.681992    Objective Loss 0.681992                                        LR 0.001000    Time 0.053605    
2024-06-06 00:23:28,018 - Epoch: [38][ 1218/ 1218]    Overall Loss 0.681612    Objective Loss 0.681612    Top1 70.415648    Top5 94.376528    LR 0.001000    Time 0.053481    
2024-06-06 00:23:28,217 - --- validate (epoch=38)-----------
2024-06-06 00:23:28,218 - 34633 samples (256 per mini-batch)
2024-06-06 00:23:34,393 - Epoch: [38][  100/  136]    Loss 0.600371    Top1 71.199219    Top5 94.835938    
2024-06-06 00:23:36,168 - Epoch: [38][  136/  136]    Loss 0.616206    Top1 70.730228    Top5 94.620737    
2024-06-06 00:23:36,359 - ==> Top1: 70.730    Top5: 94.621    Loss: 0.616

2024-06-06 00:23:36,360 - ==> Confusion:
[[ 769    1    3    1   16    2    1    6    7   75    1    4    0    3   10    4    5    2    1    3   17]
 [   4  862    6    1   26   44    3   32    8    4    5    7    2    0   11    2   15    2   14    6    9]
 [  10    2  796   10    8    0   31   20    2    8   13    5    9    3    4    6   15    2    8    8   10]
 [   2    5   40  815    2    5    7    4    5    7   35    6    9    1   36    2    3    6   13    4    9]
 [  25   11    3    2  899   15    4    5    3   17    2    7    1    4   13   11   10    0    5    4   13]
 [   7   38    4    7   16  791    6   52    3    7    5   26    5   16    4    2   12    3    9   22    8]
 [   2    5   35    2    1    3  955   15    1    0    3    7    4    2    1   16    6    1    3   15    9]
 [   7   15   28    6    5   35   10  851    5    4   10   10    7    2    3    2    1    1   36   21   18]
 [  13    4    1    1    1    4    1    5  801   56   23    3    8   19   29    1    6    3   12    3    8]
 [  92    0    4    2   10    2    2    1   64  774    2    3    1   10   16    1    1    4    0    1   11]
 [   6    9   10   12    3    5   12   12   20    2  914    4    6    6   11    0    2    0   22    0    8]
 [   4    4    3    0    2   16    4    8    0    2    1  819   49    6    0   17    5   23    2   34   12]
 [   0    2    3    7    0    7    0    5    3    0    2   98  783    3    6   14    8   27    5   15    7]
 [   5    8    2    0    4   25    4   10   34   26   16   18    9  797    7    4    9    1    1   12    9]
 [  10    9    4   21   19    1    2    3   41   12   10    2    7   12  905    0    6    3   18    2   11]
 [   4    1    7    1    2    2   10    1    1    3    0   28   18    2    0  946    8   13    0   11    8]
 [   5   14    7    1    9    6    6    6    5    1    3    8    9    2    2   22  926    2    3   11   24]
 [   4    3    0    4    2    1    8    2    3    2    2   43   54    5    6   17    5  820    0   12   12]
 [   1   16   11   13    6    7    2   38   11    2    9    3    8    2   29    1    0    0  887    5    7]
 [   0   10    5    0    2   13   20   22    0    1    1   35   16    7    2    1   10    3    4  929    7]
 [ 320  359  318  174  326  292  218  277  229  192  333  306  578  336  331  291  594  112  322  567 7457]]

2024-06-06 00:23:36,362 - ==> Best [Top1: 71.100   Top5: 94.424   Sparsity:0.00   Params: 169472 on epoch: 36]
2024-06-06 00:23:36,362 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:23:36,375 - 

2024-06-06 00:23:36,375 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:23:43,062 - Epoch: [39][  100/ 1218]    Overall Loss 0.689354    Objective Loss 0.689354                                        LR 0.001000    Time 0.066837    
2024-06-06 00:23:48,366 - Epoch: [39][  200/ 1218]    Overall Loss 0.686638    Objective Loss 0.686638                                        LR 0.001000    Time 0.059929    
2024-06-06 00:23:53,368 - Epoch: [39][  300/ 1218]    Overall Loss 0.685892    Objective Loss 0.685892                                        LR 0.001000    Time 0.056619    
2024-06-06 00:23:58,533 - Epoch: [39][  400/ 1218]    Overall Loss 0.689859    Objective Loss 0.689859                                        LR 0.001000    Time 0.055371    
2024-06-06 00:24:03,606 - Epoch: [39][  500/ 1218]    Overall Loss 0.689024    Objective Loss 0.689024                                        LR 0.001000    Time 0.054437    
2024-06-06 00:24:08,949 - Epoch: [39][  600/ 1218]    Overall Loss 0.690469    Objective Loss 0.690469                                        LR 0.001000    Time 0.054267    
2024-06-06 00:24:13,928 - Epoch: [39][  700/ 1218]    Overall Loss 0.685159    Objective Loss 0.685159                                        LR 0.001000    Time 0.053623    
2024-06-06 00:24:18,992 - Epoch: [39][  800/ 1218]    Overall Loss 0.683972    Objective Loss 0.683972                                        LR 0.001000    Time 0.053248    
2024-06-06 00:24:24,470 - Epoch: [39][  900/ 1218]    Overall Loss 0.682597    Objective Loss 0.682597                                        LR 0.001000    Time 0.053416    
2024-06-06 00:24:29,516 - Epoch: [39][ 1000/ 1218]    Overall Loss 0.682954    Objective Loss 0.682954                                        LR 0.001000    Time 0.053118    
2024-06-06 00:24:34,714 - Epoch: [39][ 1100/ 1218]    Overall Loss 0.682568    Objective Loss 0.682568                                        LR 0.001000    Time 0.053013    
2024-06-06 00:24:39,870 - Epoch: [39][ 1200/ 1218]    Overall Loss 0.681283    Objective Loss 0.681283                                        LR 0.001000    Time 0.052890    
2024-06-06 00:24:40,869 - Epoch: [39][ 1218/ 1218]    Overall Loss 0.681516    Objective Loss 0.681516    Top1 67.970660    Top5 94.132029    LR 0.001000    Time 0.052927    
2024-06-06 00:24:41,077 - --- validate (epoch=39)-----------
2024-06-06 00:24:41,077 - 34633 samples (256 per mini-batch)
2024-06-06 00:24:47,386 - Epoch: [39][  100/  136]    Loss 0.611634    Top1 70.035156    Top5 93.902344    
2024-06-06 00:24:49,229 - Epoch: [39][  136/  136]    Loss 0.620166    Top1 69.973724    Top5 93.818035    
2024-06-06 00:24:49,455 - ==> Top1: 69.974    Top5: 93.818    Loss: 0.620

2024-06-06 00:24:49,457 - ==> Confusion:
[[ 801    3    5    0   14    1    1    3    5   59    1    3    1    5    5    2    6    4    1    1   10]
 [   1  886    3    5   20   31   14   25    8    1    7   13    4    1    4    2   14    2    6    8    8]
 [   9    5  786   18    5    5   67   10    1    2    7    6    5    3    4    6    4    1    9   10    7]
 [   6    4   38  839    3    9    8    3    2    2   22    4    9    6   25    5    5    6   13    4    3]
 [  51   16   17    1  871   21    5    6    0   15    4    7    1    4    3    3   10    2    4    6    7]
 [   3   36   10    8   17  786    6   51    4    4    5   31   13   25    3    4    7    1    5   17    7]
 [   1    2   28    4    1    4  976   14    1    0    6    6    1    0    1   10    4    0    2   14   11]
 [   5   19   26    3    1   49   12  865    4    0    8    7    9    4    1    0    6    2   23   20   13]
 [  12    9    3    1    3    3    1    2  801   68   19    2   11   17   30    0    2    1    7    4    6]
 [ 130    1    8    3   10    2    1    2   49  725    3    2    2   31   11    1    2    4    2    4    8]
 [   6    5   17   24    2    4   15    6   13    1  930    2    2   10    2    0    1    0   15    5    4]
 [   3    2    9    0    1    9    5    5    0    0    3  809   55   13    3   21    8   22    3   29   11]
 [   1    1    3    7    0    3    6    6    2    0    2   79  802    6    3   11    7   32    8    9    7]
 [   4    2    7    0    7   20    7    8   21   17   17   21    7  817    9    4    6    2    2   10   13]
 [  16   12    9   30   22    4    1    2   36   10   17    1    3    9  888    0    7    1   22    2    6]
 [   4    1    6    3    5    2   14    0    1    0    0   22   16    3    1  922   27   19    4   11    5]
 [   2   13    2    4    5    8    6    3    6    2    5    9    8    6    1   12  953    1    3   10   13]
 [   2    1    2    7    0    4    6    1    4    3    1   36   69    7    3   19    2  818    6    6    8]
 [   0   13   16   31    4    6    4   42   11    3   19    4    5    1   20    2    3    3  864    3    4]
 [   0    9    9    0    1   10   30    7    2    1    4   24   10    6    0    7   13    3    3  942    7]
 [ 380  415  501  219  333  310  319  277  150  145  313  286  567  450  265  264  689  144  233  519 7153]]

2024-06-06 00:24:49,459 - ==> Best [Top1: 71.100   Top5: 94.424   Sparsity:0.00   Params: 169472 on epoch: 36]
2024-06-06 00:24:49,459 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:24:49,471 - 

2024-06-06 00:24:49,471 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:24:56,231 - Epoch: [40][  100/ 1218]    Overall Loss 0.669511    Objective Loss 0.669511                                        LR 0.001000    Time 0.067572    
2024-06-06 00:25:01,459 - Epoch: [40][  200/ 1218]    Overall Loss 0.669466    Objective Loss 0.669466                                        LR 0.001000    Time 0.059912    
2024-06-06 00:25:06,475 - Epoch: [40][  300/ 1218]    Overall Loss 0.676155    Objective Loss 0.676155                                        LR 0.001000    Time 0.056654    
2024-06-06 00:25:11,639 - Epoch: [40][  400/ 1218]    Overall Loss 0.677728    Objective Loss 0.677728                                        LR 0.001000    Time 0.055380    
2024-06-06 00:25:16,953 - Epoch: [40][  500/ 1218]    Overall Loss 0.675617    Objective Loss 0.675617                                        LR 0.001000    Time 0.054926    
2024-06-06 00:25:22,320 - Epoch: [40][  600/ 1218]    Overall Loss 0.677043    Objective Loss 0.677043                                        LR 0.001000    Time 0.054714    
2024-06-06 00:25:27,247 - Epoch: [40][  700/ 1218]    Overall Loss 0.675969    Objective Loss 0.675969                                        LR 0.001000    Time 0.053932    
2024-06-06 00:25:32,265 - Epoch: [40][  800/ 1218]    Overall Loss 0.677072    Objective Loss 0.677072                                        LR 0.001000    Time 0.053460    
2024-06-06 00:25:37,437 - Epoch: [40][  900/ 1218]    Overall Loss 0.678537    Objective Loss 0.678537                                        LR 0.001000    Time 0.053264    
2024-06-06 00:25:42,487 - Epoch: [40][ 1000/ 1218]    Overall Loss 0.679525    Objective Loss 0.679525                                        LR 0.001000    Time 0.052986    
2024-06-06 00:25:48,047 - Epoch: [40][ 1100/ 1218]    Overall Loss 0.679092    Objective Loss 0.679092                                        LR 0.001000    Time 0.053221    
2024-06-06 00:25:53,186 - Epoch: [40][ 1200/ 1218]    Overall Loss 0.680243    Objective Loss 0.680243                                        LR 0.001000    Time 0.053067    
2024-06-06 00:25:53,976 - Epoch: [40][ 1218/ 1218]    Overall Loss 0.680139    Objective Loss 0.680139    Top1 69.437653    Top5 95.354523    LR 0.001000    Time 0.052931    
2024-06-06 00:25:54,202 - --- validate (epoch=40)-----------
2024-06-06 00:25:54,202 - 34633 samples (256 per mini-batch)
2024-06-06 00:26:00,520 - Epoch: [40][  100/  136]    Loss 0.630667    Top1 70.757812    Top5 94.113281    
2024-06-06 00:26:02,478 - Epoch: [40][  136/  136]    Loss 0.625526    Top1 70.600295    Top5 94.115439    
2024-06-06 00:26:02,675 - ==> Top1: 70.600    Top5: 94.115    Loss: 0.626

2024-06-06 00:26:02,677 - ==> Confusion:
[[ 790    1    5    0   10    0    1    4   17   63    0    1    1    7   12    1    3    3    3    4    5]
 [   1  903    5    0   29   21    7   13    7    2    9    6    1    1    4    1   10    1   22    6   14]
 [   8    6  793    7    9    4   37   14    0    3   12    3    3    6    9    4   13    3   10   13   13]
 [   6    3   46  789    5    6    5    2    3    3   27    2    7    3   52    3    7    3   30    3   11]
 [  37   16    3    1  903   16    4    2    2   10    3    5    0    1   15    7   13    1    8    0    7]
 [  11   55    6    4   22  735   16   43    7    7    9   23    8   36   10    3    6    2    9   10   21]
 [   5    4   34    2    3    7  966    9    2    0    4    3    1    1    2    5    3    4    4   19    8]
 [   2   25   20    0    3   53   11  834    6    4    7    9    4    7    1    1    1    1   57   22    9]
 [  15    5    2    2    2    2    3    3  796   62   22    6    3    7   41    0    5    5   13    2    6]
 [ 115    2    1    0    5    1    1    4   76  740    3    0    1   12   23    2    2    1    4    1    7]
 [   5    1   13    9    4    6    8    9   19    1  916    3    2    7   15    0    6    0   28    3    9]
 [   4    2    4    2    4   11   10   10    2    5    0  774   35   10    1   24   15   41    7   37   13]
 [   4    4    6   11    0    8    2    7    8    0    4   85  714    4    3   14    3   83   12   10   13]
 [   2    1    4    3    7   15    3    7   30   37   20   10    3  799   10    1    9   16    1   16    7]
 [  16    4    8    9   14    1    0    0   33   10   13    0    2    7  933    0    5    6   20    2   15]
 [   6    7    5    2   10    3   21    1    0    3    0   20   14    3    0  915   17   20    3    5   11]
 [   3   18    5    0   19    9    7    2   10    2    0    8    4    2    3   15  942    2    2    9   10]
 [   2    5    1    4    3    3    6    2    5    1    1   16   33    6    5   12    0  878    6    9    7]
 [   7    9   13   21    2    2    1   21   12    2    9    3    4    2   31    0    4    1  900    2   12]
 [   4   11    3    0    2   16   18   16    0    1    2   25    6    6    0    9   12    2    7  938   10]
 [ 424  357  344  137  397  245  236  261  227  173  297  214  419  377  424  201  598  200  352  556 7493]]

2024-06-06 00:26:02,679 - ==> Best [Top1: 71.100   Top5: 94.424   Sparsity:0.00   Params: 169472 on epoch: 36]
2024-06-06 00:26:02,679 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:26:02,691 - 

2024-06-06 00:26:02,692 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:26:09,324 - Epoch: [41][  100/ 1218]    Overall Loss 0.673574    Objective Loss 0.673574                                        LR 0.001000    Time 0.066292    
2024-06-06 00:26:14,472 - Epoch: [41][  200/ 1218]    Overall Loss 0.666991    Objective Loss 0.666991                                        LR 0.001000    Time 0.058872    
2024-06-06 00:26:19,623 - Epoch: [41][  300/ 1218]    Overall Loss 0.664451    Objective Loss 0.664451                                        LR 0.001000    Time 0.056410    
2024-06-06 00:26:24,797 - Epoch: [41][  400/ 1218]    Overall Loss 0.666759    Objective Loss 0.666759                                        LR 0.001000    Time 0.055237    
2024-06-06 00:26:29,747 - Epoch: [41][  500/ 1218]    Overall Loss 0.670284    Objective Loss 0.670284                                        LR 0.001000    Time 0.054085    
2024-06-06 00:26:34,872 - Epoch: [41][  600/ 1218]    Overall Loss 0.671685    Objective Loss 0.671685                                        LR 0.001000    Time 0.053609    
2024-06-06 00:26:40,050 - Epoch: [41][  700/ 1218]    Overall Loss 0.672999    Objective Loss 0.672999                                        LR 0.001000    Time 0.053345    
2024-06-06 00:26:45,056 - Epoch: [41][  800/ 1218]    Overall Loss 0.674128    Objective Loss 0.674128                                        LR 0.001000    Time 0.052931    
2024-06-06 00:26:49,988 - Epoch: [41][  900/ 1218]    Overall Loss 0.675455    Objective Loss 0.675455                                        LR 0.001000    Time 0.052527    
2024-06-06 00:26:55,086 - Epoch: [41][ 1000/ 1218]    Overall Loss 0.676732    Objective Loss 0.676732                                        LR 0.001000    Time 0.052370    
2024-06-06 00:27:00,098 - Epoch: [41][ 1100/ 1218]    Overall Loss 0.676046    Objective Loss 0.676046                                        LR 0.001000    Time 0.052164    
2024-06-06 00:27:04,978 - Epoch: [41][ 1200/ 1218]    Overall Loss 0.676405    Objective Loss 0.676405                                        LR 0.001000    Time 0.051881    
2024-06-06 00:27:05,860 - Epoch: [41][ 1218/ 1218]    Overall Loss 0.676351    Objective Loss 0.676351    Top1 72.127139    Top5 93.154034    LR 0.001000    Time 0.051838    
2024-06-06 00:27:06,051 - --- validate (epoch=41)-----------
2024-06-06 00:27:06,051 - 34633 samples (256 per mini-batch)
2024-06-06 00:27:12,098 - Epoch: [41][  100/  136]    Loss 0.631762    Top1 70.277344    Top5 93.480469    
2024-06-06 00:27:14,005 - Epoch: [41][  136/  136]    Loss 0.634537    Top1 70.132533    Top5 93.485982    
2024-06-06 00:27:14,233 - ==> Top1: 70.133    Top5: 93.486    Loss: 0.635

2024-06-06 00:27:14,234 - ==> Confusion:
[[ 797    0    2    1   11    5    1    1   12   49    1    2    2    6   11    4    5    3    5    6    7]
 [   6  828    2    5   18   55    7   23    5    1    7   13    4    4   14    8   19    4   21   10    9]
 [  12    4  757   39    3    4   43    7    1    6   14    5    6    6    5   21    6    4    6   13    8]
 [   2    0   10  869    3    6    6    3    1    0   19    1   20    6   27    4    6    8    8    3   14]
 [  49    4    3    3  866   21    3    2    3   10    2    3    4   10   18   14   11    2    8    3   15]
 [   4   24    5   12   19  805    5   23    5    3    0   34   13   20    5   10    8    7    9   18   14]
 [   2    1   21    9    1    4  951    4    0    0    1   12    3    2    0   26    4    5    3   24   13]
 [   4   14   25    8    4   66    8  782    4    0    7   21    9    2    6    1    2    7   56   42    9]
 [  19    5    1    3    6   10    0    0  786   61   11    5   12   17   31    3    1    4   11    4   12]
 [ 153    1    3    1   10    7    1    0   58  695    1    1    1   33   17    4    0    6    3    0    6]
 [   5    1    7   28    2    4    5    5   24    2  895    2    8   19   18    3    1    0   16    8   11]
 [   2    1    7    1    0   14    3    5    0    0    0  843   37    6    3   24    5   28    3   19   10]
 [   0    0    1   10    1    0    3    2    3    0    0   98  776    6    0   10    5   51    7    8   14]
 [   6    0    0    3    4   18    0    3   20   11   11   22    9  851    7    7    4    7    2    7    9]
 [  16    2    1   21    9    3    1    2   46   10    6    5   11    6  921    4    3    6   14    1   10]
 [   3    2    3    1    1    3    6    0    0    2    0   23   13    1    1  974    3   21    2    0    7]
 [   6    6    8    5    7    9    2    1    9    1    2   14   11    4    6   35  904    6    6   15   15]
 [   1    1    0    4    2    3    2    1    3    0    0   40   54    4    5   19    2  854    1    3    6]
 [   5    8    7   39    5    4    3   18    7    1   10    7   17    0   39    0    5    2  871    3    7]
 [   1    4    1    2    2    5   15    7    2    0    0   53   20    7    1   14    6    7    7  924   10]
 [ 375  175  312  354  271  309  173  227  195  133  270  380  641  483  460  430  418  240  309  438 7339]]

2024-06-06 00:27:14,236 - ==> Best [Top1: 71.100   Top5: 94.424   Sparsity:0.00   Params: 169472 on epoch: 36]
2024-06-06 00:27:14,236 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:27:14,244 - 

2024-06-06 00:27:14,244 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:27:20,774 - Epoch: [42][  100/ 1218]    Overall Loss 0.676234    Objective Loss 0.676234                                        LR 0.001000    Time 0.065269    
2024-06-06 00:27:25,988 - Epoch: [42][  200/ 1218]    Overall Loss 0.669686    Objective Loss 0.669686                                        LR 0.001000    Time 0.058696    
2024-06-06 00:27:30,980 - Epoch: [42][  300/ 1218]    Overall Loss 0.674079    Objective Loss 0.674079                                        LR 0.001000    Time 0.055762    
2024-06-06 00:27:35,866 - Epoch: [42][  400/ 1218]    Overall Loss 0.670327    Objective Loss 0.670327                                        LR 0.001000    Time 0.054031    
2024-06-06 00:27:41,128 - Epoch: [42][  500/ 1218]    Overall Loss 0.671440    Objective Loss 0.671440                                        LR 0.001000    Time 0.053745    
2024-06-06 00:27:46,371 - Epoch: [42][  600/ 1218]    Overall Loss 0.671423    Objective Loss 0.671423                                        LR 0.001000    Time 0.053521    
2024-06-06 00:27:51,546 - Epoch: [42][  700/ 1218]    Overall Loss 0.672072    Objective Loss 0.672072                                        LR 0.001000    Time 0.053265    
2024-06-06 00:27:56,947 - Epoch: [42][  800/ 1218]    Overall Loss 0.673255    Objective Loss 0.673255                                        LR 0.001000    Time 0.053354    
2024-06-06 00:28:02,012 - Epoch: [42][  900/ 1218]    Overall Loss 0.675202    Objective Loss 0.675202                                        LR 0.001000    Time 0.053051    
2024-06-06 00:28:07,005 - Epoch: [42][ 1000/ 1218]    Overall Loss 0.676191    Objective Loss 0.676191                                        LR 0.001000    Time 0.052737    
2024-06-06 00:28:12,148 - Epoch: [42][ 1100/ 1218]    Overall Loss 0.676214    Objective Loss 0.676214                                        LR 0.001000    Time 0.052616    
2024-06-06 00:28:17,269 - Epoch: [42][ 1200/ 1218]    Overall Loss 0.675818    Objective Loss 0.675818                                        LR 0.001000    Time 0.052496    
2024-06-06 00:28:18,121 - Epoch: [42][ 1218/ 1218]    Overall Loss 0.675819    Objective Loss 0.675819    Top1 69.926650    Top5 93.887531    LR 0.001000    Time 0.052420    
2024-06-06 00:28:18,306 - --- validate (epoch=42)-----------
2024-06-06 00:28:18,306 - 34633 samples (256 per mini-batch)
2024-06-06 00:28:24,394 - Epoch: [42][  100/  136]    Loss 0.602418    Top1 70.851562    Top5 94.851562    
2024-06-06 00:28:26,225 - Epoch: [42][  136/  136]    Loss 0.613786    Top1 70.738891    Top5 94.799757    
2024-06-06 00:28:26,441 - ==> Top1: 70.739    Top5: 94.800    Loss: 0.614

2024-06-06 00:28:26,443 - ==> Confusion:
[[ 814    1    3    0    6    1    1    4    9   62    1    0    0    3    5    1    4    4    3    2    7]
 [   6  918    2    0   21   23    3   20   10    3    3    4    3    3    7    2    3    1   12    6   13]
 [  20    7  793   13    7    1   26   15    0    7    7    4    7    7    6    8    7    3    7    9   16]
 [   9    0   26  822    4    8    4    3    2    2   24    2    8    6   40    3    6    9   23    2   13]
 [  45   31    2    1  869   13    0    5    3   22    1    2    1    7    9    4   14    1    9    1   14]
 [  13   62    3    6   14  794   11   40    7   11    1   13    5   19    6    0    3    5    9   13    8]
 [   4   13   40    1    1    7  940   10    2    1    7    3    4    2    3   15    4    4    2   14    9]
 [   6   26   20    5    5   43    3  849    8    2    7   10    6    3    2    0    4    4   33   27   14]
 [  25    7    2    1    0    0    0    4  801   64   12    4    9   25   22    1    2    1   10    1   11]
 [ 135    2    0    0    3    3    2    4   70  729    2    0    0   16   15    0    2    4    5    1    8]
 [   9   11   14   11    3    3    4   10   28    3  879    3    0   32   11    2    6    0   22    3   10]
 [   4    3    4    1    0   19    4    7    2    2    0  808   52   18    2   18   11   22    2   26    6]
 [   3    1    3    6    1    4    3    5    5    0    0   88  783    9    5    9   13   35    7    5   10]
 [  14    2    3    1    6   12    2    1   17   34    6   15    6  847    9    4    3    4    2    4    9]
 [  18    7    2   17    9    2    1    1   54   13    4    2    5    7  905    0    4    5   27    3   12]
 [  10    2    3    4    4    1   12    0    2    0    0   20    7    9    0  932   23   18    5    4   10]
 [   4   14    7    2   11    6    2    3    9    2    0    8    6    6    4    8  950    3    2   11   14]
 [   4    6    3    1    3    3    3    3    2    6    0   24   55   15    8   20    4  829    7    1    8]
 [   7   20   12   28    0    3    0   28    7    1    6    6    4    1   24    1    1    1  893    5   10]
 [   4   13    3    0    1   13   18   16    0    0    1   20   12   12    5   10   12   10    9  925    4]
 [ 542  398  346  193  338  248  151  271  224  203  169  236  520  473  390  228  537  181  330  535 7419]]

2024-06-06 00:28:26,444 - ==> Best [Top1: 71.100   Top5: 94.424   Sparsity:0.00   Params: 169472 on epoch: 36]
2024-06-06 00:28:26,444 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:28:26,453 - 

2024-06-06 00:28:26,453 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:28:33,230 - Epoch: [43][  100/ 1218]    Overall Loss 0.662500    Objective Loss 0.662500                                        LR 0.001000    Time 0.067739    
2024-06-06 00:28:38,440 - Epoch: [43][  200/ 1218]    Overall Loss 0.660235    Objective Loss 0.660235                                        LR 0.001000    Time 0.059907    
2024-06-06 00:28:43,453 - Epoch: [43][  300/ 1218]    Overall Loss 0.665561    Objective Loss 0.665561                                        LR 0.001000    Time 0.056642    
2024-06-06 00:28:48,292 - Epoch: [43][  400/ 1218]    Overall Loss 0.662155    Objective Loss 0.662155                                        LR 0.001000    Time 0.054572    
2024-06-06 00:28:53,464 - Epoch: [43][  500/ 1218]    Overall Loss 0.664139    Objective Loss 0.664139                                        LR 0.001000    Time 0.053997    
2024-06-06 00:28:58,829 - Epoch: [43][  600/ 1218]    Overall Loss 0.665964    Objective Loss 0.665964                                        LR 0.001000    Time 0.053936    
2024-06-06 00:29:03,764 - Epoch: [43][  700/ 1218]    Overall Loss 0.665805    Objective Loss 0.665805                                        LR 0.001000    Time 0.053277    
2024-06-06 00:29:09,031 - Epoch: [43][  800/ 1218]    Overall Loss 0.665343    Objective Loss 0.665343                                        LR 0.001000    Time 0.053198    
2024-06-06 00:29:14,190 - Epoch: [43][  900/ 1218]    Overall Loss 0.664906    Objective Loss 0.664906                                        LR 0.001000    Time 0.053016    
2024-06-06 00:29:19,332 - Epoch: [43][ 1000/ 1218]    Overall Loss 0.666544    Objective Loss 0.666544                                        LR 0.001000    Time 0.052854    
2024-06-06 00:29:24,276 - Epoch: [43][ 1100/ 1218]    Overall Loss 0.666330    Objective Loss 0.666330                                        LR 0.001000    Time 0.052542    
2024-06-06 00:29:29,340 - Epoch: [43][ 1200/ 1218]    Overall Loss 0.668056    Objective Loss 0.668056                                        LR 0.001000    Time 0.052382    
2024-06-06 00:29:30,148 - Epoch: [43][ 1218/ 1218]    Overall Loss 0.668438    Objective Loss 0.668438    Top1 67.970660    Top5 94.621027    LR 0.001000    Time 0.052271    
2024-06-06 00:29:30,343 - --- validate (epoch=43)-----------
2024-06-06 00:29:30,343 - 34633 samples (256 per mini-batch)
2024-06-06 00:29:36,483 - Epoch: [43][  100/  136]    Loss 0.630997    Top1 71.609375    Top5 94.558594    
2024-06-06 00:29:38,348 - Epoch: [43][  136/  136]    Loss 0.625560    Top1 71.582017    Top5 94.548552    
2024-06-06 00:29:38,527 - ==> Top1: 71.582    Top5: 94.549    Loss: 0.626

2024-06-06 00:29:38,528 - ==> Confusion:
[[ 732    3    3    2    3    4    1    4    9  116    3    2    0    7   10    1    4    5    3    3   16]
 [   2  934    1    3   16   19    6   14    6    2    7    6    3    2   12    0    7    3   10    4    6]
 [   9    9  767   17    5    1   60   14    3    6    8    7    2    6    5    9    5    1    7    7   22]
 [   5    2   29  806    6    8   12    2    2    4   29    3    9    3   43    1    4    6   28    1   13]
 [  31   27   11    1  867    9    2    2    4   22    3    3    1    8   23    8    9    2    8    2   11]
 [   8   54    4    7   18  787    8   41    6    7    6   22    7   21    4    2    7    2    5   13   14]
 [   1    5   29    1    1    6  985    5    0    0    5    7    0    0    1    6    4    2    1   15   12]
 [   4   40   30    0    1   53   10  830    4    3    9   12    4    4    2    2    3    2   43   12    9]
 [   4   13    0    4    2    0    1    3  812   56   21    2    3   22   33    1    4    4    7    1    9]
 [  53    5    1    0    9    5    2    6   69  792    1    3    1   16   12    2    6    4    1    4    9]
 [   2   15   15   16    1    5    7    6   20    0  910    2    2   14   15    0    1    0   16    9    8]
 [   2    7    4    0    3   25    6   10    1    0    0  812   31    9    0   18    8   17    6   38   14]
 [   1    1    6    7    1    3    2    6    1    0    4  116  715    1    6   17   12   58    8   14   16]
 [   4    3    4    0    7   23    2    7   29   29   13   16    8  800    6    7    7    4    2   18   12]
 [  13   13    5   12   12    2    0    0   63    9    6    5    3   11  904    1    5    6    9    4   15]
 [   2    5    7    0    3    4   17    1    1    1    0   22    6    3    0  950   12   13    1    5   13]
 [   6   19    6    3   11    4    5    3    4    1    1   15    2    3    6   20  929    4    3    9   18]
 [   2    6    0    2    1    3    6    1    4    2    0   50   33    4    6   25    4  833    3    8   12]
 [   2   14   14   19    4    4    3   33   11    0   17    2    4    0   24    2    3    3  884    4   11]
 [   1   14    7    1    2   14   31   26    2    1    1   32   11    0    0   13   10    3    6  902   11]
 [ 246  457  296  130  274  325  250  251  213  217  243  307  388  386  355  300  593  158  251  452 7840]]

2024-06-06 00:29:38,530 - ==> Best [Top1: 71.582   Top5: 94.549   Sparsity:0.00   Params: 169472 on epoch: 43]
2024-06-06 00:29:38,530 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:29:38,546 - 

2024-06-06 00:29:38,546 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:29:45,089 - Epoch: [44][  100/ 1218]    Overall Loss 0.674743    Objective Loss 0.674743                                        LR 0.001000    Time 0.065396    
2024-06-06 00:29:49,907 - Epoch: [44][  200/ 1218]    Overall Loss 0.676265    Objective Loss 0.676265                                        LR 0.001000    Time 0.056780    
2024-06-06 00:29:54,947 - Epoch: [44][  300/ 1218]    Overall Loss 0.677100    Objective Loss 0.677100                                        LR 0.001000    Time 0.054646    
2024-06-06 00:30:00,164 - Epoch: [44][  400/ 1218]    Overall Loss 0.676390    Objective Loss 0.676390                                        LR 0.001000    Time 0.054019    
2024-06-06 00:30:05,216 - Epoch: [44][  500/ 1218]    Overall Loss 0.673841    Objective Loss 0.673841                                        LR 0.001000    Time 0.053317    
2024-06-06 00:30:10,658 - Epoch: [44][  600/ 1218]    Overall Loss 0.677577    Objective Loss 0.677577                                        LR 0.001000    Time 0.053496    
2024-06-06 00:30:15,721 - Epoch: [44][  700/ 1218]    Overall Loss 0.680209    Objective Loss 0.680209                                        LR 0.001000    Time 0.053083    
2024-06-06 00:30:20,912 - Epoch: [44][  800/ 1218]    Overall Loss 0.678497    Objective Loss 0.678497                                        LR 0.001000    Time 0.052934    
2024-06-06 00:30:25,785 - Epoch: [44][  900/ 1218]    Overall Loss 0.676416    Objective Loss 0.676416                                        LR 0.001000    Time 0.052464    
2024-06-06 00:30:31,040 - Epoch: [44][ 1000/ 1218]    Overall Loss 0.675386    Objective Loss 0.675386                                        LR 0.001000    Time 0.052470    
2024-06-06 00:30:36,211 - Epoch: [44][ 1100/ 1218]    Overall Loss 0.675521    Objective Loss 0.675521                                        LR 0.001000    Time 0.052399    
2024-06-06 00:30:41,529 - Epoch: [44][ 1200/ 1218]    Overall Loss 0.675516    Objective Loss 0.675516                                        LR 0.001000    Time 0.052463    
2024-06-06 00:30:42,331 - Epoch: [44][ 1218/ 1218]    Overall Loss 0.675434    Objective Loss 0.675434    Top1 67.970660    Top5 93.643032    LR 0.001000    Time 0.052345    
2024-06-06 00:30:42,536 - --- validate (epoch=44)-----------
2024-06-06 00:30:42,536 - 34633 samples (256 per mini-batch)
2024-06-06 00:30:48,824 - Epoch: [44][  100/  136]    Loss 0.612967    Top1 72.773438    Top5 94.968750    
2024-06-06 00:30:50,701 - Epoch: [44][  136/  136]    Loss 0.613874    Top1 72.615713    Top5 94.785320    
2024-06-06 00:30:50,881 - ==> Top1: 72.616    Top5: 94.785    Loss: 0.614

2024-06-06 00:30:50,882 - ==> Confusion:
[[ 758    3    9    2   15    4    1    6   13   81    1    1    3    5    7    4    2    2    5    0    9]
 [   2  932    1    1   19   18    6   17   12    2    6    4    2    2    5    2    3    2   13    2   12]
 [  15    4  744   31    8    4   59   18    2    6   15    3    4    4    0   12    5    3   11    3   19]
 [   1    9   20  821    4   10    3    3    4    3   31    2   12    5   34    8    1   13   14    3   15]
 [  34   23    0    1  873   22    4    2    6   20    3    5    3   11   11    6    3    2    7    1   17]
 [   8   76    3    7   14  759   10   43    8    2    9   20    7   31    2    4    1    3   10   11   15]
 [   0   11   25    1    1    8  965    5    2    4    8    2    1    4    1   17    3    4    3    5   16]
 [   3   30    9    4    3   45    8  850    8    4    8   12    5    2    0    2    1    5   49   16   13]
 [  15    6    0    0    2    3    2    1  810   58   22    3    9   24   20    0    0    2   14    0   11]
 [  77    1    2    1    4    7    0    0   76  780    4    1    0   23    9    1    2    5    2    2    4]
 [   1   11    4   10    2    1    6   11   29    2  913    1    3   19   15    0    0    2   18    4   12]
 [   2    3    1    1    1   14    6    9    4    0    2  792   46   16    3   25    6   36    6   25   13]
 [   2    3    3    9    0    7    3    5    5    0    1   63  755    8    6   15    4   75   12    9   10]
 [   4    2    2    0    5   23    2    8   19   27   11   22    4  828    2    1    5    8    3    7   18]
 [   6    7    2   19    9    5    0    0   63   13    5    1    3   10  907    0    2    4   22    1   19]
 [   3    3    2    1    2    3   17    1    3    1    1   18    9    8    0  946    7   20    2    4   15]
 [   8   16    7    5    9    6    3    4    8    2    3    9    8    8    4   20  909    8    5    9   21]
 [   2    3    1    4    0    0    0    2    9    5    2   23   28    9    4   14    1  881    3    2   12]
 [   3   10    8   15    3    3    3   21   23    1   10    1    3    1   20    0    2    1  916    4   10]
 [   1   10    2    3    5   13   24   24    0    0    2   34   16   13    1    9   11   11    8  881   20]
 [ 306  433  229  192  263  247  172  256  259  214  280  222  454  426  342  244  338  227  332  367 8129]]

2024-06-06 00:30:50,884 - ==> Best [Top1: 72.616   Top5: 94.785   Sparsity:0.00   Params: 169472 on epoch: 44]
2024-06-06 00:30:50,884 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:30:50,899 - 

2024-06-06 00:30:50,899 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:30:57,566 - Epoch: [45][  100/ 1218]    Overall Loss 0.676973    Objective Loss 0.676973                                        LR 0.001000    Time 0.066640    
2024-06-06 00:31:02,765 - Epoch: [45][  200/ 1218]    Overall Loss 0.671949    Objective Loss 0.671949                                        LR 0.001000    Time 0.059302    
2024-06-06 00:31:07,755 - Epoch: [45][  300/ 1218]    Overall Loss 0.674362    Objective Loss 0.674362                                        LR 0.001000    Time 0.056160    
2024-06-06 00:31:12,817 - Epoch: [45][  400/ 1218]    Overall Loss 0.674527    Objective Loss 0.674527                                        LR 0.001000    Time 0.054770    
2024-06-06 00:31:17,773 - Epoch: [45][  500/ 1218]    Overall Loss 0.675862    Objective Loss 0.675862                                        LR 0.001000    Time 0.053723    
2024-06-06 00:31:23,213 - Epoch: [45][  600/ 1218]    Overall Loss 0.673593    Objective Loss 0.673593                                        LR 0.001000    Time 0.053832    
2024-06-06 00:31:28,104 - Epoch: [45][  700/ 1218]    Overall Loss 0.671348    Objective Loss 0.671348                                        LR 0.001000    Time 0.053126    
2024-06-06 00:31:32,970 - Epoch: [45][  800/ 1218]    Overall Loss 0.671015    Objective Loss 0.671015                                        LR 0.001000    Time 0.052563    
2024-06-06 00:31:38,120 - Epoch: [45][  900/ 1218]    Overall Loss 0.670215    Objective Loss 0.670215                                        LR 0.001000    Time 0.052441    
2024-06-06 00:31:43,250 - Epoch: [45][ 1000/ 1218]    Overall Loss 0.669095    Objective Loss 0.669095                                        LR 0.001000    Time 0.052325    
2024-06-06 00:31:48,202 - Epoch: [45][ 1100/ 1218]    Overall Loss 0.669543    Objective Loss 0.669543                                        LR 0.001000    Time 0.052068    
2024-06-06 00:31:53,264 - Epoch: [45][ 1200/ 1218]    Overall Loss 0.668718    Objective Loss 0.668718                                        LR 0.001000    Time 0.051945    
2024-06-06 00:31:54,132 - Epoch: [45][ 1218/ 1218]    Overall Loss 0.668824    Objective Loss 0.668824    Top1 69.926650    Top5 94.621027    LR 0.001000    Time 0.051890    
2024-06-06 00:31:54,312 - --- validate (epoch=45)-----------
2024-06-06 00:31:54,312 - 34633 samples (256 per mini-batch)
2024-06-06 00:32:00,553 - Epoch: [45][  100/  136]    Loss 0.599913    Top1 72.058594    Top5 94.679688    
2024-06-06 00:32:02,398 - Epoch: [45][  136/  136]    Loss 0.603853    Top1 72.107528    Top5 94.620737    
2024-06-06 00:32:02,610 - ==> Top1: 72.108    Top5: 94.621    Loss: 0.604

2024-06-06 00:32:02,611 - ==> Confusion:
[[ 769    0   10    2   15    3    0    5    7   92    0    3    1    4    5    1    0    1    3    1    9]
 [   6  873   10    3   31   28    6   27    6    4    2    5    4    3    9    3    8    2   19    6    8]
 [  11    4  851    5    7    1   19   16    1    5    6    5    4    4    3    9    0    1    5    4    9]
 [   6    2   50  820    4    7    3    5    2    3   26    2   12    3   23    3    4    2   28    2    9]
 [  42    9    1    4  903    9    2    8    4   25    1    3    2    3    9    5    7    0    8    2    7]
 [   7   54   10    3   21  743    6   62    7   10    2   22   10   24    3    4    9    3   12   16   15]
 [   6   10   52    2    1    4  928   11    1    2    5    5    3    4    0   11    2    5    1   19   14]
 [   6   14   21    1    6   28    3  868    2    3    6   15    7    0    3    0    2    3   53   19   17]
 [  20    7    3    2    3    3    0    3  782   61   15    3    5   28   31    3    1    3   21    2    6]
 [  93    2    3    1   11    2    1    2   50  782    2    1    0   20   13    2    0    5    2    3    6]
 [   4    4   17   14    0    6    3   10   17    6  905    0    1   12    4    0    6    0   37    3   15]
 [   3    4    4    1    3   12    3   10    0    1    0  836   29   11    1   16   12   18    4   21   22]
 [   3    0    3   11    0    6    2    4    1    1    2  102  747    3    5   13    7   43   15   10   17]
 [   4    1    7    0    3   15    1    4   18   33   11    9    6  838    5    8    4    4    2   12   16]
 [  21    2    4   28   10    5    1    2   20   13    6    1    3    8  919    0    3    3   30    3   16]
 [   6    1    7    2    3    2   12    0    0    0    0   30    6    1    0  948    5   14    6    9   14]
 [  10   15    9    1   11    8    7    3    5    0    1    9    4    7    4   10  926    1    7   13   21]
 [   5    2    2    5    2    3    2    2    1    4    1   40   52    5    9   26    3  816    2   10   13]
 [   2    7   15   17    1    3    2   27    8    2    8    4    3    1   21    0    2    1  921    4    9]
 [   1    9    8    2    1   11   12   18    0    0    0   38    9    2    0    9    6    2    2  947   11]
 [ 367  301  466  176  385  219  142  299  154  170  250  332  464  418  333  241  360  137  423  444 7851]]

2024-06-06 00:32:02,613 - ==> Best [Top1: 72.616   Top5: 94.785   Sparsity:0.00   Params: 169472 on epoch: 44]
2024-06-06 00:32:02,613 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:32:02,623 - 

2024-06-06 00:32:02,623 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:32:09,477 - Epoch: [46][  100/ 1218]    Overall Loss 0.684436    Objective Loss 0.684436                                        LR 0.001000    Time 0.068515    
2024-06-06 00:32:14,468 - Epoch: [46][  200/ 1218]    Overall Loss 0.675533    Objective Loss 0.675533                                        LR 0.001000    Time 0.059198    
2024-06-06 00:32:19,649 - Epoch: [46][  300/ 1218]    Overall Loss 0.672176    Objective Loss 0.672176                                        LR 0.001000    Time 0.056727    
2024-06-06 00:32:24,744 - Epoch: [46][  400/ 1218]    Overall Loss 0.672412    Objective Loss 0.672412                                        LR 0.001000    Time 0.055277    
2024-06-06 00:32:29,784 - Epoch: [46][  500/ 1218]    Overall Loss 0.671006    Objective Loss 0.671006                                        LR 0.001000    Time 0.054297    
2024-06-06 00:32:34,771 - Epoch: [46][  600/ 1218]    Overall Loss 0.671091    Objective Loss 0.671091                                        LR 0.001000    Time 0.053556    
2024-06-06 00:32:40,209 - Epoch: [46][  700/ 1218]    Overall Loss 0.671050    Objective Loss 0.671050                                        LR 0.001000    Time 0.053670    
2024-06-06 00:32:45,208 - Epoch: [46][  800/ 1218]    Overall Loss 0.671103    Objective Loss 0.671103                                        LR 0.001000    Time 0.053207    
2024-06-06 00:32:50,410 - Epoch: [46][  900/ 1218]    Overall Loss 0.672073    Objective Loss 0.672073                                        LR 0.001000    Time 0.053066    
2024-06-06 00:32:55,687 - Epoch: [46][ 1000/ 1218]    Overall Loss 0.671961    Objective Loss 0.671961                                        LR 0.001000    Time 0.053033    
2024-06-06 00:33:00,734 - Epoch: [46][ 1100/ 1218]    Overall Loss 0.673547    Objective Loss 0.673547                                        LR 0.001000    Time 0.052792    
2024-06-06 00:33:05,764 - Epoch: [46][ 1200/ 1218]    Overall Loss 0.673852    Objective Loss 0.673852                                        LR 0.001000    Time 0.052583    
2024-06-06 00:33:06,564 - Epoch: [46][ 1218/ 1218]    Overall Loss 0.673994    Objective Loss 0.673994    Top1 69.926650    Top5 94.376528    LR 0.001000    Time 0.052457    
2024-06-06 00:33:06,770 - --- validate (epoch=46)-----------
2024-06-06 00:33:06,770 - 34633 samples (256 per mini-batch)
2024-06-06 00:33:13,140 - Epoch: [46][  100/  136]    Loss 0.623563    Top1 70.695312    Top5 94.359375    
2024-06-06 00:33:15,077 - Epoch: [46][  136/  136]    Loss 0.613408    Top1 70.877487    Top5 94.363757    
2024-06-06 00:33:15,275 - ==> Top1: 70.877    Top5: 94.364    Loss: 0.613

2024-06-06 00:33:15,277 - ==> Confusion:
[[ 767    1    6    1    5    0    2    3    4   99    3    2    1    1    7    3    4    3    4    0   15]
 [  10  873    7    2   29   24    4   23    9    1   10    3    2    2   15    5   13    2   13    5   11]
 [  11    3  820   13    4    1   30   12    2    9   18    6    3    8    3    5    8    3    3    5    3]
 [   4    1   34  794    1   10    4    6    6    6   51    1    6    2   46    1    3    6   19    3   12]
 [  40   11    8    0  877   13    1    2    2   27    7    3    1    6   12    5   16    3    9    2    9]
 [   7   47    6    9   26  741    3   63    6   14   11   16   10   39    2    2    9    5    4    6   17]
 [   2    4   24    4    0    4  965   10    0    2   15    5    2    1    3   13    4    8    5    9    6]
 [   4   17   18    7    1   35   11  861    8    9   12    7    5    7    5    2    1    1   46   11    9]
 [  18    5    1    3    0    2    0    0  801   67   24    1    5   15   33    0    3    8    6    1    9]
 [  81    1    3    0    2    1    2    0   69  790    3    0    1   23    8    2    1    5    3    2    4]
 [   5    0   14   12    3    4    4    8   18    1  939    0    2   13   16    0    3    1   11    2    8]
 [   3    3    7    2    2   34    6   16    4    3    6  699   69   28    3   23    5   46    7   35   10]
 [   2    1    3   15    0    9    1    7    9    1    6   54  772   12    3   10    4   57   10    9   10]
 [   6    1    6    0    3    7    0    5   27   35   18    5    2  844    9    4    3    6    4    4   12]
 [  12    7    7   14    8    3    2    1   33   16   12    0    3    7  938    1    0    4   20    0   10]
 [   7    3    8    0    1    1    8    0    0    1    1   12   13    7    0  953   16   24    2    2    7]
 [   2    8    5    8    4    9    2    2    7    5    3    6    8    4    0   15  944    6    6    7   21]
 [   1    2    2    3    0    2    2    3    5    2    1    7   38   14    6   13    4  879    3    6   12]
 [   1    1    9   20    3    2    2   24    7    3   29    2    6    1   37    4    3    3  883    7   11]
 [   3    9   12    1    3   12   18   33    3    0    7   26    7   14    1    5   10    8    3  902   11]
 [ 337  303  408  257  316  225  228  245  236  254  357  152  487  481  467  275  508  224  296  371 7505]]

2024-06-06 00:33:15,279 - ==> Best [Top1: 72.616   Top5: 94.785   Sparsity:0.00   Params: 169472 on epoch: 44]
2024-06-06 00:33:15,279 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:33:15,291 - 

2024-06-06 00:33:15,291 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:33:21,965 - Epoch: [47][  100/ 1218]    Overall Loss 0.673988    Objective Loss 0.673988                                        LR 0.001000    Time 0.066705    
2024-06-06 00:33:26,854 - Epoch: [47][  200/ 1218]    Overall Loss 0.674198    Objective Loss 0.674198                                        LR 0.001000    Time 0.057780    
2024-06-06 00:33:31,950 - Epoch: [47][  300/ 1218]    Overall Loss 0.673741    Objective Loss 0.673741                                        LR 0.001000    Time 0.055498    
2024-06-06 00:33:37,172 - Epoch: [47][  400/ 1218]    Overall Loss 0.666139    Objective Loss 0.666139                                        LR 0.001000    Time 0.054672    
2024-06-06 00:33:42,341 - Epoch: [47][  500/ 1218]    Overall Loss 0.665559    Objective Loss 0.665559                                        LR 0.001000    Time 0.054072    
2024-06-06 00:33:47,750 - Epoch: [47][  600/ 1218]    Overall Loss 0.667301    Objective Loss 0.667301                                        LR 0.001000    Time 0.054072    
2024-06-06 00:33:52,671 - Epoch: [47][  700/ 1218]    Overall Loss 0.668908    Objective Loss 0.668908                                        LR 0.001000    Time 0.053372    
2024-06-06 00:33:57,704 - Epoch: [47][  800/ 1218]    Overall Loss 0.671179    Objective Loss 0.671179                                        LR 0.001000    Time 0.052990    
2024-06-06 00:34:02,889 - Epoch: [47][  900/ 1218]    Overall Loss 0.673053    Objective Loss 0.673053                                        LR 0.001000    Time 0.052860    
2024-06-06 00:34:07,962 - Epoch: [47][ 1000/ 1218]    Overall Loss 0.672541    Objective Loss 0.672541                                        LR 0.001000    Time 0.052645    
2024-06-06 00:34:12,883 - Epoch: [47][ 1100/ 1218]    Overall Loss 0.673127    Objective Loss 0.673127                                        LR 0.001000    Time 0.052330    
2024-06-06 00:34:18,187 - Epoch: [47][ 1200/ 1218]    Overall Loss 0.672750    Objective Loss 0.672750                                        LR 0.001000    Time 0.052388    
2024-06-06 00:34:19,044 - Epoch: [47][ 1218/ 1218]    Overall Loss 0.672513    Objective Loss 0.672513    Top1 75.550122    Top5 95.354523    LR 0.001000    Time 0.052316    
2024-06-06 00:34:19,276 - --- validate (epoch=47)-----------
2024-06-06 00:34:19,277 - 34633 samples (256 per mini-batch)
2024-06-06 00:34:25,340 - Epoch: [47][  100/  136]    Loss 0.608839    Top1 72.636719    Top5 94.957031    
2024-06-06 00:34:27,167 - Epoch: [47][  136/  136]    Loss 0.612007    Top1 72.589727    Top5 95.042301    
2024-06-06 00:34:27,410 - ==> Top1: 72.590    Top5: 95.042    Loss: 0.612

2024-06-06 00:34:27,411 - ==> Confusion:
[[ 782    0    5    1    6    5    0    2   12   77    1    2    1    2    9    1    3    3    1    3   15]
 [   2  881    5    1   21   38    2   31   11    4    7    4    2    1    4    3    9    4   12    7   14]
 [  12    0  828   13    3    6   12   15    2    5   10    3    4    8    2    6   13    0    9    8   11]
 [   6    0   28  844    3    9    3    1    4    3   22    5    6    4   38    3    6    9   13    0    9]
 [  45   15    6    7  854   22    1    6    3   16    2    2    1    4   22    5   15    1    9    1   17]
 [   7   42    8    6   13  803    1   57    9    7    4   15    4   31    2    2    6    5    3    9    9]
 [   2   12   62    4    1   14  889   18    3    3    7    7    2    3    1   11    9    6    6    9   17]
 [   7   19   14    5    1   35    3  885    6    2   10   14    2    5    3    3    3    3   28   10   19]
 [  14    1    2    2    1    3    0    1  849   50    9    4    6   12   20    0    7    4   10    0    7]
 [ 103    0    2    0    3    7    0    4   65  769    2    0    1   17    9    3    0    4    0    1   11]
 [   3    6   19   18    0    9    1   15   27    3  916    2    3   12    7    0    2    1    8    3    9]
 [   4    4    2    0    2   22    0   13    4    1    2  822   24   13    0   10    6   48    3   14   17]
 [   0    2    1   11    0    8    2    5    5    0    3  106  721    5    3   16   11   68    8    4   16]
 [   6    3    4    4   10   10    0    5   30   31    6   13    7  828    7    4    4    7    2    5   15]
 [   9    8    4   25    7    1    0    2   51   13    8    3    1    4  918    0    2    6   11    1   24]
 [   6    2    9    0    5    4    8    2    0    3    1   33   14    1    1  901   29   26    0    4   17]
 [   7    7    6    3    7   15    3    5    5    3    3   12    3    6    1    7  941    4    3    4   27]
 [   1    2    0    3    0    1    1    4    6    4    3   22   35    8    5   13    4  879    0    4   10]
 [   5    5   10   22    2    5    1   37   15    1   13    2    7    1   26    3    2    0  873    6   22]
 [   3   10   10    2    2   16   15   24    1    1    2   48   13   13    0    7   17    4    3  880   17]
 [ 393  237  363  195  240  288  112  304  250  240  226  369  399  369  286  156  632  198  225  373 8077]]

2024-06-06 00:34:27,413 - ==> Best [Top1: 72.616   Top5: 94.785   Sparsity:0.00   Params: 169472 on epoch: 44]
2024-06-06 00:34:27,413 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:34:27,425 - 

2024-06-06 00:34:27,425 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:34:34,365 - Epoch: [48][  100/ 1218]    Overall Loss 0.664744    Objective Loss 0.664744                                        LR 0.001000    Time 0.069371    
2024-06-06 00:34:39,374 - Epoch: [48][  200/ 1218]    Overall Loss 0.661779    Objective Loss 0.661779                                        LR 0.001000    Time 0.059715    
2024-06-06 00:34:44,676 - Epoch: [48][  300/ 1218]    Overall Loss 0.656053    Objective Loss 0.656053                                        LR 0.001000    Time 0.057478    
2024-06-06 00:34:50,013 - Epoch: [48][  400/ 1218]    Overall Loss 0.659451    Objective Loss 0.659451                                        LR 0.001000    Time 0.056444    
2024-06-06 00:34:55,149 - Epoch: [48][  500/ 1218]    Overall Loss 0.659901    Objective Loss 0.659901                                        LR 0.001000    Time 0.055424    
2024-06-06 00:35:00,695 - Epoch: [48][  600/ 1218]    Overall Loss 0.663135    Objective Loss 0.663135                                        LR 0.001000    Time 0.055425    
2024-06-06 00:35:05,746 - Epoch: [48][  700/ 1218]    Overall Loss 0.665783    Objective Loss 0.665783                                        LR 0.001000    Time 0.054719    
2024-06-06 00:35:10,930 - Epoch: [48][  800/ 1218]    Overall Loss 0.666000    Objective Loss 0.666000                                        LR 0.001000    Time 0.054356    
2024-06-06 00:35:16,156 - Epoch: [48][  900/ 1218]    Overall Loss 0.666135    Objective Loss 0.666135                                        LR 0.001000    Time 0.054121    
2024-06-06 00:35:21,357 - Epoch: [48][ 1000/ 1218]    Overall Loss 0.665952    Objective Loss 0.665952                                        LR 0.001000    Time 0.053908    
2024-06-06 00:35:26,450 - Epoch: [48][ 1100/ 1218]    Overall Loss 0.666570    Objective Loss 0.666570                                        LR 0.001000    Time 0.053635    
2024-06-06 00:35:31,430 - Epoch: [48][ 1200/ 1218]    Overall Loss 0.664811    Objective Loss 0.664811                                        LR 0.001000    Time 0.053314    
2024-06-06 00:35:32,332 - Epoch: [48][ 1218/ 1218]    Overall Loss 0.664619    Objective Loss 0.664619    Top1 71.393643    Top5 94.376528    LR 0.001000    Time 0.053265    
2024-06-06 00:35:32,567 - --- validate (epoch=48)-----------
2024-06-06 00:35:32,567 - 34633 samples (256 per mini-batch)
2024-06-06 00:35:38,833 - Epoch: [48][  100/  136]    Loss 0.589217    Top1 72.703125    Top5 94.921875    
2024-06-06 00:35:40,686 - Epoch: [48][  136/  136]    Loss 0.591181    Top1 72.621488    Top5 94.900817    
2024-06-06 00:35:40,903 - ==> Top1: 72.621    Top5: 94.901    Loss: 0.591

2024-06-06 00:35:40,904 - ==> Confusion:
[[ 751    3    4    2   19    3    0    5    9  101    1    2    1    6    3    0    1    2    1    2   15]
 [   6  904    3    0   25   26    1   29    2    4    2    7    4    2   13    2   10    1   10    3    9]
 [  10    3  788   18    8    4   33   22    1    4   11    6    1    6    3    6    9    5    2    9   21]
 [   3    3   23  836   12    9    3    3    4    2   27    3    8    2   38    1    6    9   13    4    7]
 [  27    9    4    1  930   12    2    7    4   11    0    5    1    1    8    6   11    0    4    4    7]
 [   6   50    6    3   21  784    3   47    3    9    3   17   11   21    8    2    9    2    8   16   14]
 [   5   12   22    5    3    7  941   13    0    2    5    8    5    0    0    9    8    6    2   19   14]
 [   9   19   13    6    7   31    2  898    6    2    5   15    8    0    2    0    0    2   19   21   12]
 [  20    5    0    4    4    3    1    1  781   82   10    6    6   23   26    0    3    4    8    2   13]
 [  87    2    5    0    6    0    0    4   53  802    0    1    0   18   10    1    2    4    0    1    5]
 [   2    5   10   12    4    4    5   13   17    2  917    1    5   11   18    1    0    0   27    5    5]
 [   2    1    1    0    1   20    2   12    2    1    0  803   56   14    1   12    4   26    0   33   20]
 [   0    2    1    8    0    5    2    5    2    0    1   95  759    3    1    3    5   48    5   26   24]
 [   5    4    2    0    7   30    2    6   21   27   16   17    4  800   11    4    7    8    3   15   12]
 [  11    8    2   12   18    5    0    1   37   18    5    3    1    2  938    0    1    3   21    4    8]
 [   5    5    3    1    8    1   14    0    0    1    0   37   12    3    0  898   30   25    4    7   12]
 [   4   12    4    1   11    8    3    3    3    5    2    9    6    6    3    9  947    1    3   15   17]
 [   0    6    1    2    3    2    1    1    0    2    0   34   45    6    6    7    2  861    3    9   14]
 [   7   12    7   27    8    6    2   48   10    3    5    5    7    2   18    0    2    0  874    2   13]
 [   1    9    5    0    2   11   11   19    2    0    0   24    9    7    1    4    8    3    2  961    9]
 [ 341  399  239  201  434  271  150  350  172  213  199  303  467  372  328  160  409  167  255  524 7978]]

2024-06-06 00:35:40,906 - ==> Best [Top1: 72.621   Top5: 94.901   Sparsity:0.00   Params: 169472 on epoch: 48]
2024-06-06 00:35:40,906 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:35:40,925 - 

2024-06-06 00:35:40,925 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:35:47,601 - Epoch: [49][  100/ 1218]    Overall Loss 0.638615    Objective Loss 0.638615                                        LR 0.001000    Time 0.066726    
2024-06-06 00:35:52,545 - Epoch: [49][  200/ 1218]    Overall Loss 0.655199    Objective Loss 0.655199                                        LR 0.001000    Time 0.058073    
2024-06-06 00:35:57,637 - Epoch: [49][  300/ 1218]    Overall Loss 0.659599    Objective Loss 0.659599                                        LR 0.001000    Time 0.055681    
2024-06-06 00:36:02,790 - Epoch: [49][  400/ 1218]    Overall Loss 0.666541    Objective Loss 0.666541                                        LR 0.001000    Time 0.054635    
2024-06-06 00:36:07,900 - Epoch: [49][  500/ 1218]    Overall Loss 0.667783    Objective Loss 0.667783                                        LR 0.001000    Time 0.053925    
2024-06-06 00:36:13,195 - Epoch: [49][  600/ 1218]    Overall Loss 0.667400    Objective Loss 0.667400                                        LR 0.001000    Time 0.053758    
2024-06-06 00:36:18,257 - Epoch: [49][  700/ 1218]    Overall Loss 0.667936    Objective Loss 0.667936                                        LR 0.001000    Time 0.053306    
2024-06-06 00:36:23,318 - Epoch: [49][  800/ 1218]    Overall Loss 0.669818    Objective Loss 0.669818                                        LR 0.001000    Time 0.052966    
2024-06-06 00:36:28,446 - Epoch: [49][  900/ 1218]    Overall Loss 0.669221    Objective Loss 0.669221                                        LR 0.001000    Time 0.052776    
2024-06-06 00:36:33,355 - Epoch: [49][ 1000/ 1218]    Overall Loss 0.669240    Objective Loss 0.669240                                        LR 0.001000    Time 0.052406    
2024-06-06 00:36:38,685 - Epoch: [49][ 1100/ 1218]    Overall Loss 0.667011    Objective Loss 0.667011                                        LR 0.001000    Time 0.052484    
2024-06-06 00:36:43,873 - Epoch: [49][ 1200/ 1218]    Overall Loss 0.667074    Objective Loss 0.667074                                        LR 0.001000    Time 0.052432    
2024-06-06 00:36:44,758 - Epoch: [49][ 1218/ 1218]    Overall Loss 0.666389    Objective Loss 0.666389    Top1 69.926650    Top5 96.088020    LR 0.001000    Time 0.052384    
2024-06-06 00:36:44,966 - --- validate (epoch=49)-----------
2024-06-06 00:36:44,966 - 34633 samples (256 per mini-batch)
2024-06-06 00:36:51,216 - Epoch: [49][  100/  136]    Loss 0.601973    Top1 70.695312    Top5 94.480469    
2024-06-06 00:36:53,035 - Epoch: [49][  136/  136]    Loss 0.603986    Top1 70.565645    Top5 94.464817    
2024-06-06 00:36:53,220 - ==> Top1: 70.566    Top5: 94.465    Loss: 0.604

2024-06-06 00:36:53,221 - ==> Confusion:
[[ 787    3    3    0   18    1    1    2   20   65    0    1    0    1    5    6    6    3    3    0    6]
 [   5  910    3    2   25   34    4   17    6    1    4    5    1    3    1    1   11    2   12    5   11]
 [  15    8  787   12    3    4   44   15    1    2    6    7    5    9    2    7    9    0    9   16    9]
 [   4    7   43  807    7   10    5    4    2    3   19    3   11    6   38    1    4    7   28    2    5]
 [  38   12    3    2  919   17    1    0    3   10    2    4    1    8    7    5   11    1    5    0    5]
 [  10   45    6    4   23  821    1   37    4    6    4   13    9   25    3    3    4    2    8    7    8]
 [   1    6   36    0    5    7  952    8    3    0    5    6    5    1    1    9    5    4    6   18    8]
 [   5   25   15    3    5   53    8  835    4    4    7   12    6    6    3    2    0    1   51   21   11]
 [  19   14    1    2    2    4    0    2  799   66   14    3    2   21   21    0    7    2   12    1   10]
 [ 111    1    2    0   11    4    0    1   47  776    0    3    2   20    5    2    3    2    2    2    7]
 [   2    8   17   12    8    6    8   10   22    2  884    2    1   22   16    0    3    1   26    2   12]
 [   1    2    4    0    2   19    3    8    3    5    1  825   43   10    0   15   10   27    5   22    6]
 [   0    2    4    8    2    9    3    7    5    1    1   91  768    8    5    8    4   37   11    7   14]
 [  10    2    3    4    7   18    3    2   18   24    6   14    3  843    3    1    6   10    6    6   12]
 [  22    8   10   15   18    2    0    2   60   16   10    3    4    8  873    0    4    2   28    1   12]
 [   7    1    5    0    8    4    9    0    1    3    0   28   16    6    0  930   21   11    3    6    7]
 [   7   24    3    2   13   19    4    0    5    0    0    8    6    8    0   12  929    2    2   12   16]
 [   5    3    2    7    3    8    1    1    4    1    0   34   33    8    7   20    6  850    5    3    4]
 [   5   13   11   14    7    6    1   28    8    1   11    4    6    2   20    1    4    0  905    3    8]
 [   4    6    4    3    2   14   22    9    1    0    0   32    8    5    2    5    7    1    9  945    9]
 [ 420  417  363  164  513  379  208  232  237  157  219  289  510  475  256  284  552  146  347  470 7294]]

2024-06-06 00:36:53,223 - ==> Best [Top1: 72.621   Top5: 94.901   Sparsity:0.00   Params: 169472 on epoch: 48]
2024-06-06 00:36:53,223 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:36:53,232 - 

2024-06-06 00:36:53,232 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:36:59,816 - Epoch: [50][  100/ 1218]    Overall Loss 0.662017    Objective Loss 0.662017                                        LR 0.001000    Time 0.065808    
2024-06-06 00:37:05,009 - Epoch: [50][  200/ 1218]    Overall Loss 0.658634    Objective Loss 0.658634                                        LR 0.001000    Time 0.058856    
2024-06-06 00:37:10,391 - Epoch: [50][  300/ 1218]    Overall Loss 0.661468    Objective Loss 0.661468                                        LR 0.001000    Time 0.057168    
2024-06-06 00:37:15,233 - Epoch: [50][  400/ 1218]    Overall Loss 0.664695    Objective Loss 0.664695                                        LR 0.001000    Time 0.054975    
2024-06-06 00:37:20,329 - Epoch: [50][  500/ 1218]    Overall Loss 0.665818    Objective Loss 0.665818                                        LR 0.001000    Time 0.054167    
2024-06-06 00:37:25,308 - Epoch: [50][  600/ 1218]    Overall Loss 0.664121    Objective Loss 0.664121                                        LR 0.001000    Time 0.053435    
2024-06-06 00:37:30,538 - Epoch: [50][  700/ 1218]    Overall Loss 0.662505    Objective Loss 0.662505                                        LR 0.001000    Time 0.053269    
2024-06-06 00:37:35,627 - Epoch: [50][  800/ 1218]    Overall Loss 0.662921    Objective Loss 0.662921                                        LR 0.001000    Time 0.052969    
2024-06-06 00:37:40,580 - Epoch: [50][  900/ 1218]    Overall Loss 0.663008    Objective Loss 0.663008                                        LR 0.001000    Time 0.052585    
2024-06-06 00:37:45,661 - Epoch: [50][ 1000/ 1218]    Overall Loss 0.663269    Objective Loss 0.663269                                        LR 0.001000    Time 0.052405    
2024-06-06 00:37:50,911 - Epoch: [50][ 1100/ 1218]    Overall Loss 0.664826    Objective Loss 0.664826                                        LR 0.001000    Time 0.052406    
2024-06-06 00:37:55,977 - Epoch: [50][ 1200/ 1218]    Overall Loss 0.665773    Objective Loss 0.665773                                        LR 0.001000    Time 0.052258    
2024-06-06 00:37:56,774 - Epoch: [50][ 1218/ 1218]    Overall Loss 0.665143    Objective Loss 0.665143    Top1 69.193154    Top5 95.354523    LR 0.001000    Time 0.052140    
2024-06-06 00:37:56,969 - --- validate (epoch=50)-----------
2024-06-06 00:37:56,969 - 34633 samples (256 per mini-batch)
2024-06-06 00:38:03,335 - Epoch: [50][  100/  136]    Loss 0.617394    Top1 71.867188    Top5 94.785156    
2024-06-06 00:38:05,179 - Epoch: [50][  136/  136]    Loss 0.613388    Top1 71.888084    Top5 94.782433    
2024-06-06 00:38:05,391 - ==> Top1: 71.888    Top5: 94.782    Loss: 0.613

2024-06-06 00:38:05,392 - ==> Confusion:
[[ 792    0    5    2   13    4    0    4    6   68    1    2    0    6    6    4    3    0    3    1   11]
 [   4  865    2    2   28   39   11   38    5    5    4    5    1    2    3    2   16    2   13    6   10]
 [  15    5  775   16    4    6   44   25    1    4    6    7    4    7    5    2   12    1    6    8   17]
 [   7    2   32  827    8   12    6   12    4    5   17    2    9    5   28    1    1    6   18    5    9]
 [  38   18    1    1  897   27    0    5    5   15    1    4    0    3    7    4   10    1    3    2   12]
 [   5   20    2    3   22  815    8   55    6    7    3   23    6   34    1    2    9    0    3   13    6]
 [   5    8   30    3    3    8  975   14    1    0    5    2    0    1    0    7    3    0    1   11    9]
 [   6   12   12    4    5   47    5  892    3    1    3   12    3    3    3    2    2    4   35   17    6]
 [  20    9    2    0    4    3    1    7  787   77   11    6    6   16   22    1    4    2   12    3    9]
 [ 117    3    0    1   10    7    0    7   42  776    1    2    0   21    6    1    0    1    1    0    5]
 [   3    6   11   16    3   12    7   30   25    7  863    2    0   18   16    3    5    0   22    3   12]
 [   4    2    2    0    2   16    1   13    1    3    0  829   15   24    0   13   12   23    3   34   14]
 [   4    2    2    6    0   11    3    4    4    1    0  123  720   10    1   10    7   56    6   10   15]
 [  10    2    4    2    7   22    3    7   21   32    8   12    2  820    8    3    7    1    2   17   11]
 [  18    8    2   17   18    6    0    5   57   23    1    1    0    7  897    0    3    5   17    2   11]
 [   4    1    7    1    6    1   16    6    0    0    0   22    5   10    0  943   19    5    2    2   16]
 [   5   13    6    2   11   18    1    3    5    0    2   12    2    5    4    8  935    1    5   11   23]
 [   5    3    2    6    0    2    5    5    3    4    1   42   17    9    5   26    5  842    3    8   12]
 [   7    8   10   16    9    5    2   59    7    4    8    8    6    2   12    0    0    1  878    4   12]
 [   4    6    6    0    4   13   20   21    1    0    1   31    6    6    2    9   10    2    3  932   11]
 [ 383  329  262  139  400  384  217  382  195  210  156  307  362  438  299  190  540  120  282  500 7837]]

2024-06-06 00:38:05,394 - ==> Best [Top1: 72.621   Top5: 94.901   Sparsity:0.00   Params: 169472 on epoch: 48]
2024-06-06 00:38:05,394 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:38:05,406 - 

2024-06-06 00:38:05,407 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:38:12,318 - Epoch: [51][  100/ 1218]    Overall Loss 0.660650    Objective Loss 0.660650                                        LR 0.001000    Time 0.069087    
2024-06-06 00:38:17,611 - Epoch: [51][  200/ 1218]    Overall Loss 0.665741    Objective Loss 0.665741                                        LR 0.001000    Time 0.060995    
2024-06-06 00:38:22,579 - Epoch: [51][  300/ 1218]    Overall Loss 0.669511    Objective Loss 0.669511                                        LR 0.001000    Time 0.057215    
2024-06-06 00:38:27,487 - Epoch: [51][  400/ 1218]    Overall Loss 0.671065    Objective Loss 0.671065                                        LR 0.001000    Time 0.055176    
2024-06-06 00:38:32,565 - Epoch: [51][  500/ 1218]    Overall Loss 0.667527    Objective Loss 0.667527                                        LR 0.001000    Time 0.054292    
2024-06-06 00:38:37,742 - Epoch: [51][  600/ 1218]    Overall Loss 0.668109    Objective Loss 0.668109                                        LR 0.001000    Time 0.053867    
2024-06-06 00:38:43,274 - Epoch: [51][  700/ 1218]    Overall Loss 0.667252    Objective Loss 0.667252                                        LR 0.001000    Time 0.054072    
2024-06-06 00:38:48,521 - Epoch: [51][  800/ 1218]    Overall Loss 0.665394    Objective Loss 0.665394                                        LR 0.001000    Time 0.053868    
2024-06-06 00:38:53,540 - Epoch: [51][  900/ 1218]    Overall Loss 0.665605    Objective Loss 0.665605                                        LR 0.001000    Time 0.053457    
2024-06-06 00:38:58,736 - Epoch: [51][ 1000/ 1218]    Overall Loss 0.665842    Objective Loss 0.665842                                        LR 0.001000    Time 0.053305    
2024-06-06 00:39:03,703 - Epoch: [51][ 1100/ 1218]    Overall Loss 0.665858    Objective Loss 0.665858                                        LR 0.001000    Time 0.052972    
2024-06-06 00:39:08,740 - Epoch: [51][ 1200/ 1218]    Overall Loss 0.666295    Objective Loss 0.666295                                        LR 0.001000    Time 0.052753    
2024-06-06 00:39:09,634 - Epoch: [51][ 1218/ 1218]    Overall Loss 0.666507    Objective Loss 0.666507    Top1 72.860636    Top5 96.088020    LR 0.001000    Time 0.052708    
2024-06-06 00:39:09,870 - --- validate (epoch=51)-----------
2024-06-06 00:39:09,870 - 34633 samples (256 per mini-batch)
2024-06-06 00:39:16,016 - Epoch: [51][  100/  136]    Loss 0.625945    Top1 69.605469    Top5 93.695312    
2024-06-06 00:39:17,853 - Epoch: [51][  136/  136]    Loss 0.632417    Top1 69.583923    Top5 93.653452    
2024-06-06 00:39:18,041 - ==> Top1: 69.584    Top5: 93.653    Loss: 0.632

2024-06-06 00:39:18,043 - ==> Confusion:
[[ 792    2    5    0    8    1    1    2   11   82    0    8    2    1    7    1    0    1    1    0    6]
 [   6  867    4    1   27   41    9   21   15    2    7   10    4    1    9    1    7    1   18    4    8]
 [  24    2  764    8    7    2   49   25    3   10    9    7    3    6    6   12   11    2    7    8    5]
 [   5    4   20  811    2    6    5    5   11    6   22    3   15    2   47    4    2    8   26    2   10]
 [  38   14    2    0  909   10    0    3    7   23    2    8    1    4   10    8    3    2    1    1    8]
 [  13   36    6    4   31  749    1   49   12   12    2   48   13   25    4    2    6    4    5   12    9]
 [   1    6   21    0    4   10  957   16    1    3    2   12    3    0    2    9    3    2    5   14   15]
 [  10   21    8    2    3   37    6  874    4    5    4   25    7    3    0    0    1    3   36   17   11]
 [  24    3    0    1    0    2    0    2  822   88   13   11    6   12    9    0    1    1    4    0    3]
 [ 121    3    0    1    9    3    0    1   53  778    1    2    1   10    3    2    2    2    0    0    9]
 [   7    9   16    4    4    2    4   14   52   12  877    7    2   15   11    0    0    1   18    3    6]
 [   6    4    0    1    0   13    1    2    3    2    1  878   30    7    0   11    5   31    3    7    6]
 [   1    0    0    5    2    4    1    2   12    1    4  143  743    3    7   10    3   34    9    4    7]
 [   6    3    1    0    4   11    1    5   40   47    7   37    9  798    6    2    2   10    1    2    9]
 [  19    7    1   13   21    2    0    2   93   22    0    1    4    3  892    1    3    3    6    0    5]
 [   3    1    5    0    8    0    9    0    0    3    0   43   13    2    0  921   15   26    3    4   10]
 [   7    9    7    2   14    8    3    3   13    4    1   21    9    6    5   14  921    3    1    3   18]
 [   4    2    0    6    1    0    0    1    7    4    0   49   46    2    5   10    2  854    1    3    8]
 [   8   16    4   18    6    7    0   48   37    1    6    4    6    1   21    1    2    2  860    6    4]
 [   3    7    1    0    6   13   11   17    4    0    0   97   24    5    0   15    4    4    1  869    7]
 [ 605  380  300  153  437  254  167  294  392  323  209  548  501  404  387  225  390  180  288  332 7163]]

2024-06-06 00:39:18,045 - ==> Best [Top1: 72.621   Top5: 94.901   Sparsity:0.00   Params: 169472 on epoch: 48]
2024-06-06 00:39:18,045 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:39:18,056 - 

2024-06-06 00:39:18,057 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:39:24,653 - Epoch: [52][  100/ 1218]    Overall Loss 0.695598    Objective Loss 0.695598                                        LR 0.001000    Time 0.065929    
2024-06-06 00:39:30,107 - Epoch: [52][  200/ 1218]    Overall Loss 0.678184    Objective Loss 0.678184                                        LR 0.001000    Time 0.060224    
2024-06-06 00:39:35,520 - Epoch: [52][  300/ 1218]    Overall Loss 0.674563    Objective Loss 0.674563                                        LR 0.001000    Time 0.058185    
2024-06-06 00:39:40,609 - Epoch: [52][  400/ 1218]    Overall Loss 0.672336    Objective Loss 0.672336                                        LR 0.001000    Time 0.056355    
2024-06-06 00:39:45,515 - Epoch: [52][  500/ 1218]    Overall Loss 0.667840    Objective Loss 0.667840                                        LR 0.001000    Time 0.054891    
2024-06-06 00:39:50,876 - Epoch: [52][  600/ 1218]    Overall Loss 0.666024    Objective Loss 0.666024                                        LR 0.001000    Time 0.054673    
2024-06-06 00:39:56,089 - Epoch: [52][  700/ 1218]    Overall Loss 0.665391    Objective Loss 0.665391                                        LR 0.001000    Time 0.054307    
2024-06-06 00:40:01,276 - Epoch: [52][  800/ 1218]    Overall Loss 0.664101    Objective Loss 0.664101                                        LR 0.001000    Time 0.053999    
2024-06-06 00:40:06,223 - Epoch: [52][  900/ 1218]    Overall Loss 0.664389    Objective Loss 0.664389                                        LR 0.001000    Time 0.053494    
2024-06-06 00:40:11,114 - Epoch: [52][ 1000/ 1218]    Overall Loss 0.664716    Objective Loss 0.664716                                        LR 0.001000    Time 0.053032    
2024-06-06 00:40:16,389 - Epoch: [52][ 1100/ 1218]    Overall Loss 0.665154    Objective Loss 0.665154                                        LR 0.001000    Time 0.053004    
2024-06-06 00:40:21,358 - Epoch: [52][ 1200/ 1218]    Overall Loss 0.664962    Objective Loss 0.664962                                        LR 0.001000    Time 0.052727    
2024-06-06 00:40:22,281 - Epoch: [52][ 1218/ 1218]    Overall Loss 0.664890    Objective Loss 0.664890    Top1 67.237164    Top5 95.110024    LR 0.001000    Time 0.052705    
2024-06-06 00:40:22,500 - --- validate (epoch=52)-----------
2024-06-06 00:40:22,501 - 34633 samples (256 per mini-batch)
2024-06-06 00:40:28,726 - Epoch: [52][  100/  136]    Loss 0.621487    Top1 70.453125    Top5 93.945312    
2024-06-06 00:40:30,674 - Epoch: [52][  136/  136]    Loss 0.619113    Top1 70.571420    Top5 93.988393    
2024-06-06 00:40:30,869 - ==> Top1: 70.571    Top5: 93.988    Loss: 0.619

2024-06-06 00:40:30,870 - ==> Confusion:
[[ 745    0    6    0    6    7    0    6   14  115    0    0    2    2    7    2    1    3    6    2    7]
 [   8  875    4    4   25   36    3   27    8    3    2   10    2    2   11    1    6    4   17    6    9]
 [  13    3  791    9   12    5   22   21    1    9    9    4    5    9    8   10    6    3   14   11    5]
 [   3    0   31  823    6    4    3    2    2    6   25    3    5    4   39    5    0    9   33    4    9]
 [  45   15    3    1  866   20    1    5    7   37    1    2    0    6   10    3    7    1    7    4   13]
 [   6   28    3    8   15  780    1   61    8   11    2   21   12   40    9    1    6    7    7    8    9]
 [   2    7   55    2    3    9  914   20    2    2    6    8    4    1    0   12    3    3    4   16   13]
 [   4   16   16    4    3   40    4  893    5    2    5   15    5    7    5    1    2    2   28   12    8]
 [  19    1    2    2    0    0    1    2  824   60    7    0    4   25   29    0    1    6   11    3    5]
 [  69    1    3    0    4    0    0    2   73  794    0    2    0   23   11    1    1    7    2    1    7]
 [   4    7   16   23    2    8    1   12   29    6  882    2    3   18   12    0    3    1   25    2    8]
 [   2    1    1    0    0   16    4   11    1    3    1  799   57    8    1   16    3   46    7   24   10]
 [   2    1    1    7    0    4    3    1    3    0    3   81  746    9    7    6    2   83   16    9   11]
 [   1    0    4    2    1   12    1    7   24   21    4   17    5  856    9    3    4   10    6    7    7]
 [  20    6    5   15   14    0    0    4   42   20   10    5    2   13  912    0    2    3   17    0    8]
 [   0    2    8    3    6    1    9    5    1    2    0   19    8    1    2  941    8   26    5    5   14]
 [   2    9   11    5   11   15    0    4    9    2    5    8   14    9   10   10  918    2    5    9   14]
 [   1    1    1    3    0    0    0    4    0    6    0   12   21    4    6   19    1  910    5    4    7]
 [   3   12   14   22    3    4    0   31   24    1    7    5    7    3   36    0    2    0  866   10    8]
 [   1    7    5    0    6   12   10   28    0    0    0   48   11    5    0    5    9    5   10  913   13]
 [ 357  323  350  196  335  273  129  384  243  272  217  304  503  490  416  221  375  296  369  486 7393]]

2024-06-06 00:40:30,872 - ==> Best [Top1: 72.621   Top5: 94.901   Sparsity:0.00   Params: 169472 on epoch: 48]
2024-06-06 00:40:30,872 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:40:30,881 - 

2024-06-06 00:40:30,881 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:40:37,810 - Epoch: [53][  100/ 1218]    Overall Loss 0.660858    Objective Loss 0.660858                                        LR 0.001000    Time 0.069263    
2024-06-06 00:40:42,869 - Epoch: [53][  200/ 1218]    Overall Loss 0.666492    Objective Loss 0.666492                                        LR 0.001000    Time 0.059911    
2024-06-06 00:40:48,069 - Epoch: [53][  300/ 1218]    Overall Loss 0.658894    Objective Loss 0.658894                                        LR 0.001000    Time 0.057267    
2024-06-06 00:40:53,066 - Epoch: [53][  400/ 1218]    Overall Loss 0.659030    Objective Loss 0.659030                                        LR 0.001000    Time 0.055436    
2024-06-06 00:40:58,048 - Epoch: [53][  500/ 1218]    Overall Loss 0.662206    Objective Loss 0.662206                                        LR 0.001000    Time 0.054310    
2024-06-06 00:41:03,157 - Epoch: [53][  600/ 1218]    Overall Loss 0.659627    Objective Loss 0.659627                                        LR 0.001000    Time 0.053768    
2024-06-06 00:41:08,694 - Epoch: [53][  700/ 1218]    Overall Loss 0.659996    Objective Loss 0.659996                                        LR 0.001000    Time 0.053995    
2024-06-06 00:41:13,668 - Epoch: [53][  800/ 1218]    Overall Loss 0.661440    Objective Loss 0.661440                                        LR 0.001000    Time 0.053460    
2024-06-06 00:41:18,608 - Epoch: [53][  900/ 1218]    Overall Loss 0.661959    Objective Loss 0.661959                                        LR 0.001000    Time 0.053006    
2024-06-06 00:41:23,592 - Epoch: [53][ 1000/ 1218]    Overall Loss 0.663143    Objective Loss 0.663143                                        LR 0.001000    Time 0.052688    
2024-06-06 00:41:28,933 - Epoch: [53][ 1100/ 1218]    Overall Loss 0.663852    Objective Loss 0.663852                                        LR 0.001000    Time 0.052751    
2024-06-06 00:41:33,760 - Epoch: [53][ 1200/ 1218]    Overall Loss 0.663793    Objective Loss 0.663793                                        LR 0.001000    Time 0.052376    
2024-06-06 00:41:34,599 - Epoch: [53][ 1218/ 1218]    Overall Loss 0.663215    Objective Loss 0.663215    Top1 70.171149    Top5 94.865526    LR 0.001000    Time 0.052290    
2024-06-06 00:41:34,827 - --- validate (epoch=53)-----------
2024-06-06 00:41:34,827 - 34633 samples (256 per mini-batch)
2024-06-06 00:41:41,102 - Epoch: [53][  100/  136]    Loss 0.591679    Top1 72.656250    Top5 94.984375    
2024-06-06 00:41:43,060 - Epoch: [53][  136/  136]    Loss 0.597876    Top1 72.624376    Top5 94.949903    
2024-06-06 00:41:43,264 - ==> Top1: 72.624    Top5: 94.950    Loss: 0.598

2024-06-06 00:41:43,266 - ==> Confusion:
[[ 765    1    8    1    7    7    4    3   10   84    0    5    0    2    5    4    1    1    4    3   16]
 [   3  886    0    0   15   44    9   34    4    2    5   15    4    2    7    1    2    1   12    7   10]
 [  11    4  782    7    0    7   64   16    1    7    8    9    2    3    6    7    7    1    6   12   10]
 [   5    6   36  816    4   15    5    9    3    3   24    3   13    1   24    3    5    7   15    7   12]
 [  39   30    6    1  857   19    2    7    2   19    3    9    1    4   11   10    8    3    5    2   16]
 [   8   28    1    1   14  833    4   53    4    4    4   22   12   19    3    1    4    1    5   11   11]
 [   0    4   22    1    1   10  975   13    0    0    4   12    2    2    0    9    1    3    4   14    9]
 [   2   22   12    1    1   57   11  879    0    1    4   21    7    2    0    0    2    1   25   22    7]
 [  20   13    2    1    2    2    1    3  795   69   10    5    7   28   21    0    2    0   10    1   10]
 [  98    1    0    1    5   13    1    6   54  763    0    3    2   23   12    3    3    4    1    1    7]
 [   3    6    8   12    3    8   13   12   26    3  885    1    4   20   16    0    0    1   21    4   18]
 [   4    0    3    0    0   13    5    6    1    0    3  872   23    7    1   13    1   21    5   23   10]
 [   1    2    2    6    0    7    6    5    2    0    0  127  747    5    5    6    3   46    4    7   14]
 [   5    3    3    1    7   40    4    8   14   27    5   26    3  813    8    1    2    6    2   11   12]
 [  23    7    6   13    9    5    0    6   41   13    7    4    3    7  914    0    1    7   15    2   15]
 [   3    5    3    1    5    0   15    1    1    3    0   33   10    3    0  945    6   17    3    5    7]
 [   4   19    8    2   12   19    4    1   10    2    1   21    5    4    1   20  890    3    2   14   30]
 [   2    0    3    2    1    3    3    5    0    3    0   31   40    4    2   17    0  870    1    6   12]
 [   4   11   10   22    2    5    4   60    7    1    6    1    7    3   19    1    0    2  874    6   13]
 [   4    3    1    1    0   15   28   17    1    0    1   53   12    6    0    8    5    4    3  916   10]
 [ 344  370  291  158  220  381  251  318  181  175  213  407  434  400  295  218  291  166  230  514 8075]]

2024-06-06 00:41:43,268 - ==> Best [Top1: 72.624   Top5: 94.950   Sparsity:0.00   Params: 169472 on epoch: 53]
2024-06-06 00:41:43,268 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:41:43,288 - 

2024-06-06 00:41:43,288 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:41:49,962 - Epoch: [54][  100/ 1218]    Overall Loss 0.655305    Objective Loss 0.655305                                        LR 0.001000    Time 0.066706    
2024-06-06 00:41:54,936 - Epoch: [54][  200/ 1218]    Overall Loss 0.660983    Objective Loss 0.660983                                        LR 0.001000    Time 0.058212    
2024-06-06 00:42:00,216 - Epoch: [54][  300/ 1218]    Overall Loss 0.662267    Objective Loss 0.662267                                        LR 0.001000    Time 0.056399    
2024-06-06 00:42:05,389 - Epoch: [54][  400/ 1218]    Overall Loss 0.660852    Objective Loss 0.660852                                        LR 0.001000    Time 0.055225    
2024-06-06 00:42:10,886 - Epoch: [54][  500/ 1218]    Overall Loss 0.660670    Objective Loss 0.660670                                        LR 0.001000    Time 0.055169    
2024-06-06 00:42:16,009 - Epoch: [54][  600/ 1218]    Overall Loss 0.658448    Objective Loss 0.658448                                        LR 0.001000    Time 0.054510    
2024-06-06 00:42:21,264 - Epoch: [54][  700/ 1218]    Overall Loss 0.659401    Objective Loss 0.659401                                        LR 0.001000    Time 0.054227    
2024-06-06 00:42:26,146 - Epoch: [54][  800/ 1218]    Overall Loss 0.660068    Objective Loss 0.660068                                        LR 0.001000    Time 0.053548    
2024-06-06 00:42:31,105 - Epoch: [54][  900/ 1218]    Overall Loss 0.661531    Objective Loss 0.661531                                        LR 0.001000    Time 0.053105    
2024-06-06 00:42:36,240 - Epoch: [54][ 1000/ 1218]    Overall Loss 0.660991    Objective Loss 0.660991                                        LR 0.001000    Time 0.052927    
2024-06-06 00:42:41,252 - Epoch: [54][ 1100/ 1218]    Overall Loss 0.661928    Objective Loss 0.661928                                        LR 0.001000    Time 0.052670    
2024-06-06 00:42:46,513 - Epoch: [54][ 1200/ 1218]    Overall Loss 0.660905    Objective Loss 0.660905                                        LR 0.001000    Time 0.052664    
2024-06-06 00:42:47,574 - Epoch: [54][ 1218/ 1218]    Overall Loss 0.661338    Objective Loss 0.661338    Top1 71.149144    Top5 94.865526    LR 0.001000    Time 0.052755    
2024-06-06 00:42:47,754 - --- validate (epoch=54)-----------
2024-06-06 00:42:47,754 - 34633 samples (256 per mini-batch)
2024-06-06 00:42:53,830 - Epoch: [54][  100/  136]    Loss 0.603523    Top1 70.964844    Top5 94.527344    
2024-06-06 00:42:55,659 - Epoch: [54][  136/  136]    Loss 0.606547    Top1 70.854387    Top5 94.430168    
2024-06-06 00:42:55,868 - ==> Top1: 70.854    Top5: 94.430    Loss: 0.607

2024-06-06 00:42:55,870 - ==> Confusion:
[[ 778    3    7    0   16    0    1    4    7   80    0    3    0    2   15    2    3    2    4    0    4]
 [   6  886    6    5   35   22    6   15    6    4    7    3    3    2   13    3   13    3   13    3    9]
 [  16    5  809   16   11    0   24   14    0   10    6    6    3    6    6    1   11    4    7    6    9]
 [   2    2   30  820    6    8    4    5    3    4   25    2    4    1   53    4    4    6   22    1   10]
 [  36    9    3    3  929    5    1    2    1   15    2    1    1    2   15    6    9    3    4    2    5]
 [  11   75    2    7   25  744    6   53    4   13    1   23    8   11    5    3   10    7    9   10   16]
 [   1   11   41    3    3    6  940   12    1    2    5   12    5    1    0    7    2    7    6   18    3]
 [   6   29   16    3    6   43    6  869    2    4    2   12    6    3    2    2    1    4   42   13    6]
 [  24    7    1    4    4    1    2    3  795   60   12    1    4    8   47    1    0   10   10    0    8]
 [ 109    0    2    2   10    1    0    2   54  779    2    0    0    6   16    0    3    7    2    2    4]
 [   2   13   13   12    6    4    3   11   16    3  903    2    1   11   28    0    3    2   18    2   11]
 [   7    2    3    0    2   16    5   13    5    1    0  810   45    3    2   11    9   41    3   24    9]
 [   5    5    4   15    1    5    2    6    4    0    1   78  773    2    2    7   10   52    7    9    7]
 [  10    3    6    0    8   21    0    8   38   31   16   17    6  758   22    6    8   18    1   12   12]
 [  14    5    3   11    9    2    0    1   24   12    2    3    2    4  976    0    5    5   12    1    7]
 [   8    7    8    1    9    1   10    2    0    0    0   28    9    0    2  915   13   29    3    7   14]
 [  13   20   11    1   19    9    0    1    7    1    2    5    5    4    6   17  922    4    3    8   14]
 [   2    1    1    4    0    2    2    3    4    3    0   20   34    1   10    8    5  893    3    0    9]
 [   7   29   11   23    4    4    1   28   11    2    7    7    3    0   36    2    4    2  863    5    9]
 [   2    8    6    1    1   11    9   22    0    1    3   32   16    5    1   10   15    5    5  924   11]
 [ 424  422  382  209  475  250  125  288  232  175  232  272  481  272  580  204  467  262  305  422 7453]]

2024-06-06 00:42:55,872 - ==> Best [Top1: 72.624   Top5: 94.950   Sparsity:0.00   Params: 169472 on epoch: 53]
2024-06-06 00:42:55,872 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:42:55,883 - 

2024-06-06 00:42:55,883 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:43:02,538 - Epoch: [55][  100/ 1218]    Overall Loss 0.660694    Objective Loss 0.660694                                        LR 0.001000    Time 0.066521    
2024-06-06 00:43:07,644 - Epoch: [55][  200/ 1218]    Overall Loss 0.656751    Objective Loss 0.656751                                        LR 0.001000    Time 0.058781    
2024-06-06 00:43:12,821 - Epoch: [55][  300/ 1218]    Overall Loss 0.662040    Objective Loss 0.662040                                        LR 0.001000    Time 0.056435    
2024-06-06 00:43:17,906 - Epoch: [55][  400/ 1218]    Overall Loss 0.661758    Objective Loss 0.661758                                        LR 0.001000    Time 0.055033    
2024-06-06 00:43:23,075 - Epoch: [55][  500/ 1218]    Overall Loss 0.662175    Objective Loss 0.662175                                        LR 0.001000    Time 0.054361    
2024-06-06 00:43:28,039 - Epoch: [55][  600/ 1218]    Overall Loss 0.661471    Objective Loss 0.661471                                        LR 0.001000    Time 0.053570    
2024-06-06 00:43:33,348 - Epoch: [55][  700/ 1218]    Overall Loss 0.662743    Objective Loss 0.662743                                        LR 0.001000    Time 0.053497    
2024-06-06 00:43:38,462 - Epoch: [55][  800/ 1218]    Overall Loss 0.661347    Objective Loss 0.661347                                        LR 0.001000    Time 0.053200    
2024-06-06 00:43:43,532 - Epoch: [55][  900/ 1218]    Overall Loss 0.659606    Objective Loss 0.659606                                        LR 0.001000    Time 0.052921    
2024-06-06 00:43:48,660 - Epoch: [55][ 1000/ 1218]    Overall Loss 0.661428    Objective Loss 0.661428                                        LR 0.001000    Time 0.052754    
2024-06-06 00:43:54,392 - Epoch: [55][ 1100/ 1218]    Overall Loss 0.662091    Objective Loss 0.662091                                        LR 0.001000    Time 0.053167    
2024-06-06 00:43:59,625 - Epoch: [55][ 1200/ 1218]    Overall Loss 0.661426    Objective Loss 0.661426                                        LR 0.001000    Time 0.053095    
2024-06-06 00:44:00,637 - Epoch: [55][ 1218/ 1218]    Overall Loss 0.661634    Objective Loss 0.661634    Top1 75.061125    Top5 93.887531    LR 0.001000    Time 0.053141    
2024-06-06 00:44:00,834 - --- validate (epoch=55)-----------
2024-06-06 00:44:00,834 - 34633 samples (256 per mini-batch)
2024-06-06 00:44:07,368 - Epoch: [55][  100/  136]    Loss 0.601783    Top1 70.210938    Top5 94.093750    
2024-06-06 00:44:09,139 - Epoch: [55][  136/  136]    Loss 0.603479    Top1 70.337539    Top5 94.083677    
2024-06-06 00:44:09,338 - ==> Top1: 70.338    Top5: 94.084    Loss: 0.603

2024-06-06 00:44:09,340 - ==> Confusion:
[[ 735    4    3    0   17    1    0    5   11  111    1    8    1    7    7    6    4    4    2    1    3]
 [   2  873    2    1   24   43    4   29   10    1    4   13    4    2    5    5    9    5   11   11    5]
 [  10    4  773    8    8    6   57   23    1    8    9    7    5    5    1   14    7    1    4   12    7]
 [   3    5   27  807    2    7    4    8    7    4   30    6    7    6   28    4    4   13   21    9   14]
 [  27   16    1    2  900   21    1    4    4   25    1    3    1    7    7   12   13    3    1    3    2]
 [   1   30    1    6   12  827    5   44    3    7    2   23    8   26    1    4    7    3    2   20   11]
 [   2    7   26    4    3    7  959   11    0    2    4   10    1    2    0   12    5    1    3   19    8]
 [   2   15   11    1    7   48    7  876    6    1    4   24    5    4    2    6    2    3   21   28    4]
 [   9    9    0    2    1    3    0    1  823   62   17    8    6   24   17    1    3    2    8    1    5]
 [  73    2    1    1   11    7    1    4   65  785    0    4    0   22    5    6    2    4    1    3    4]
 [   3    9   10   10    6    9   13   10   28    5  892    1    5   27    7    0    2    2   16    3    6]
 [   0    0    0    0    3    9    4    7    2    1    1  855   33   11    0   17    6   25    0   32    5]
 [   2    0    1    2    0    7    3    6    4    1    0  141  743    7    4   13    2   29    4   15   11]
 [   3    2    2    0    7   29    1    6   11   20    3   22    2  872    1    3    0    4    0    9    4]
 [   8    7    8   22   12    7    0    3   45   21    7    5    5   18  884    1    2    5   22    1   15]
 [   2    2    0    0    1    2    6    4    1    0    0   41   14    4    0  951   13   16    2    1    6]
 [   7   18    3    1   12   13    2    1    8    2    1   14   10    4    2   20  921    4    0   15   14]
 [   0    4    0    2    2    4    0    4    2    2    0   53   48    7    2   18    0  844    0    2   11]
 [   4   10   11   18    9    6    2   50   14    0    6    4    8    3   20    2    2    4  872    6    7]
 [   1    9    0    0    2   16   17    7    2    0    3   34    9    9    0    9    6    2    0  956    6]
 [ 273  338  242  158  401  360  186  357  224  205  215  423  523  536  302  420  471  210  251  625 7212]]

2024-06-06 00:44:09,341 - ==> Best [Top1: 72.624   Top5: 94.950   Sparsity:0.00   Params: 169472 on epoch: 53]
2024-06-06 00:44:09,341 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:44:09,350 - 

2024-06-06 00:44:09,350 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:44:16,170 - Epoch: [56][  100/ 1218]    Overall Loss 0.647345    Objective Loss 0.647345                                        LR 0.001000    Time 0.068168    
2024-06-06 00:44:21,094 - Epoch: [56][  200/ 1218]    Overall Loss 0.661568    Objective Loss 0.661568                                        LR 0.001000    Time 0.058696    
2024-06-06 00:44:26,163 - Epoch: [56][  300/ 1218]    Overall Loss 0.659794    Objective Loss 0.659794                                        LR 0.001000    Time 0.056019    
2024-06-06 00:44:31,671 - Epoch: [56][  400/ 1218]    Overall Loss 0.662771    Objective Loss 0.662771                                        LR 0.001000    Time 0.055777    
2024-06-06 00:44:36,826 - Epoch: [56][  500/ 1218]    Overall Loss 0.664735    Objective Loss 0.664735                                        LR 0.001000    Time 0.054928    
2024-06-06 00:44:42,190 - Epoch: [56][  600/ 1218]    Overall Loss 0.664507    Objective Loss 0.664507                                        LR 0.001000    Time 0.054709    
2024-06-06 00:44:47,505 - Epoch: [56][  700/ 1218]    Overall Loss 0.665088    Objective Loss 0.665088                                        LR 0.001000    Time 0.054484    
2024-06-06 00:44:52,493 - Epoch: [56][  800/ 1218]    Overall Loss 0.665364    Objective Loss 0.665364                                        LR 0.001000    Time 0.053905    
2024-06-06 00:44:57,704 - Epoch: [56][  900/ 1218]    Overall Loss 0.663481    Objective Loss 0.663481                                        LR 0.001000    Time 0.053702    
2024-06-06 00:45:02,844 - Epoch: [56][ 1000/ 1218]    Overall Loss 0.662722    Objective Loss 0.662722                                        LR 0.001000    Time 0.053469    
2024-06-06 00:45:07,930 - Epoch: [56][ 1100/ 1218]    Overall Loss 0.661793    Objective Loss 0.661793                                        LR 0.001000    Time 0.053230    
2024-06-06 00:45:13,066 - Epoch: [56][ 1200/ 1218]    Overall Loss 0.661781    Objective Loss 0.661781                                        LR 0.001000    Time 0.053073    
2024-06-06 00:45:13,982 - Epoch: [56][ 1218/ 1218]    Overall Loss 0.662387    Objective Loss 0.662387    Top1 68.215159    Top5 94.376528    LR 0.001000    Time 0.053040    
2024-06-06 00:45:14,215 - --- validate (epoch=56)-----------
2024-06-06 00:45:14,215 - 34633 samples (256 per mini-batch)
2024-06-06 00:45:20,260 - Epoch: [56][  100/  136]    Loss 0.601278    Top1 72.074219    Top5 94.746094    
2024-06-06 00:45:21,857 - Epoch: [56][  136/  136]    Loss 0.607588    Top1 71.867872    Top5 94.554327    
2024-06-06 00:45:22,057 - ==> Top1: 71.868    Top5: 94.554    Loss: 0.608

2024-06-06 00:45:22,059 - ==> Confusion:
[[ 741    3   13    1   20    3    3    1    7   88    2    1    1    6   15    3    4    1    6    2   10]
 [   3  901    3    1   34   18    3   17   11    2    3    6    1    2    9    2    8    1   15   10   13]
 [   6    3  803   12    7    1   48   13    1    9    9    7    4    2    2   10    6    3    5   11    8]
 [   4    4   28  845    5    9    1    7    3    1   25    2    9    5   32    3    3    6   15    0    9]
 [  35   12    9    1  911    6    1    1    2   21    0    2    0    9   15    8    7    0    3    1   10]
 [   6   62   12    9   29  747    8   40    2    9    4   11    7   29    6    7    5    1   12   20   17]
 [   1    8   29    0    4    4  977    9    1    2    6    2    2    2    0    8    1    1    5   18    6]
 [   5   20   25    8    6   33    7  844    3    4   13    6    9    4    4    2    1    1   42   32    8]
 [  16    7    1    1    3    3    2    2  761   70   27    3    4   27   43    2    1    3   11    2   13]
 [  63    1    6    2   12    4    4    1   55  781    5    0    2   24   21    3    1    3    2    1   10]
 [   2    3   14   13    3    4    8    8   10    3  929    1    2   19   16    1    1    0   19    2    6]
 [   5    2    3    0    2   25    8    9    2    1    1  783   21   20    1   35    7   22    4   48   12]
 [   1    1    0   12    1    5    2    6    1    0    3  103  739   14    4   21    6   46    5   16    9]
 [   5    0    3    0    6   12    0    5   13   14   12   12    5  863   11    4    6    5    0   19    6]
 [  11    4    4   24   10    1    1    3   29   10    5    0    1    9  947    1    2    1   14    4   17]
 [   4    2    3    2    7    0   15    1    1    2    0   14    5    3    0  969   13   10    3    8    4]
 [  10   19   15    4   10    9    4    3    6    7    2    7    3   10    1   22  907    0    5   13   15]
 [   5    4    0   13    3    4    5    1    0    3    0   30   29    6   12   39    6  821    3   12    9]
 [   2   15    7   31    1    1    5   32    8    2   22    1    1    2   32    2    0    0  878    8    8]
 [   3   10    4    2    2   10   21   13    1    1    1   15    5    1    1    6    5    1    3  967   16]
 [ 294  296  453  190  432  187  245  255  145  207  271  214  375  515  439  311  368  123  242  594 7776]]

2024-06-06 00:45:22,061 - ==> Best [Top1: 72.624   Top5: 94.950   Sparsity:0.00   Params: 169472 on epoch: 53]
2024-06-06 00:45:22,062 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:45:22,073 - 

2024-06-06 00:45:22,073 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:45:28,954 - Epoch: [57][  100/ 1218]    Overall Loss 0.638782    Objective Loss 0.638782                                        LR 0.001000    Time 0.068778    
2024-06-06 00:45:34,115 - Epoch: [57][  200/ 1218]    Overall Loss 0.647208    Objective Loss 0.647208                                        LR 0.001000    Time 0.060184    
2024-06-06 00:45:39,363 - Epoch: [57][  300/ 1218]    Overall Loss 0.651821    Objective Loss 0.651821                                        LR 0.001000    Time 0.057599    
2024-06-06 00:45:44,527 - Epoch: [57][  400/ 1218]    Overall Loss 0.653401    Objective Loss 0.653401                                        LR 0.001000    Time 0.056103    
2024-06-06 00:45:49,393 - Epoch: [57][  500/ 1218]    Overall Loss 0.654988    Objective Loss 0.654988                                        LR 0.001000    Time 0.054611    
2024-06-06 00:45:54,699 - Epoch: [57][  600/ 1218]    Overall Loss 0.653323    Objective Loss 0.653323                                        LR 0.001000    Time 0.054348    
2024-06-06 00:46:00,056 - Epoch: [57][  700/ 1218]    Overall Loss 0.652716    Objective Loss 0.652716                                        LR 0.001000    Time 0.054234    
2024-06-06 00:46:05,160 - Epoch: [57][  800/ 1218]    Overall Loss 0.653553    Objective Loss 0.653553                                        LR 0.001000    Time 0.053832    
2024-06-06 00:46:10,139 - Epoch: [57][  900/ 1218]    Overall Loss 0.653146    Objective Loss 0.653146                                        LR 0.001000    Time 0.053380    
2024-06-06 00:46:15,306 - Epoch: [57][ 1000/ 1218]    Overall Loss 0.652888    Objective Loss 0.652888                                        LR 0.001000    Time 0.053207    
2024-06-06 00:46:20,347 - Epoch: [57][ 1100/ 1218]    Overall Loss 0.652905    Objective Loss 0.652905                                        LR 0.001000    Time 0.052950    
2024-06-06 00:46:25,553 - Epoch: [57][ 1200/ 1218]    Overall Loss 0.655070    Objective Loss 0.655070                                        LR 0.001000    Time 0.052874    
2024-06-06 00:46:26,345 - Epoch: [57][ 1218/ 1218]    Overall Loss 0.655248    Objective Loss 0.655248    Top1 73.594132    Top5 94.132029    LR 0.001000    Time 0.052743    
2024-06-06 00:46:26,602 - --- validate (epoch=57)-----------
2024-06-06 00:46:26,603 - 34633 samples (256 per mini-batch)
2024-06-06 00:46:33,013 - Epoch: [57][  100/  136]    Loss 0.597222    Top1 72.621094    Top5 95.402344    
2024-06-06 00:46:34,924 - Epoch: [57][  136/  136]    Loss 0.602587    Top1 72.433806    Top5 95.336817    
2024-06-06 00:46:35,147 - ==> Top1: 72.434    Top5: 95.337    Loss: 0.603

2024-06-06 00:46:35,148 - ==> Confusion:
[[ 829    2    6    0    6    2    1    3    6   45    0    3    0    2    2    3    3    0    4    2   12]
 [   7  832    9    2   32   56    7   31    4    4    7    5    2    2    8    3    9    4   15   12   12]
 [  17    1  804   30    0    1   36   12    0    4    7    4    5    6    3    6    2    2   10   10   10]
 [  12    1   19  872    2    7    5    2    1    3   18    5    5    3   32    2    0    4    9    5    9]
 [  58    9    8    3  880   12    0    2    4   14    2    6    1    3   18    7    9    2    6    0   10]
 [   7   15    1   10   16  811   10   56    5    5    2   26   10   21    3    4    7    1    3   14   16]
 [   8    4   38    1    1    4  964    8    1    0    7    5    1    1    0   12    4    1    2   14   10]
 [   9    8   24   11    3   34    9  877    4    1    3   15   11    2    2    0    1    1   27   25   10]
 [  35    4    0    6    0    5    1    5  745   81   21    5    8   30   25    1    4    2   11    0   13]
 [ 175    1    0    1    5    6    0    9   36  711    2    3    0   23   12    1    2    2    6    0    6]
 [   3    2    9   20    4    5    7    3   15    1  925    2    4   16   15    0    1    1   12    3   16]
 [   2    1    2    1    0   16    5    5    0    0    0  826   31   15    4   14    5   23    4   39   18]
 [   1    1    3   11    0    1    3    7    3    0    0  109  762    5    5   10    7   31    6   13   17]
 [   5    3    3    2    7   19    2    2   15   28    9   25    4  826    9    3    3    3    1   14   18]
 [  13    5    4   32   11    4    0    2   38   16    8    3    3    7  913    2    1    5   17    2   12]
 [   7    1    6    3    5    0   10    2    2    1    1   20   11    2    3  954    7   11    3    4   13]
 [   8    9   11    6    9    5    1    2    7    1    2   14    7    5    4   30  916    3    0   10   22]
 [   7    0    1    2    2    1    0    0    0    1    1   39   50    4    6   23    1  851    3    3   10]
 [   5    9   13   44    4    3    2   32    5    2    4    7    4    1   42    0    0    1  865    6    9]
 [   5    4    3    1    1   10   25   10    0    1    1   30    9    4    3    8    2    3    2  943   23]
 [ 426  209  394  347  291  270  166  263  126  178  224  352  472  349  407  294  305  135  252  492 7980]]

2024-06-06 00:46:35,151 - ==> Best [Top1: 72.624   Top5: 94.950   Sparsity:0.00   Params: 169472 on epoch: 53]
2024-06-06 00:46:35,151 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:46:35,162 - 

2024-06-06 00:46:35,163 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:46:41,781 - Epoch: [58][  100/ 1218]    Overall Loss 0.661123    Objective Loss 0.661123                                        LR 0.001000    Time 0.066160    
2024-06-06 00:46:46,856 - Epoch: [58][  200/ 1218]    Overall Loss 0.658015    Objective Loss 0.658015                                        LR 0.001000    Time 0.058440    
2024-06-06 00:46:51,798 - Epoch: [58][  300/ 1218]    Overall Loss 0.655686    Objective Loss 0.655686                                        LR 0.001000    Time 0.055425    
2024-06-06 00:46:57,118 - Epoch: [58][  400/ 1218]    Overall Loss 0.659540    Objective Loss 0.659540                                        LR 0.001000    Time 0.054863    
2024-06-06 00:47:02,031 - Epoch: [58][  500/ 1218]    Overall Loss 0.659299    Objective Loss 0.659299                                        LR 0.001000    Time 0.053712    
2024-06-06 00:47:06,996 - Epoch: [58][  600/ 1218]    Overall Loss 0.660214    Objective Loss 0.660214                                        LR 0.001000    Time 0.053031    
2024-06-06 00:47:12,472 - Epoch: [58][  700/ 1218]    Overall Loss 0.660706    Objective Loss 0.660706                                        LR 0.001000    Time 0.053274    
2024-06-06 00:47:17,470 - Epoch: [58][  800/ 1218]    Overall Loss 0.659357    Objective Loss 0.659357                                        LR 0.001000    Time 0.052860    
2024-06-06 00:47:22,397 - Epoch: [58][  900/ 1218]    Overall Loss 0.659816    Objective Loss 0.659816                                        LR 0.001000    Time 0.052458    
2024-06-06 00:47:27,537 - Epoch: [58][ 1000/ 1218]    Overall Loss 0.660591    Objective Loss 0.660591                                        LR 0.001000    Time 0.052351    
2024-06-06 00:47:32,803 - Epoch: [58][ 1100/ 1218]    Overall Loss 0.660512    Objective Loss 0.660512                                        LR 0.001000    Time 0.052377    
2024-06-06 00:47:38,206 - Epoch: [58][ 1200/ 1218]    Overall Loss 0.660180    Objective Loss 0.660180                                        LR 0.001000    Time 0.052512    
2024-06-06 00:47:39,046 - Epoch: [58][ 1218/ 1218]    Overall Loss 0.660350    Objective Loss 0.660350    Top1 73.838631    Top5 95.599022    LR 0.001000    Time 0.052425    
2024-06-06 00:47:39,225 - --- validate (epoch=58)-----------
2024-06-06 00:47:39,225 - 34633 samples (256 per mini-batch)
2024-06-06 00:47:45,388 - Epoch: [58][  100/  136]    Loss 0.597907    Top1 71.691406    Top5 94.460938    
2024-06-06 00:47:47,185 - Epoch: [58][  136/  136]    Loss 0.589875    Top1 71.937170    Top5 94.565876    
2024-06-06 00:47:47,395 - ==> Top1: 71.937    Top5: 94.566    Loss: 0.590

2024-06-06 00:47:47,397 - ==> Confusion:
[[ 798    1    4    4    5    3    0    2    8   75    2    0    0    5    4    2    3    0    3    6    6]
 [   3  894    2    2   21   44    2   26    7    3    6    3    2    2    6    2   18    1    8    5    6]
 [  11    3  775   16    6    6   44   15    3    7    7    4    2    8    8    8    9    1    9   18   10]
 [   5    3   20  851    3   12    6    4    3    2   11    1    2    9   30    2    4    4   28    5   11]
 [  46   17    6    1  868   18    0    4    6   23    1    6    0    6   11    5   18    1    3    6    8]
 [   8   31    4    6   13  827    6   40    3   11    4   15    7   28    5    1    7    2    3   14    8]
 [   1   13   29    0    3    6  959   11    1    1    7    2    4    1    0   16    7    5    1   16    3]
 [   8   23   13    5    4   56    7  857    8    3    6   10    3    4    0    1    3    3   27   22   14]
 [  19    1    1    2    3    3    0    2  797   79   17    2    3   27   14    1    5    3   14    1    8]
 [ 110    2    1    0    2    5    0    4   49  774    1    0    2   27    9    1    0    1    1    4    8]
 [   4    3   14   12    1    6    6   10   25    2  910    0    2   21   12    1    2    1   16    4   12]
 [   4    3    0    0    0   22    5   12    2    2    3  822   28   17    1   11    5   23    2   43    6]
 [   0    3    1   14    1    4    1   12    9    2    4   98  740    9    6   11    8   36    8   14   14]
 [   7    0    3    0    5   11    0    7   15   20    8   14    4  864    5    1    8    7    2   15    5]
 [  26    4    4   21    4    2    0    3   49   11    7    1    3   14  910    3    2    4   14    2   14]
 [   3    2    8    1    5    2   12    4    0    0    0   22    9    3    2  951   14   13    3    3    9]
 [   8    9    3    2    9   14    1    0    5    4    5    7    3    7    7   18  942    2    4   10   12]
 [   7    1    2    1    0    1    4    0    7    2    0   39   38   11    6   24    2  837    1    8   14]
 [   2    9    7   17    4    6    0   47   18    2    8    5    4    1   22    1    1    4  877    8   15]
 [   2    4    4    2    1   14   19   25    2    0    1   14    6   10    1    6    9    3    3  953    9]
 [ 414  332  324  230  301  345  187  293  196  242  216  244  366  525  313  221  475  176  273  550 7709]]

2024-06-06 00:47:47,399 - ==> Best [Top1: 72.624   Top5: 94.950   Sparsity:0.00   Params: 169472 on epoch: 53]
2024-06-06 00:47:47,399 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:47:47,410 - 

2024-06-06 00:47:47,410 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:47:54,088 - Epoch: [59][  100/ 1218]    Overall Loss 0.671769    Objective Loss 0.671769                                        LR 0.001000    Time 0.066745    
2024-06-06 00:47:59,097 - Epoch: [59][  200/ 1218]    Overall Loss 0.662566    Objective Loss 0.662566                                        LR 0.001000    Time 0.058408    
2024-06-06 00:48:04,310 - Epoch: [59][  300/ 1218]    Overall Loss 0.658809    Objective Loss 0.658809                                        LR 0.001000    Time 0.056308    
2024-06-06 00:48:09,279 - Epoch: [59][  400/ 1218]    Overall Loss 0.662095    Objective Loss 0.662095                                        LR 0.001000    Time 0.054647    
2024-06-06 00:48:14,289 - Epoch: [59][  500/ 1218]    Overall Loss 0.660365    Objective Loss 0.660365                                        LR 0.001000    Time 0.053732    
2024-06-06 00:48:19,437 - Epoch: [59][  600/ 1218]    Overall Loss 0.659960    Objective Loss 0.659960                                        LR 0.001000    Time 0.053353    
2024-06-06 00:48:24,604 - Epoch: [59][  700/ 1218]    Overall Loss 0.660805    Objective Loss 0.660805                                        LR 0.001000    Time 0.053110    
2024-06-06 00:48:29,814 - Epoch: [59][  800/ 1218]    Overall Loss 0.660009    Objective Loss 0.660009                                        LR 0.001000    Time 0.052980    
2024-06-06 00:48:34,906 - Epoch: [59][  900/ 1218]    Overall Loss 0.660134    Objective Loss 0.660134                                        LR 0.001000    Time 0.052749    
2024-06-06 00:48:40,152 - Epoch: [59][ 1000/ 1218]    Overall Loss 0.658696    Objective Loss 0.658696                                        LR 0.001000    Time 0.052717    
2024-06-06 00:48:45,374 - Epoch: [59][ 1100/ 1218]    Overall Loss 0.660114    Objective Loss 0.660114                                        LR 0.001000    Time 0.052670    
2024-06-06 00:48:50,742 - Epoch: [59][ 1200/ 1218]    Overall Loss 0.659309    Objective Loss 0.659309                                        LR 0.001000    Time 0.052752    
2024-06-06 00:48:51,662 - Epoch: [59][ 1218/ 1218]    Overall Loss 0.659647    Objective Loss 0.659647    Top1 73.349633    Top5 93.643032    LR 0.001000    Time 0.052726    
2024-06-06 00:48:51,890 - --- validate (epoch=59)-----------
2024-06-06 00:48:51,890 - 34633 samples (256 per mini-batch)
2024-06-06 00:48:58,065 - Epoch: [59][  100/  136]    Loss 0.618668    Top1 71.308594    Top5 94.296875    
2024-06-06 00:48:59,833 - Epoch: [59][  136/  136]    Loss 0.619131    Top1 71.307712    Top5 94.424393    
2024-06-06 00:49:00,059 - ==> Top1: 71.308    Top5: 94.424    Loss: 0.619

2024-06-06 00:49:00,060 - ==> Confusion:
[[ 773    5    1    2   14    2    1    2    6   81    3    6    2    1    9    1    4    6    2    1    9]
 [   3  919    2    0   30   21    2   12    5    1    5   12    3    1    8    2   14    3    7    6    7]
 [  15    8  753   21    3    0   48   21    2    4    8    6    4    7    6   10    9    1    9   10   25]
 [   2    6   19  797    4    5    7    1    3    6   32    7    8    1   54    5    8   12   24    3   12]
 [  27   23    2    0  901    6    1    2    3   20    2    5    1    2   18   13   14    2    1    2    9]
 [   6   80    3    5   26  723   10   42    8    8    5   33   13   18    8    8   11   10    6   11    9]
 [   2   12   22    0    1    2  970   10    0    1    2    9    1    0    1   11    6    4    4   20    8]
 [   8   31   20    3    5   41   11  836    2    4    7   20    9    3    1    6    1    1   45   14    9]
 [  15    7    0    2    1    3    1    1  820   52   16    1    6   16   23    2    6    4   15    2    9]
 [  91    4    1    1    6    4    1    2   80  756    1    3    2   13   16    4    2    6    2    1    5]
 [   4   10    7   13    5    3   14   11   24    2  893    2    4   17   12    0    8    0   27    3    5]
 [   3    3    2    0    6    6    4    9    0    1    1  843   37    1    0   25    7   30    7   17    9]
 [   0    3    2    8    0    5    2    2    3    1    2   90  742    5    3    8    9   71   13   13   13]
 [   4    4    2    0    3   18    4    7   39   35   10   30   10  754    6   10   12   11    2   18   22]
 [  17   13    1    7   13    2    0    2   41   12   13    5    5    3  924    0    6    4   20    1    9]
 [   3    3    2    3    5    0    9    0    2    1    0   21    9    2    0  957   14   17    1    6   11]
 [   7   20    1    3   10    9    0    3    4    1    4    9    9    1    2   27  927    5    3    5   22]
 [   3    4    0    4    1    0    2    1    2    3    0   25   29    2    2   26    2  893    0    2    4]
 [   4   20    2   16    7    2    3   29   11    3    5    5    9    0   36    2    5    2  888    3    6]
 [   2   12    2    0    2    5   21   22    2    0    1   42    8    5    4   12   15    8    6  906   13]
 [ 339  490  173  149  345  176  176  248  238  188  287  344  475  321  386  336  537  260  308  435 7721]]

2024-06-06 00:49:00,062 - ==> Best [Top1: 72.624   Top5: 94.950   Sparsity:0.00   Params: 169472 on epoch: 53]
2024-06-06 00:49:00,062 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:49:00,072 - 

2024-06-06 00:49:00,072 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:49:06,741 - Epoch: [60][  100/ 1218]    Overall Loss 0.660604    Objective Loss 0.660604                                        LR 0.001000    Time 0.066663    
2024-06-06 00:49:11,845 - Epoch: [60][  200/ 1218]    Overall Loss 0.664245    Objective Loss 0.664245                                        LR 0.001000    Time 0.058838    
2024-06-06 00:49:16,936 - Epoch: [60][  300/ 1218]    Overall Loss 0.664163    Objective Loss 0.664163                                        LR 0.001000    Time 0.056189    
2024-06-06 00:49:22,042 - Epoch: [60][  400/ 1218]    Overall Loss 0.661363    Objective Loss 0.661363                                        LR 0.001000    Time 0.054900    
2024-06-06 00:49:27,196 - Epoch: [60][  500/ 1218]    Overall Loss 0.658713    Objective Loss 0.658713                                        LR 0.001000    Time 0.054223    
2024-06-06 00:49:32,292 - Epoch: [60][  600/ 1218]    Overall Loss 0.658960    Objective Loss 0.658960                                        LR 0.001000    Time 0.053675    
2024-06-06 00:49:37,695 - Epoch: [60][  700/ 1218]    Overall Loss 0.658508    Objective Loss 0.658508                                        LR 0.001000    Time 0.053723    
2024-06-06 00:49:42,838 - Epoch: [60][  800/ 1218]    Overall Loss 0.658271    Objective Loss 0.658271                                        LR 0.001000    Time 0.053434    
2024-06-06 00:49:48,170 - Epoch: [60][  900/ 1218]    Overall Loss 0.659070    Objective Loss 0.659070                                        LR 0.001000    Time 0.053418    
2024-06-06 00:49:53,283 - Epoch: [60][ 1000/ 1218]    Overall Loss 0.658726    Objective Loss 0.658726                                        LR 0.001000    Time 0.053187    
2024-06-06 00:49:58,280 - Epoch: [60][ 1100/ 1218]    Overall Loss 0.657936    Objective Loss 0.657936                                        LR 0.001000    Time 0.052893    
2024-06-06 00:50:03,428 - Epoch: [60][ 1200/ 1218]    Overall Loss 0.658694    Objective Loss 0.658694                                        LR 0.001000    Time 0.052773    
2024-06-06 00:50:04,354 - Epoch: [60][ 1218/ 1218]    Overall Loss 0.659019    Objective Loss 0.659019    Top1 66.503667    Top5 92.909535    LR 0.001000    Time 0.052751    
2024-06-06 00:50:04,565 - --- validate (epoch=60)-----------
2024-06-06 00:50:04,566 - 34633 samples (256 per mini-batch)
2024-06-06 00:50:10,882 - Epoch: [60][  100/  136]    Loss 0.597980    Top1 71.242188    Top5 94.332031    
2024-06-06 00:50:12,798 - Epoch: [60][  136/  136]    Loss 0.594693    Top1 71.143129    Top5 94.412843    
2024-06-06 00:50:13,022 - ==> Top1: 71.143    Top5: 94.413    Loss: 0.595

2024-06-06 00:50:13,024 - ==> Confusion:
[[ 774    4    8    1   19    2    0    0    9   87    1    0    0    7    3    5    4    0    2    2    3]
 [   5  865    5    1   27   58    7   16    7    5    7    4    2   11    6    2    6    3   14    9    3]
 [   9    5  834   10    6    7   23   11    3    7    5    2    2    9    5    5    6    3    6    4    8]
 [   3    2   41  839    8   11    3    4    3    4   22    0   12    8   33    1    1    6    7    1    7]
 [  27   14    5    1  919   15    1    4    0   12    3    3    1   13   13    4    4    1    4    3    7]
 [   7   20    6    6   19  825    5   29    5   10    1   16    5   38    4    3    4    2   11   18    9]
 [   1    3   55    5    3    7  947    7    5    3    3    1    1    5    1    6    4    4    1   17    7]
 [   8    8   19    1    8   68    4  863    5    4    5    7    7    9    1    3    1    2   28   20    6]
 [  13    4    0    2    3    6    1    1  750   74   27    0    5   41   54    0    4    2   12    0    3]
 [  87    1    3    0   17    5    0    3   37  794    3    0    0   22   16    0    2    1    3    1    6]
 [   2    2   24   16    3    6    3   10   15    4  925    0    1   18   11    0    2    0   16    1    5]
 [   2    5    3    2    1   16    2    5    5    4    2  812   36   43    2   16    1   21    1   25    7]
 [   2    1    3   10    0    3    1    6    8    1    3  110  750   14    4   13    6   34    9    8    9]
 [   8    3    2    0    9   14    1    3   14   18    2   12    3  879    6    0    5    6    3    9    4]
 [  10    6    5   21   20    2    1    3   28   11    8    0    4   14  934    0    4    3   16    2    6]
 [   6    1    5    1   10    1   10    0    3    1    0   21   15   12    0  926   22   11    2    5   14]
 [   5   12    8    3   13   11    1    2    3    4    1   11    5   21    4    9  923    6    2   11   17]
 [   4    3    2    5    1    2    3    0    3    6    2   37   31   13   10   20    4  843    5    6    5]
 [   6   11   11   27    4    9    1   25    9    3    9    0    1    5   35    1    3    0  884    4   10]
 [   4    2    6    2    5   12   11   15    2    1    1   20    5   19    0    4   13    5    5  943   13]
 [ 363  320  456  195  448  369  149  227  167  222  277  237  420  653  464  212  518  141  258  426 7410]]

2024-06-06 00:50:13,026 - ==> Best [Top1: 72.624   Top5: 94.950   Sparsity:0.00   Params: 169472 on epoch: 53]
2024-06-06 00:50:13,026 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:50:13,037 - 

2024-06-06 00:50:13,037 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:50:19,727 - Epoch: [61][  100/ 1218]    Overall Loss 0.660294    Objective Loss 0.660294                                        LR 0.001000    Time 0.066864    
2024-06-06 00:50:24,882 - Epoch: [61][  200/ 1218]    Overall Loss 0.664572    Objective Loss 0.664572                                        LR 0.001000    Time 0.059198    
2024-06-06 00:50:29,792 - Epoch: [61][  300/ 1218]    Overall Loss 0.657119    Objective Loss 0.657119                                        LR 0.001000    Time 0.055826    
2024-06-06 00:50:34,615 - Epoch: [61][  400/ 1218]    Overall Loss 0.655131    Objective Loss 0.655131                                        LR 0.001000    Time 0.053923    
2024-06-06 00:50:39,475 - Epoch: [61][  500/ 1218]    Overall Loss 0.656739    Objective Loss 0.656739                                        LR 0.001000    Time 0.052855    
2024-06-06 00:50:44,110 - Epoch: [61][  600/ 1218]    Overall Loss 0.657874    Objective Loss 0.657874                                        LR 0.001000    Time 0.051768    
2024-06-06 00:50:48,727 - Epoch: [61][  700/ 1218]    Overall Loss 0.657245    Objective Loss 0.657245                                        LR 0.001000    Time 0.050966    
2024-06-06 00:50:53,276 - Epoch: [61][  800/ 1218]    Overall Loss 0.657732    Objective Loss 0.657732                                        LR 0.001000    Time 0.050279    
2024-06-06 00:50:57,939 - Epoch: [61][  900/ 1218]    Overall Loss 0.657682    Objective Loss 0.657682                                        LR 0.001000    Time 0.049871    
2024-06-06 00:51:02,493 - Epoch: [61][ 1000/ 1218]    Overall Loss 0.657262    Objective Loss 0.657262                                        LR 0.001000    Time 0.049436    
2024-06-06 00:51:07,405 - Epoch: [61][ 1100/ 1218]    Overall Loss 0.658139    Objective Loss 0.658139                                        LR 0.001000    Time 0.049406    
2024-06-06 00:51:12,337 - Epoch: [61][ 1200/ 1218]    Overall Loss 0.657933    Objective Loss 0.657933                                        LR 0.001000    Time 0.049398    
2024-06-06 00:51:13,290 - Epoch: [61][ 1218/ 1218]    Overall Loss 0.658323    Objective Loss 0.658323    Top1 70.904645    Top5 93.643032    LR 0.001000    Time 0.049449    
2024-06-06 00:51:13,462 - --- validate (epoch=61)-----------
2024-06-06 00:51:13,462 - 34633 samples (256 per mini-batch)
2024-06-06 00:51:18,944 - Epoch: [61][  100/  136]    Loss 0.594159    Top1 71.871094    Top5 94.820312    
2024-06-06 00:51:20,604 - Epoch: [61][  136/  136]    Loss 0.594762    Top1 71.888084    Top5 94.716022    
2024-06-06 00:51:20,797 - ==> Top1: 71.888    Top5: 94.716    Loss: 0.595

2024-06-06 00:51:20,798 - ==> Confusion:
[[ 775    1    1    2   20    2    1    1    9   72    4    2    2    4    7    4    2    2    7    4    9]
 [   2  855    2    2   23   59    9   26    3    2   15    6    4    2    9    1    9    3   17    6    8]
 [  11    4  750   21    9    8   59   19    1    9   16    4    4    7    4    5    5    2   13    7   12]
 [   5    3   24  837    3   15    6    0    1    2   32    4   12    3   31    3    5    6   14    1    9]
 [  33   10    2    0  907   20    3    3    1   16    5    4    2    5   15    4    4    0    7    2   11]
 [   3   33    8    8   19  833    7   33    2    3    8   25   13   11    6    4    2    0    8   10    7]
 [   1    6   20    6    2    6  977    4    1    1    8   10    2    2    1    5    2    5    4   16    7]
 [   5   16   13    3    6   62   12  838    1    3   10   18    5    3    5    0    2    5   38   19   13]
 [  11    2    1    1    3    5    0    5  789   50   31    2    6   29   38    0    2    2   15    1    9]
 [  85    0    2    0   14    4    0    1   60  779    0    0    2   28    7    0    1    3    4    0   11]
 [   4    1    7    9    3    3    3    3   14    2  954    3    3   15   12    0    0    1   17    0   10]
 [   4    4    3    0    0    9    7    7    1    1    1  835   52    8    0   17    7   24    4   18    9]
 [   1    2    2    7    1    3    3    3    2    0    4   84  804    0    2    8    2   43    7   10    7]
 [   6    3    3    1    5   30    1    4   24   19   23   20    6  811    8    5    2    2    4   11   13]
 [  15    3    3   23   12    4    1    1   40    7   13    0    3    8  933    1    2    5   17    0    7]
 [   0    3    6    2    6    1   17    0    0    1    1   32   16    3    0  932    9   20    3    7    7]
 [   3   18    6    1   14   10    6    0    6    0    6   15    9    6    1   20  910    3    8   10   20]
 [   2    1    0    7    2    3    1    0    1    3    1   27   49    9    6   20    1  853    6    3   10]
 [   4    8   10   22    3    4    2   28   11    0   17    2    6    1   26    0    1    1  898    5    9]
 [   1   10    3    2    2   12   17   11    1    0    2   26   14    7    2   11    8    3   13  927   16]
 [ 317  309  275  198  381  368  271  210  174  177  434  267  523  366  406  277  369  167  324  419 7700]]

2024-06-06 00:51:20,800 - ==> Best [Top1: 72.624   Top5: 94.950   Sparsity:0.00   Params: 169472 on epoch: 53]
2024-06-06 00:51:20,800 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:51:20,807 - 

2024-06-06 00:51:20,808 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:51:26,821 - Epoch: [62][  100/ 1218]    Overall Loss 0.643468    Objective Loss 0.643468                                        LR 0.001000    Time 0.060117    
2024-06-06 00:51:31,512 - Epoch: [62][  200/ 1218]    Overall Loss 0.639359    Objective Loss 0.639359                                        LR 0.001000    Time 0.053502    
2024-06-06 00:51:36,470 - Epoch: [62][  300/ 1218]    Overall Loss 0.647674    Objective Loss 0.647674                                        LR 0.001000    Time 0.052190    
2024-06-06 00:51:41,023 - Epoch: [62][  400/ 1218]    Overall Loss 0.650729    Objective Loss 0.650729                                        LR 0.001000    Time 0.050520    
2024-06-06 00:51:45,643 - Epoch: [62][  500/ 1218]    Overall Loss 0.648948    Objective Loss 0.648948                                        LR 0.001000    Time 0.049652    
2024-06-06 00:51:50,276 - Epoch: [62][  600/ 1218]    Overall Loss 0.647077    Objective Loss 0.647077                                        LR 0.001000    Time 0.049096    
2024-06-06 00:51:54,924 - Epoch: [62][  700/ 1218]    Overall Loss 0.648330    Objective Loss 0.648330                                        LR 0.001000    Time 0.048719    
2024-06-06 00:51:59,635 - Epoch: [62][  800/ 1218]    Overall Loss 0.649812    Objective Loss 0.649812                                        LR 0.001000    Time 0.048516    
2024-06-06 00:52:04,296 - Epoch: [62][  900/ 1218]    Overall Loss 0.650588    Objective Loss 0.650588                                        LR 0.001000    Time 0.048303    
2024-06-06 00:52:08,993 - Epoch: [62][ 1000/ 1218]    Overall Loss 0.651140    Objective Loss 0.651140                                        LR 0.001000    Time 0.048168    
2024-06-06 00:52:13,562 - Epoch: [62][ 1100/ 1218]    Overall Loss 0.653326    Objective Loss 0.653326                                        LR 0.001000    Time 0.047941    
2024-06-06 00:52:18,130 - Epoch: [62][ 1200/ 1218]    Overall Loss 0.653450    Objective Loss 0.653450                                        LR 0.001000    Time 0.047751    
2024-06-06 00:52:18,903 - Epoch: [62][ 1218/ 1218]    Overall Loss 0.653432    Objective Loss 0.653432    Top1 74.816626    Top5 93.887531    LR 0.001000    Time 0.047680    
2024-06-06 00:52:19,091 - --- validate (epoch=62)-----------
2024-06-06 00:52:19,091 - 34633 samples (256 per mini-batch)
2024-06-06 00:52:24,578 - Epoch: [62][  100/  136]    Loss 0.614682    Top1 68.992188    Top5 92.800781    
2024-06-06 00:52:26,311 - Epoch: [62][  136/  136]    Loss 0.606009    Top1 69.350042    Top5 92.882511    
2024-06-06 00:52:26,482 - ==> Top1: 69.350    Top5: 92.883    Loss: 0.606

2024-06-06 00:52:26,483 - ==> Confusion:
[[ 803    4    2    1    9    0    0    0    7   74    1    3    1    4   10    1    1    1    5    1    3]
 [   6  909    5    1   25   15    6   15    6    4   10    4    1    1   14    1    9    3   17    8    3]
 [  16    6  810   10    5    2   24   18    1    6    8    5    2    9    6    8   10    1   11    7    5]
 [   7    3   35  825    6    3    2    3    4    3   32    3    9    1   47    1    2    5   17    4    4]
 [  46   13    2    1  889   13    2    4    3   17    2    5    0    1   20    2   10    2    8    2   12]
 [   9   68    5   10   28  735    9   65    5    8    7   15    5   30    5    3    7    1   12   12    4]
 [   4    6   40    3    3    2  956   12    0    2   10    6    1    1    3    6    2    6    2   18    3]
 [  10   25   15    7    4   29    7  852    8    1    5   11    2    4    2    1    3    3   67   17    4]
 [  21    7    2    1    4    2    1    5  793   65   20    1    2   14   35    0    7    2   12    3    5]
 [ 101    2    1    1    5    1    2    3   45  782    1    1    0   29   14    1    1    1    7    1    2]
 [   4    6   10   17    2    4    6   12   19    4  911    0    6   14   16    0    3    1   21    5    3]
 [   4    5    1    1    2   10    2   15    1    1    1  837   36   12    2   10    4   33    7   24    3]
 [   2    1    1    9    0    6    2    6    6    0    3   96  751    6    9    5    4   65   14    3    6]
 [   7    6    4    0    8   12    2    9   23   32   11   15    5  832   12    2    2    5    3    8    3]
 [  20    3    1   11    7    2    1    3   56   12    5    2    1    7  944    0    0    3   13    1    6]
 [   7    3    3    1   11    0   16    2    0    1    2   32   12    1    1  894   28   31    6    7    8]
 [  16   11    3    3   13    8    3    2    8    0    2   14    7    8    6   11  923    2    8   12   12]
 [   3    3    1    3    2    1    0    2    4    3    2   27   31    4   12   14    1  869   11    4    8]
 [   4   11   12   24    3    4    1   22   13    1   12    5    1    2   39    0    4    2  888    3    7]
 [   4   12    2    1    1    8   16   21    0    2    4   34    8    7    3    5   11    5    8  933    3]
 [ 557  424  400  208  431  235  194  291  248  206  302  284  461  421  621  206  492  188  386  496 6881]]

2024-06-06 00:52:26,484 - ==> Best [Top1: 72.624   Top5: 94.950   Sparsity:0.00   Params: 169472 on epoch: 53]
2024-06-06 00:52:26,484 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:52:26,492 - 

2024-06-06 00:52:26,493 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:52:32,815 - Epoch: [63][  100/ 1218]    Overall Loss 0.653187    Objective Loss 0.653187                                        LR 0.001000    Time 0.063197    
2024-06-06 00:52:37,560 - Epoch: [63][  200/ 1218]    Overall Loss 0.652257    Objective Loss 0.652257                                        LR 0.001000    Time 0.055313    
2024-06-06 00:52:42,165 - Epoch: [63][  300/ 1218]    Overall Loss 0.653777    Objective Loss 0.653777                                        LR 0.001000    Time 0.052220    
2024-06-06 00:52:46,672 - Epoch: [63][  400/ 1218]    Overall Loss 0.654902    Objective Loss 0.654902                                        LR 0.001000    Time 0.050427    
2024-06-06 00:52:51,314 - Epoch: [63][  500/ 1218]    Overall Loss 0.654805    Objective Loss 0.654805                                        LR 0.001000    Time 0.049622    
2024-06-06 00:52:55,959 - Epoch: [63][  600/ 1218]    Overall Loss 0.651821    Objective Loss 0.651821                                        LR 0.001000    Time 0.049092    
2024-06-06 00:53:00,613 - Epoch: [63][  700/ 1218]    Overall Loss 0.650707    Objective Loss 0.650707                                        LR 0.001000    Time 0.048724    
2024-06-06 00:53:05,423 - Epoch: [63][  800/ 1218]    Overall Loss 0.651147    Objective Loss 0.651147                                        LR 0.001000    Time 0.048645    
2024-06-06 00:53:10,021 - Epoch: [63][  900/ 1218]    Overall Loss 0.652292    Objective Loss 0.652292                                        LR 0.001000    Time 0.048346    
2024-06-06 00:53:14,673 - Epoch: [63][ 1000/ 1218]    Overall Loss 0.654073    Objective Loss 0.654073                                        LR 0.001000    Time 0.048162    
2024-06-06 00:53:19,358 - Epoch: [63][ 1100/ 1218]    Overall Loss 0.655816    Objective Loss 0.655816                                        LR 0.001000    Time 0.048040    
2024-06-06 00:53:24,082 - Epoch: [63][ 1200/ 1218]    Overall Loss 0.655876    Objective Loss 0.655876                                        LR 0.001000    Time 0.047973    
2024-06-06 00:53:24,885 - Epoch: [63][ 1218/ 1218]    Overall Loss 0.655912    Objective Loss 0.655912    Top1 72.860636    Top5 96.821516    LR 0.001000    Time 0.047922    
2024-06-06 00:53:25,068 - --- validate (epoch=63)-----------
2024-06-06 00:53:25,068 - 34633 samples (256 per mini-batch)
2024-06-06 00:53:30,640 - Epoch: [63][  100/  136]    Loss 0.605025    Top1 71.371094    Top5 93.875000    
2024-06-06 00:53:32,489 - Epoch: [63][  136/  136]    Loss 0.603908    Top1 71.426096    Top5 94.008605    
2024-06-06 00:53:32,672 - ==> Top1: 71.426    Top5: 94.009    Loss: 0.604

2024-06-06 00:53:32,674 - ==> Confusion:
[[ 742    2    8    2   27    2    0    3   10   90    5    3    4    5    6    0    3    2    2    1   14]
 [   0  912    2    2   25   37    8   17    2    2    4    6    6    0    4    2    6    0   13    4   11]
 [   8    9  786   12    5    4   52    9    2    2   12    8    1    7    5   12    6    4   10    2   14]
 [   3    5   38  809    9    6    5    1    2    2   36    3   17    4   29    2    4    9   22    2    8]
 [  29   18    3    0  912   10    2    3    2   15    8    3    5    7   10    5    5    1    5    0   11]
 [   4   48    4    5   12  814    6   35    2    3    3   31   12   24    1    3   11    0    7    6   12]
 [   3   10   30    2    4    2  964    5    2    1    7    8    8    0    0   16    0    3    3    8   10]
 [   8   31   14    4    1   70   11  814    2    0   15   20   12    5    3    2    3    3   34   21    4]
 [  14   12    0    0    3    3    1    3  773   56   22    7   16   33   27    1    1    7   13    2    8]
 [  71    1    0    1   14    3    0    3   54  781    1    2    2   34   11    2    4    2    1    1   13]
 [   6    5   10    9    3    8    9    6   11    4  936    2    3   17   13    1    1    1   11    0    8]
 [   3    2    6    0    1   10    6    5    1    2    2  844   41    5    0   18    7   22    3   23   10]
 [   0    3    6    4    1    1    5    4    1    1    3   87  795    6    2    7    2   35    5   11   16]
 [   4    2    7    0   10   15    2    4    9   14   19   24   11  838    4    2    5    9    0    4   18]
 [   8    8    8   19   14    3    0    2   42   13   10    7    5   11  903    0    3    8   20    3   11]
 [   1    1    2    1    8    2   15    1    2    0    2   23   21    5    0  918   18   17    4    7   18]
 [   1   16    4    1   15    8    1    3    4    2    6   14   13   11    3   15  918    7    1   14   15]
 [   2    2    0    3    2    4    3    1    4    2    2   38   54    5    4   21    1  841    2    7    7]
 [   0   13   10   20    8    9    2   26   13    2   19    6    5    1   31    1    0    2  874    5   11]
 [   0   14    8    0    3   14   25   12    0    0    4   34   17    6    1    6    9    3    1  920   11]
 [ 247  469  357  131  457  325  215  210  144  141  328  334  619  456  309  266  393  217  280  391 7643]]

2024-06-06 00:53:32,676 - ==> Best [Top1: 72.624   Top5: 94.950   Sparsity:0.00   Params: 169472 on epoch: 53]
2024-06-06 00:53:32,676 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:53:32,687 - 

2024-06-06 00:53:32,687 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:53:38,949 - Epoch: [64][  100/ 1218]    Overall Loss 0.661649    Objective Loss 0.661649                                        LR 0.001000    Time 0.062592    
2024-06-06 00:53:43,789 - Epoch: [64][  200/ 1218]    Overall Loss 0.660249    Objective Loss 0.660249                                        LR 0.001000    Time 0.055487    
2024-06-06 00:53:48,643 - Epoch: [64][  300/ 1218]    Overall Loss 0.652120    Objective Loss 0.652120                                        LR 0.001000    Time 0.053167    
2024-06-06 00:53:53,266 - Epoch: [64][  400/ 1218]    Overall Loss 0.654888    Objective Loss 0.654888                                        LR 0.001000    Time 0.051428    
2024-06-06 00:53:57,827 - Epoch: [64][  500/ 1218]    Overall Loss 0.656730    Objective Loss 0.656730                                        LR 0.001000    Time 0.050259    
2024-06-06 00:54:02,441 - Epoch: [64][  600/ 1218]    Overall Loss 0.657443    Objective Loss 0.657443                                        LR 0.001000    Time 0.049569    
2024-06-06 00:54:07,111 - Epoch: [64][  700/ 1218]    Overall Loss 0.656721    Objective Loss 0.656721                                        LR 0.001000    Time 0.049158    
2024-06-06 00:54:11,708 - Epoch: [64][  800/ 1218]    Overall Loss 0.655700    Objective Loss 0.655700                                        LR 0.001000    Time 0.048758    
2024-06-06 00:54:16,945 - Epoch: [64][  900/ 1218]    Overall Loss 0.656911    Objective Loss 0.656911                                        LR 0.001000    Time 0.049157    
2024-06-06 00:54:21,962 - Epoch: [64][ 1000/ 1218]    Overall Loss 0.657572    Objective Loss 0.657572                                        LR 0.001000    Time 0.049256    
2024-06-06 00:54:26,614 - Epoch: [64][ 1100/ 1218]    Overall Loss 0.656820    Objective Loss 0.656820                                        LR 0.001000    Time 0.049005    
2024-06-06 00:54:31,601 - Epoch: [64][ 1200/ 1218]    Overall Loss 0.657230    Objective Loss 0.657230                                        LR 0.001000    Time 0.049076    
2024-06-06 00:54:32,435 - Epoch: [64][ 1218/ 1218]    Overall Loss 0.656675    Objective Loss 0.656675    Top1 72.860636    Top5 95.354523    LR 0.001000    Time 0.049036    
2024-06-06 00:54:32,622 - --- validate (epoch=64)-----------
2024-06-06 00:54:32,622 - 34633 samples (256 per mini-batch)
2024-06-06 00:54:38,370 - Epoch: [64][  100/  136]    Loss 0.603957    Top1 71.718750    Top5 94.507812    
2024-06-06 00:54:39,941 - Epoch: [64][  136/  136]    Loss 0.598211    Top1 72.012243    Top5 94.519678    
2024-06-06 00:54:40,130 - ==> Top1: 72.012    Top5: 94.520    Loss: 0.598

2024-06-06 00:54:40,131 - ==> Confusion:
[[ 795    2    1    2    9    3    1    1    7   72    0    2    1    3    8    5    3    4    2    0   10]
 [   6  874    4    3   34   40    4   24    8    2    7    2    5    2   15    1    8    2   15    2    5]
 [  12    2  807   17    3    8   42    9    1    6    7    5    5    3    5    8    5    1   10    6    8]
 [   1    3   25  827    5   11    6    2    1    3   26    1    5    0   52    5    5    7   20    2    9]
 [  34    3    3    3  915   12    0    4    1   20    2    5    1    4   20    4    7    1    7    1    7]
 [  10   34    3    8   26  825    2   29    6    8    3   15    7   16    8    3    7    6    6    7   14]
 [   3    6   21    7    5    8  949    6    0    4    5    6    1    2    0   17    8    3    5   16   14]
 [   6   14   16    5    3   72    9  816    3    6   12    6    2    4    5    3    1    2   64   14   14]
 [  14    7    2    2    2    2    1    0  802   82   12    3    3   21   30    1    2    1    7    1    7]
 [  97    0    0    1   12    2    0    0   66  793    0    0    0   14    9    0    1    1    0    0    5]
 [   2    2    6   19    2    4    4    9   22    2  920    2    6   16   12    0    0    0   22    3   11]
 [   2    6    4    1    7   33    4    8    2    1    0  809   30    9    1   17    4   31    9   23   10]
 [   3    2    3   13    0   10    5    5    3    1    4   99  748    5    4   11    5   49    7    4   14]
 [   5    3    0    2   14   29    1    5   27   30   14    9    3  817   10    4    2    2    3    8   13]
 [  19    4    1   13    8    0    0    1   45   13    8    1    5    8  942    1    2    2   17    3    5]
 [   4    5    6    0   14    4   10    1    0    3    1   23    5    4    1  938   13   17    2    3   12]
 [   7   17    5    2   19   14    2    0    5    0    5    6    2    6    7   21  919    3    3    8   21]
 [   6    1    4    4    0    3    5    1    3    5    2   34   26   11    7   22    2  854    5    2    8]
 [   4   13   10   18    4    5    0   20   15    0    9    2    3    3   34    2    0    1  904    3    8]
 [   2    9    7    2    5   23   26   22    1    0    3   27    8    5    2   11    7    4    8  902   14]
 [ 399  318  304  209  453  399  128  201  219  272  285  254  356  341  416  273  417  193  330  381 7784]]

2024-06-06 00:54:40,133 - ==> Best [Top1: 72.624   Top5: 94.950   Sparsity:0.00   Params: 169472 on epoch: 53]
2024-06-06 00:54:40,133 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:54:40,140 - 

2024-06-06 00:54:40,140 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:54:46,203 - Epoch: [65][  100/ 1218]    Overall Loss 0.645090    Objective Loss 0.645090                                        LR 0.001000    Time 0.060607    
2024-06-06 00:54:50,968 - Epoch: [65][  200/ 1218]    Overall Loss 0.648952    Objective Loss 0.648952                                        LR 0.001000    Time 0.054120    
2024-06-06 00:54:55,563 - Epoch: [65][  300/ 1218]    Overall Loss 0.646775    Objective Loss 0.646775                                        LR 0.001000    Time 0.051393    
2024-06-06 00:55:00,549 - Epoch: [65][  400/ 1218]    Overall Loss 0.647261    Objective Loss 0.647261                                        LR 0.001000    Time 0.051005    
2024-06-06 00:55:05,475 - Epoch: [65][  500/ 1218]    Overall Loss 0.650832    Objective Loss 0.650832                                        LR 0.001000    Time 0.050652    
2024-06-06 00:55:10,218 - Epoch: [65][  600/ 1218]    Overall Loss 0.653259    Objective Loss 0.653259                                        LR 0.001000    Time 0.050112    
2024-06-06 00:55:14,839 - Epoch: [65][  700/ 1218]    Overall Loss 0.653367    Objective Loss 0.653367                                        LR 0.001000    Time 0.049553    
2024-06-06 00:55:19,647 - Epoch: [65][  800/ 1218]    Overall Loss 0.654418    Objective Loss 0.654418                                        LR 0.001000    Time 0.049366    
2024-06-06 00:55:24,558 - Epoch: [65][  900/ 1218]    Overall Loss 0.653654    Objective Loss 0.653654                                        LR 0.001000    Time 0.049335    
2024-06-06 00:55:29,452 - Epoch: [65][ 1000/ 1218]    Overall Loss 0.651874    Objective Loss 0.651874                                        LR 0.001000    Time 0.049294    
2024-06-06 00:55:34,226 - Epoch: [65][ 1100/ 1218]    Overall Loss 0.652818    Objective Loss 0.652818                                        LR 0.001000    Time 0.049151    
2024-06-06 00:55:38,877 - Epoch: [65][ 1200/ 1218]    Overall Loss 0.653493    Objective Loss 0.653493                                        LR 0.001000    Time 0.048930    
2024-06-06 00:55:39,687 - Epoch: [65][ 1218/ 1218]    Overall Loss 0.653522    Objective Loss 0.653522    Top1 72.371638    Top5 94.621027    LR 0.001000    Time 0.048871    
2024-06-06 00:55:39,881 - --- validate (epoch=65)-----------
2024-06-06 00:55:39,882 - 34633 samples (256 per mini-batch)
2024-06-06 00:55:45,737 - Epoch: [65][  100/  136]    Loss 0.590338    Top1 72.937500    Top5 95.386719    
2024-06-06 00:55:47,512 - Epoch: [65][  136/  136]    Loss 0.593706    Top1 72.708111    Top5 95.281957    
2024-06-06 00:55:47,703 - ==> Top1: 72.708    Top5: 95.282    Loss: 0.594

2024-06-06 00:55:47,704 - ==> Confusion:
[[ 779    0    3    2   22    6    0    2   10   55    1    7    0    4    9    4    7    2    4    2   12]
 [   3  910    4    1   24   29    3   23    2    1    2   10    3    2   11    2    8    3   15    1    6]
 [   9    4  783   20   11    7   29   19    0    4    8    8    4   10    2   16   12    0    9    3   12]
 [   2    2   23  827    5   15    3    3    1    1   35    3   15    3   33    5    2    6   24    0    8]
 [  23   16    1    1  916   15    1    4    2   10    4    5    1    5    7    8   17    3    5    2    8]
 [   3   33    1   10   13  823    2   41    1    3    6   26   12   21    8    4    6    2    4    9   15]
 [   0    8   39    2    2    7  940    8    1    0    7    3   11    0    0    9    6    6    8   12   17]
 [   2   22   17    6    4   77    6  830    3    2    4   18    5    2    3    0    1    2   48   13   12]
 [  19   13    1    1    3    5    0    3  767   68   14    4    8   24   31    0    9    1   18    0   13]
 [  95    2    0    0   13    8    1    4   50  744    2    6    3   31   14    3    4    2    3    1   15]
 [   0    5   13   13    3    8    6   15   14    1  901    5    6   11   17    1    7    1   24    1   12]
 [   2    2    3    1    1   14    2    8    0    1    0  856   36   16    1   15    3   20    3   21    6]
 [   0    0    1    4    2    7    0    5    1    0    2  124  786    6    2    7    2   24    8    3   11]
 [   2    4    5    3    5   31    0    6   10   27   17   26   11  813    3    6    6    4    2    6   14]
 [  18   12    0   22   18    4    0    4   29    5    6    2    7   12  924    0    4    7   13    1   10]
 [   2    2    8    2    3    3    9    0    0    0    0   43   13    2    0  914   26   16    4    5   14]
 [   6    5    3    1    3   11    2    1    5    0    3   14    8    5    2   12  957    1    6    8   19]
 [   2    2    1    2    0    6    1    3    0    1    0   51   63    6    6   17    2  825    2    2   13]
 [   3    6    8   18    3    5    4   26    8    1   11   11    6    1   16    1    3    2  903    4   18]
 [   2   10    2    2    1   21   10   26    1    0    4   29   15    8    0   12   10    0   10  914   11]
 [ 257  335  220  154  325  352  157  268  150  128  224  377  514  379  323  253  643  141  338  325 8069]]

2024-06-06 00:55:47,706 - ==> Best [Top1: 72.708   Top5: 95.282   Sparsity:0.00   Params: 169472 on epoch: 65]
2024-06-06 00:55:47,706 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:55:47,721 - 

2024-06-06 00:55:47,721 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:55:54,074 - Epoch: [66][  100/ 1218]    Overall Loss 0.659522    Objective Loss 0.659522                                        LR 0.001000    Time 0.063510    
2024-06-06 00:55:58,635 - Epoch: [66][  200/ 1218]    Overall Loss 0.657732    Objective Loss 0.657732                                        LR 0.001000    Time 0.054552    
2024-06-06 00:56:03,225 - Epoch: [66][  300/ 1218]    Overall Loss 0.655118    Objective Loss 0.655118                                        LR 0.001000    Time 0.051663    
2024-06-06 00:56:07,766 - Epoch: [66][  400/ 1218]    Overall Loss 0.655391    Objective Loss 0.655391                                        LR 0.001000    Time 0.050093    
2024-06-06 00:56:12,540 - Epoch: [66][  500/ 1218]    Overall Loss 0.654299    Objective Loss 0.654299                                        LR 0.001000    Time 0.049621    
2024-06-06 00:56:17,213 - Epoch: [66][  600/ 1218]    Overall Loss 0.655999    Objective Loss 0.655999                                        LR 0.001000    Time 0.049135    
2024-06-06 00:56:21,818 - Epoch: [66][  700/ 1218]    Overall Loss 0.657533    Objective Loss 0.657533                                        LR 0.001000    Time 0.048693    
2024-06-06 00:56:26,386 - Epoch: [66][  800/ 1218]    Overall Loss 0.656493    Objective Loss 0.656493                                        LR 0.001000    Time 0.048313    
2024-06-06 00:56:30,933 - Epoch: [66][  900/ 1218]    Overall Loss 0.656536    Objective Loss 0.656536                                        LR 0.001000    Time 0.047996    
2024-06-06 00:56:35,531 - Epoch: [66][ 1000/ 1218]    Overall Loss 0.655890    Objective Loss 0.655890                                        LR 0.001000    Time 0.047792    
2024-06-06 00:56:40,140 - Epoch: [66][ 1100/ 1218]    Overall Loss 0.656028    Objective Loss 0.656028                                        LR 0.001000    Time 0.047636    
2024-06-06 00:56:44,850 - Epoch: [66][ 1200/ 1218]    Overall Loss 0.656286    Objective Loss 0.656286                                        LR 0.001000    Time 0.047589    
2024-06-06 00:56:45,631 - Epoch: [66][ 1218/ 1218]    Overall Loss 0.656087    Objective Loss 0.656087    Top1 68.704156    Top5 92.909535    LR 0.001000    Time 0.047528    
2024-06-06 00:56:45,809 - --- validate (epoch=66)-----------
2024-06-06 00:56:45,810 - 34633 samples (256 per mini-batch)
2024-06-06 00:56:51,425 - Epoch: [66][  100/  136]    Loss 0.598392    Top1 71.468750    Top5 94.507812    
2024-06-06 00:56:53,137 - Epoch: [66][  136/  136]    Loss 0.589213    Top1 71.478070    Top5 94.511016    
2024-06-06 00:56:53,335 - ==> Top1: 71.478    Top5: 94.511    Loss: 0.589

2024-06-06 00:56:53,336 - ==> Confusion:
[[ 813    2    3    0    6    4    2    2    7   54    1    3    2    1    8    2    2    4    3    2   10]
 [   3  862    4    1   20   53    7   23    9    3    7    8    7    1    1    3   11    3   21    5   11]
 [  13    8  776   14    5    5   28   13    3    5    9   12    6    8    5   14    5    0    9   13   19]
 [   5    1   29  834    4   12    2    5    4    1   15    4   13    3   30    6    4   12   23    4    5]
 [  56   17    1    0  876   15    1    2    6   19    3    3    3    0    9    8   13    2    4    5   11]
 [   6   27    4    1    9  810    5   53    6    7    4   24   12   31    3    3    4    3    7   17    7]
 [   4    3   26    2    4    2  972    8    2    0    4    9    4    2    0    9    3    4    4   14   10]
 [   5   13   14    2    2   45    9  892    5    1    7   11   14    3    2    1    1    3   25   15    7]
 [  18    3    0    4    0    1    2    1  843   41   12    2    7   17   15    3    6    6   10    2    9]
 [ 142    0    6    0    6    9    0    4   80  703    2    2    0   19    9    1    4    4    4    0    6]
 [   0    6   18   17    2    5    6   13   32    5  876    2    3   24   12    2    1    1   29    4    6]
 [   5    1    1    0    1   10    3   10    1    2    0  850   38    8    0   11    4   31    4   22    9]
 [   2    1    0    8    0    5    2    3    5    0    1  114  763    2    6    9    4   52    9    3    6]
 [   6    1    2    0    3   14    2    5   24   25    8   16    5  841    1    5    8    3    4   17   11]
 [  14   10    4   21    9    2    1    4   58   14    6    4    4    6  900    2    6    4   18    1   10]
 [   6    0    2    2    7    0    8    0    2    3    0   34   10    3    1  946   15   11    5    4    7]
 [   6   11    2    0    1   11    4    1    6    0    1   15    8    4    2   23  944    4    2   11   16]
 [   2    0    0    4    1    4    2    1    4    3    0   36   35    5    2   18    5  873    3    2    5]
 [   4    4    5   33    2    8    4   30    9    2   16    6    3    1   21    0    4    1  889    4   12]
 [   2    2    6    0    2    8   11   15    3    0    0   45   11    5    3   11   12    9    5  930    8]
 [ 468  254  281  189  281  318  151  278  224  165  239  410  507  449  318  312  525  220  278  503 7562]]

2024-06-06 00:56:53,338 - ==> Best [Top1: 72.708   Top5: 95.282   Sparsity:0.00   Params: 169472 on epoch: 65]
2024-06-06 00:56:53,338 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:56:53,346 - 

2024-06-06 00:56:53,346 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:56:59,542 - Epoch: [67][  100/ 1218]    Overall Loss 0.666829    Objective Loss 0.666829                                        LR 0.001000    Time 0.061939    
2024-06-06 00:57:04,183 - Epoch: [67][  200/ 1218]    Overall Loss 0.665235    Objective Loss 0.665235                                        LR 0.001000    Time 0.054166    
2024-06-06 00:57:08,844 - Epoch: [67][  300/ 1218]    Overall Loss 0.659883    Objective Loss 0.659883                                        LR 0.001000    Time 0.051640    
2024-06-06 00:57:13,382 - Epoch: [67][  400/ 1218]    Overall Loss 0.656298    Objective Loss 0.656298                                        LR 0.001000    Time 0.050071    
2024-06-06 00:57:18,154 - Epoch: [67][  500/ 1218]    Overall Loss 0.656023    Objective Loss 0.656023                                        LR 0.001000    Time 0.049599    
2024-06-06 00:57:22,792 - Epoch: [67][  600/ 1218]    Overall Loss 0.653902    Objective Loss 0.653902                                        LR 0.001000    Time 0.049059    
2024-06-06 00:57:27,467 - Epoch: [67][  700/ 1218]    Overall Loss 0.653866    Objective Loss 0.653866                                        LR 0.001000    Time 0.048727    
2024-06-06 00:57:32,120 - Epoch: [67][  800/ 1218]    Overall Loss 0.653665    Objective Loss 0.653665                                        LR 0.001000    Time 0.048449    
2024-06-06 00:57:36,892 - Epoch: [67][  900/ 1218]    Overall Loss 0.653568    Objective Loss 0.653568                                        LR 0.001000    Time 0.048366    
2024-06-06 00:57:41,926 - Epoch: [67][ 1000/ 1218]    Overall Loss 0.653214    Objective Loss 0.653214                                        LR 0.001000    Time 0.048562    
2024-06-06 00:57:46,894 - Epoch: [67][ 1100/ 1218]    Overall Loss 0.654214    Objective Loss 0.654214                                        LR 0.001000    Time 0.048662    
2024-06-06 00:57:51,583 - Epoch: [67][ 1200/ 1218]    Overall Loss 0.652796    Objective Loss 0.652796                                        LR 0.001000    Time 0.048512    
2024-06-06 00:57:52,535 - Epoch: [67][ 1218/ 1218]    Overall Loss 0.652825    Objective Loss 0.652825    Top1 72.616137    Top5 93.887531    LR 0.001000    Time 0.048576    
2024-06-06 00:57:52,736 - --- validate (epoch=67)-----------
2024-06-06 00:57:52,736 - 34633 samples (256 per mini-batch)
2024-06-06 00:57:58,293 - Epoch: [67][  100/  136]    Loss 0.588821    Top1 72.488281    Top5 94.613281    
2024-06-06 00:57:59,997 - Epoch: [67][  136/  136]    Loss 0.589308    Top1 72.263448    Top5 94.632287    
2024-06-06 00:58:00,163 - ==> Top1: 72.263    Top5: 94.632    Loss: 0.589

2024-06-06 00:58:00,164 - ==> Confusion:
[[ 796    0    3    1   10    1    0    5    9   82    0    1    1    4    6    3    0    0    0    2    7]
 [   7  910    3    2   21   23    5   12   14    4    6    9    3    0    3    2   10    2   10    6   11]
 [  16    5  765   27    5    3   39   23    1    6   10    4    3    5    5   13   11    1    5    9   14]
 [  10    3   24  816    3    9    6    3    4    3   13    2   14    4   54    5    1    7   14    4   17]
 [  49   15    5    0  892    8    0    3    3   18    2    7    2    6   10   11    5    1    2    5   10]
 [  12   44    1    9   18  790    3   41    3   12    3   30    6   25    2    3    6    2    7   16   10]
 [   2    6   22    3    5    4  982    8    2    2    2    4    0    1    1    8    2    4    2   19    7]
 [   3   24   10    3    2   45   12  856    7    2    6   19    2    6    4    1    3    1   36   24   11]
 [  18   11    0    1    4    0    0    1  785   86   15    4    6   17   24    2    4    2    9    7    6]
 [ 101    0    1    1    5    2    0    1   45  801    1    2    2   16    6    1    0    6    1    2    7]
 [   3    3   12   24    3    3    8    6   30    2  895    3    3   13   19    2    2    1   21    2    9]
 [   3    4    3    1    0    7    4   11    3    2    1  847   30    6    0   13    4   21    3   43    5]
 [   5    2    1    7    0    4    2    5    2    0    2  108  764    1    3   11    8   34   16    8   12]
 [  13    0    3    2    4   12    3    2   21   31    9   24    6  821    4    5    4    6    3   17   11]
 [  20    4    3    9   11    2    1    4   40   22    5    1    7    7  920    1    5    3   17    2   14]
 [   3    0    5    2    8    0   11    0    0    1    0   29    6    4    2  942    7   23    4    7   12]
 [  10    9   11    2   10   12    1    2    5    2    1   13    4    8    2   24  910    2    7   13   24]
 [   4    1    2    2    2    1    3    1    1    4    1   34   35    5    5   18    3  874    2    3    4]
 [   4    9   11   20    2    1    0   30   13    4    9    1    7    2   31    0    3    0  887   13   11]
 [   2    7    1    2    1   15   16   10    3    1    1   31    4    1    0   15    8    5   10  947    8]
 [ 440  288  313  182  340  232  194  217  244  274  202  306  437  387  311  297  448  191  273  529 7827]]

2024-06-06 00:58:00,166 - ==> Best [Top1: 72.708   Top5: 95.282   Sparsity:0.00   Params: 169472 on epoch: 65]
2024-06-06 00:58:00,166 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:58:00,174 - 

2024-06-06 00:58:00,174 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:58:06,448 - Epoch: [68][  100/ 1218]    Overall Loss 0.652697    Objective Loss 0.652697                                        LR 0.001000    Time 0.062724    
2024-06-06 00:58:11,072 - Epoch: [68][  200/ 1218]    Overall Loss 0.649930    Objective Loss 0.649930                                        LR 0.001000    Time 0.054472    
2024-06-06 00:58:15,788 - Epoch: [68][  300/ 1218]    Overall Loss 0.653196    Objective Loss 0.653196                                        LR 0.001000    Time 0.052026    
2024-06-06 00:58:20,595 - Epoch: [68][  400/ 1218]    Overall Loss 0.653576    Objective Loss 0.653576                                        LR 0.001000    Time 0.051034    
2024-06-06 00:58:25,286 - Epoch: [68][  500/ 1218]    Overall Loss 0.655557    Objective Loss 0.655557                                        LR 0.001000    Time 0.050205    
2024-06-06 00:58:29,877 - Epoch: [68][  600/ 1218]    Overall Loss 0.655907    Objective Loss 0.655907                                        LR 0.001000    Time 0.049486    
2024-06-06 00:58:34,604 - Epoch: [68][  700/ 1218]    Overall Loss 0.656641    Objective Loss 0.656641                                        LR 0.001000    Time 0.049167    
2024-06-06 00:58:39,472 - Epoch: [68][  800/ 1218]    Overall Loss 0.657592    Objective Loss 0.657592                                        LR 0.001000    Time 0.049104    
2024-06-06 00:58:44,470 - Epoch: [68][  900/ 1218]    Overall Loss 0.656672    Objective Loss 0.656672                                        LR 0.001000    Time 0.049200    
2024-06-06 00:58:49,408 - Epoch: [68][ 1000/ 1218]    Overall Loss 0.655824    Objective Loss 0.655824                                        LR 0.001000    Time 0.049216    
2024-06-06 00:58:54,017 - Epoch: [68][ 1100/ 1218]    Overall Loss 0.654148    Objective Loss 0.654148                                        LR 0.001000    Time 0.048930    
2024-06-06 00:58:58,631 - Epoch: [68][ 1200/ 1218]    Overall Loss 0.653982    Objective Loss 0.653982                                        LR 0.001000    Time 0.048697    
2024-06-06 00:58:59,428 - Epoch: [68][ 1218/ 1218]    Overall Loss 0.653948    Objective Loss 0.653948    Top1 68.459658    Top5 93.887531    LR 0.001000    Time 0.048631    
2024-06-06 00:58:59,620 - --- validate (epoch=68)-----------
2024-06-06 00:58:59,621 - 34633 samples (256 per mini-batch)
2024-06-06 00:59:05,038 - Epoch: [68][  100/  136]    Loss 0.592372    Top1 71.304688    Top5 94.121094    
2024-06-06 00:59:06,711 - Epoch: [68][  136/  136]    Loss 0.584480    Top1 71.530044    Top5 94.236711    
2024-06-06 00:59:06,889 - ==> Top1: 71.530    Top5: 94.237    Loss: 0.584

2024-06-06 00:59:06,890 - ==> Confusion:
[[ 762    0    4    2   14    1    1    5   11   83    2    5    2    5    7    4    4    4    0    4   11]
 [   2  901    2    3   25   28    9   12    9    1    6    5    5    4   11    3   11    2   10    7    7]
 [  11    4  823   18    4    3   31    8    0    5    8    7    5    4    2    5    7    1    8   10    6]
 [   4    2   20  848    1    4    6    3    4    1   30    7   12    4   30    2    2    7   20    2    7]
 [  29   15    5    3  885   19    3    6    3   14    3    6    2    7   17    6   14    0    4    4    9]
 [   4   52    5   12   25  802    4   32    6    3    6   26    8   21    6    3    6    5    4    7    6]
 [   1    5   31    7    0    5  966    7    1    1    5    8    4    0    1   13    3    4    3   16    5]
 [   4   29   18    5    1   45   15  851    6    1    8   19    7    6    0    0    0    3   36   14    9]
 [   8    8    1    3    0    4    1    2  833   41   23    4    5   21   23    1    2    2    6    4   10]
 [  95    1    7    0    8    2    1    0   79  751    2    3    0   25    7    3    2    2    5    0    8]
 [   1    5   17   23    1    3   10    6   22    3  927    3    1   13   13    0    3    1    8    1    3]
 [   4    0    6    0    0    9    3    4    1    1    2  832   41   11    1   24    8   31    3   19   11]
 [   2    3    0    6    0    6    3    6    2    0    2   88  787    7    4    8    7   41    7    8    8]
 [   2    3    2    2    8   16    1    2   21   16   23   27    4  825    9    4    7   11    1   10    7]
 [  13    1    3   23   11    4    0    1   56   12    7    3    5    8  916    0    2    6   16    2    9]
 [   3    1    8    2    1    0   13    1    2    2    0   34   15    2    1  938   16   11    1    8    7]
 [   5   10   10    3    4    7    2    2    7    1    8   13    9    2    3   13  941    8    2   10   12]
 [   3    0    1    6    1    2    0    3    3    1    3   33   32   10    4   16    4  872    2    1    8]
 [   7   11    9   27    3    5    0   35   12    1    8    5    2    1   34    0    1    1  886    2    8]
 [   2    7    7    2    1   13   28   10    2    0    4   39   10    8    0    6    9    7    0  921   12]
 [ 341  358  375  293  256  293  201  219  235  140  345  320  493  441  372  244  554  211  280  455 7506]]

2024-06-06 00:59:06,891 - ==> Best [Top1: 72.708   Top5: 95.282   Sparsity:0.00   Params: 169472 on epoch: 65]
2024-06-06 00:59:06,891 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 00:59:06,899 - 

2024-06-06 00:59:06,899 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 00:59:13,337 - Epoch: [69][  100/ 1218]    Overall Loss 0.656373    Objective Loss 0.656373                                        LR 0.001000    Time 0.064360    
2024-06-06 00:59:17,869 - Epoch: [69][  200/ 1218]    Overall Loss 0.662469    Objective Loss 0.662469                                        LR 0.001000    Time 0.054831    
2024-06-06 00:59:22,408 - Epoch: [69][  300/ 1218]    Overall Loss 0.655007    Objective Loss 0.655007                                        LR 0.001000    Time 0.051675    
2024-06-06 00:59:27,139 - Epoch: [69][  400/ 1218]    Overall Loss 0.655027    Objective Loss 0.655027                                        LR 0.001000    Time 0.050581    
2024-06-06 00:59:31,893 - Epoch: [69][  500/ 1218]    Overall Loss 0.652799    Objective Loss 0.652799                                        LR 0.001000    Time 0.049969    
2024-06-06 00:59:36,601 - Epoch: [69][  600/ 1218]    Overall Loss 0.651722    Objective Loss 0.651722                                        LR 0.001000    Time 0.049480    
2024-06-06 00:59:41,332 - Epoch: [69][  700/ 1218]    Overall Loss 0.650258    Objective Loss 0.650258                                        LR 0.001000    Time 0.049168    
2024-06-06 00:59:45,865 - Epoch: [69][  800/ 1218]    Overall Loss 0.649537    Objective Loss 0.649537                                        LR 0.001000    Time 0.048686    
2024-06-06 00:59:50,395 - Epoch: [69][  900/ 1218]    Overall Loss 0.648081    Objective Loss 0.648081                                        LR 0.001000    Time 0.048308    
2024-06-06 00:59:54,931 - Epoch: [69][ 1000/ 1218]    Overall Loss 0.646894    Objective Loss 0.646894                                        LR 0.001000    Time 0.048011    
2024-06-06 00:59:59,513 - Epoch: [69][ 1100/ 1218]    Overall Loss 0.646083    Objective Loss 0.646083                                        LR 0.001000    Time 0.047810    
2024-06-06 01:00:04,132 - Epoch: [69][ 1200/ 1218]    Overall Loss 0.645863    Objective Loss 0.645863                                        LR 0.001000    Time 0.047673    
2024-06-06 01:00:04,936 - Epoch: [69][ 1218/ 1218]    Overall Loss 0.645805    Objective Loss 0.645805    Top1 72.616137    Top5 94.865526    LR 0.001000    Time 0.047629    
2024-06-06 01:00:05,127 - --- validate (epoch=69)-----------
2024-06-06 01:00:05,127 - 34633 samples (256 per mini-batch)
2024-06-06 01:00:10,679 - Epoch: [69][  100/  136]    Loss 0.590318    Top1 71.542969    Top5 94.750000    
2024-06-06 01:00:12,412 - Epoch: [69][  136/  136]    Loss 0.590773    Top1 71.452083    Top5 94.739122    
2024-06-06 01:00:12,597 - ==> Top1: 71.452    Top5: 94.739    Loss: 0.591

2024-06-06 01:00:12,598 - ==> Confusion:
[[ 777    0    5    0   12    1    2    1    6   72    1    2    2    6    9    3    6    5    8    0   13]
 [   2  896    5    0   23   32    2   16   10    3    5   10    2    1    8    2   17    4   16    3    6]
 [   9    3  807   15    4    3   24   13    1   10    8    4    7    5    7   15    4    1    8    6   16]
 [   4    2   28  836    7    4    2    5    1    1   16    4   12    4   40    1    2   13   24    1    9]
 [  39   14    1    4  884   13    0    2    4   16    2    7    1    8   12   16   12    2    7    1    9]
 [   4   43    7    8   15  776    4   49    4    9    1   30    6   29    4    7    9    6    9   10   13]
 [   2    6   34    2    1    8  947    9    1    2    4    4    2    2    2   25    7    2    6   13    7]
 [   4   22   26    4   10   43    5  835    1    2    4   12    6    5    4    3    0    4   61   16   10]
 [  17    7    0    3    1    2    1    3  774   88   14    2    7   16   35    2    4    4   11    3    8]
 [ 114    3    1    1    7    6    0    1   42  770    2    0    1   18   11    2    1    7    5    2    7]
 [   6    1   13   21    3    3    6   10   17    5  893    3    2   22   18    1    1    2   24    3   10]
 [   2    1    1    0    4    9    2    9    0    2    0  836   33   13    1   27    4   30    6   26    5]
 [   0    1    3    8    0    3    4    4    5    1    3   86  763    5    3   15    6   53    9    9   14]
 [   3    1    4    3    4   13    4    5   22   27    5   17    8  837    6    7    4    6    4    7   14]
 [  16    2    4   27   18    4    1    0   30   17    6    3    0    8  915    4    0   11   24    1    7]
 [   0    1    3    2    4    0    7    0    0    0    0   20   10    4    2  965   10   20    5    6    7]
 [   6    8   14    4    6    4    3    2    4    2    2   14    4    7    3   25  927    5    4    8   20]
 [   4    3    1    1    0    3    1    3    1    6    0   27   29    4    7   21    1  881    2    3    7]
 [   2    7   14   18    7    4    3   22    7    2    5    4    3    1   24    2    1    4  917    4    7]
 [   2    7    6    2    2   15   21   18    1    0    2   26    7   11    0   14   13    3    4  925    9]
 [ 319  321  425  231  346  208  182  226  131  224  216  283  519  449  361  387  493  230  379  417 7585]]

2024-06-06 01:00:12,600 - ==> Best [Top1: 72.708   Top5: 95.282   Sparsity:0.00   Params: 169472 on epoch: 65]
2024-06-06 01:00:12,600 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:00:12,608 - 

2024-06-06 01:00:12,608 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:00:18,823 - Epoch: [70][  100/ 1218]    Overall Loss 0.634692    Objective Loss 0.634692                                        LR 0.001000    Time 0.062133    
2024-06-06 01:00:23,502 - Epoch: [70][  200/ 1218]    Overall Loss 0.628280    Objective Loss 0.628280                                        LR 0.001000    Time 0.054442    
2024-06-06 01:00:28,321 - Epoch: [70][  300/ 1218]    Overall Loss 0.633308    Objective Loss 0.633308                                        LR 0.001000    Time 0.052350    
2024-06-06 01:00:33,123 - Epoch: [70][  400/ 1218]    Overall Loss 0.633785    Objective Loss 0.633785                                        LR 0.001000    Time 0.051264    
2024-06-06 01:00:37,824 - Epoch: [70][  500/ 1218]    Overall Loss 0.633523    Objective Loss 0.633523                                        LR 0.001000    Time 0.050410    
2024-06-06 01:00:42,415 - Epoch: [70][  600/ 1218]    Overall Loss 0.634893    Objective Loss 0.634893                                        LR 0.001000    Time 0.049658    
2024-06-06 01:00:47,138 - Epoch: [70][  700/ 1218]    Overall Loss 0.637235    Objective Loss 0.637235                                        LR 0.001000    Time 0.049308    
2024-06-06 01:00:51,720 - Epoch: [70][  800/ 1218]    Overall Loss 0.639110    Objective Loss 0.639110                                        LR 0.001000    Time 0.048870    
2024-06-06 01:00:56,258 - Epoch: [70][  900/ 1218]    Overall Loss 0.640820    Objective Loss 0.640820                                        LR 0.001000    Time 0.048480    
2024-06-06 01:01:00,838 - Epoch: [70][ 1000/ 1218]    Overall Loss 0.643454    Objective Loss 0.643454                                        LR 0.001000    Time 0.048210    
2024-06-06 01:01:05,516 - Epoch: [70][ 1100/ 1218]    Overall Loss 0.643750    Objective Loss 0.643750                                        LR 0.001000    Time 0.048079    
2024-06-06 01:01:10,116 - Epoch: [70][ 1200/ 1218]    Overall Loss 0.643318    Objective Loss 0.643318                                        LR 0.001000    Time 0.047904    
2024-06-06 01:01:10,927 - Epoch: [70][ 1218/ 1218]    Overall Loss 0.643336    Objective Loss 0.643336    Top1 72.127139    Top5 96.577017    LR 0.001000    Time 0.047862    
2024-06-06 01:01:11,107 - --- validate (epoch=70)-----------
2024-06-06 01:01:11,107 - 34633 samples (256 per mini-batch)
2024-06-06 01:01:16,872 - Epoch: [70][  100/  136]    Loss 0.585556    Top1 72.449219    Top5 94.464844    
2024-06-06 01:01:18,557 - Epoch: [70][  136/  136]    Loss 0.578333    Top1 72.682124    Top5 94.562989    
2024-06-06 01:01:18,745 - ==> Top1: 72.682    Top5: 94.563    Loss: 0.578

2024-06-06 01:01:18,747 - ==> Confusion:
[[ 783    0    5    0   10    4    1    2    8   69    0    3    1    4   12    4    5    2    7    1   10]
 [   1  903    7    3   13   38    6   20   11    0    3    7    4    3    8    1    9    3   10    7    6]
 [   6    2  826   14    8    5   30    6    0    4   12    4    2    7   10    5    6    1    7    7    8]
 [   4    3   19  846    6   13    8    1    5    1   22    2   15    0   35    0    2    7   12    5   10]
 [  33   21    6    0  875   22    1    5    4   16    2    2    1    9   22    7    5    1    9    1   12]
 [   7   25    3    3    8  838    2   45    7    8    4   20    8   29    7    1    6    0    8    9    5]
 [   3    6   30    5    0    6  983    7    0    1    8    3    3    0    2    9    2    1    3    5    9]
 [   6   11   20    6    3   68    8  827    4    0    6   10    9    5    5    0    0    2   51   33    3]
 [  18    1    2    1    2    4    1    2  783   49   21    5    4   38   37    1    4    6   12    2    9]
 [ 104    1    0    1    7    3    0    1   71  729    1    1    0   39   23    1    3    5    0    2    9]
 [   1    5   12   22    3    7    7    5   17    0  907    0    4   19   13    0    2    0   24    7    9]
 [   2    0    4    1    1   16    3    5    2    1    2  816   36   22    1   14    7   25    2   46    5]
 [   2    0    2    6    0    3    3    0    2    0    2   92  790    5    3    4    3   37   15   19    7]
 [   5    0    5    4    3   16    1    1   17   14    9   11   11  865    9    3    5    3    2    9    8]
 [  17    6    6   16    9    3    1    2   28    6    5    4    5   10  936    0    5   10   17    0   12]
 [   2    1    7    1    5    4    6    1    1    2    0   29   23    3    3  932   10   14    6    6   10]
 [   1    9   11    2   15   11    1    2    6    1    2    7    7    4    3   17  932    2    6   14   19]
 [   1    2    0    3    1    1    2    1    4    0    1   23   41    7    4   13    3  879    5    8    6]
 [   2    9    6   24    4    5    1   23    7    3   14    4    8    1   32    0    1    5  897    4    8]
 [   0    4   11    1    5   19   32    7    0    1    0   20    5   10    0   11    7    3    5  939    8]
 [ 267  334  373  225  304  354  199  260  143  141  285  276  489  439  366  167  443  166  311  504 7886]]

2024-06-06 01:01:18,748 - ==> Best [Top1: 72.708   Top5: 95.282   Sparsity:0.00   Params: 169472 on epoch: 65]
2024-06-06 01:01:18,748 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:01:18,756 - 

2024-06-06 01:01:18,756 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:01:24,656 - Epoch: [71][  100/ 1218]    Overall Loss 0.637666    Objective Loss 0.637666                                        LR 0.001000    Time 0.058979    
2024-06-06 01:01:29,346 - Epoch: [71][  200/ 1218]    Overall Loss 0.646053    Objective Loss 0.646053                                        LR 0.001000    Time 0.052929    
2024-06-06 01:01:34,019 - Epoch: [71][  300/ 1218]    Overall Loss 0.645366    Objective Loss 0.645366                                        LR 0.001000    Time 0.050856    
2024-06-06 01:01:38,695 - Epoch: [71][  400/ 1218]    Overall Loss 0.641745    Objective Loss 0.641745                                        LR 0.001000    Time 0.049826    
2024-06-06 01:01:43,514 - Epoch: [71][  500/ 1218]    Overall Loss 0.643922    Objective Loss 0.643922                                        LR 0.001000    Time 0.049496    
2024-06-06 01:01:48,276 - Epoch: [71][  600/ 1218]    Overall Loss 0.644869    Objective Loss 0.644869                                        LR 0.001000    Time 0.049182    
2024-06-06 01:01:52,946 - Epoch: [71][  700/ 1218]    Overall Loss 0.645541    Objective Loss 0.645541                                        LR 0.001000    Time 0.048824    
2024-06-06 01:01:57,483 - Epoch: [71][  800/ 1218]    Overall Loss 0.645375    Objective Loss 0.645375                                        LR 0.001000    Time 0.048391    
2024-06-06 01:02:02,098 - Epoch: [71][  900/ 1218]    Overall Loss 0.645063    Objective Loss 0.645063                                        LR 0.001000    Time 0.048139    
2024-06-06 01:02:06,865 - Epoch: [71][ 1000/ 1218]    Overall Loss 0.645383    Objective Loss 0.645383                                        LR 0.001000    Time 0.048091    
2024-06-06 01:02:11,680 - Epoch: [71][ 1100/ 1218]    Overall Loss 0.644295    Objective Loss 0.644295                                        LR 0.001000    Time 0.048095    
2024-06-06 01:02:16,364 - Epoch: [71][ 1200/ 1218]    Overall Loss 0.645680    Objective Loss 0.645680                                        LR 0.001000    Time 0.047989    
2024-06-06 01:02:17,218 - Epoch: [71][ 1218/ 1218]    Overall Loss 0.645892    Objective Loss 0.645892    Top1 72.127139    Top5 94.621027    LR 0.001000    Time 0.047981    
2024-06-06 01:02:17,399 - --- validate (epoch=71)-----------
2024-06-06 01:02:17,399 - 34633 samples (256 per mini-batch)
2024-06-06 01:02:22,899 - Epoch: [71][  100/  136]    Loss 0.591468    Top1 71.289062    Top5 94.265625    
2024-06-06 01:02:24,675 - Epoch: [71][  136/  136]    Loss 0.592637    Top1 71.573355    Top5 94.280022    
2024-06-06 01:02:24,873 - ==> Top1: 71.573    Top5: 94.280    Loss: 0.593

2024-06-06 01:02:24,875 - ==> Confusion:
[[ 772    3    4    0   14    3    2    3   10   83    0    2    8    0    7    2    5    1    2    1    9]
 [   3  890    0    1   33   32    3   22    6    3    2    6    4    2    8    6   13    0   12    9    8]
 [  13    3  801   13    6    1   37    7    1   11    1    6   10    4    9    9   11    0    7    9   11]
 [   4    0   30  849    4    6    3    3    4    2   15    3   11    3   31    5    5    8   17    5    8]
 [  35   13    4    1  908    8    0    0    3   17    2    4    5    4   17   12    8    1    6    1    5]
 [   7   27    4    5   23  791    6   45    3   12    1   29   13   30    2    4   10    1    7   17    6]
 [   2    7   44    4    3    2  946    5    2    1    3   10    5    1    0   15    2    4    2   20    8]
 [   7   24   20    5    1   42    5  849    3    0    6   14    3   12    2    1    7    1   39   25   11]
 [  16    3    2    1    4    2    0    1  775   81   10    7   11   25   36    1    5    2    9    2    9]
 [  85    0    7    1   12    5    0    0   46  788    3    0    5   27   10    4    0    1    1    2    4]
 [   7    5   12   21    7    3   10   12   19    5  867    3    5   23   23    1    3    0   25    6    7]
 [   5    3    4    0    3   13    2    5    0    1    0  869   26   17    0   18    3   10    5   22    5]
 [   3    1    1    5    1    7    4    5    0    1    1  115  768    8    3   11    6   33    5    5   12]
 [   6    1    2    1    4   12    0    2   20   18    6   18   11  865    5    2    2    3    5    7   11]
 [   7    2    4   16   12    3    1    2   33   15    5    3    3    8  951    3    7    2   11    1    9]
 [   1    2    4    0    7    0    6    0    0    2    0   33   11    4    0  960    5   14    1    8    8]
 [   8   13    9    4   10   10    2    1    3    3    4   12    8   10    4   23  918    3    2    8   17]
 [   2    1    2    3    2    1    2    2    0    4    0   58   57    7    4   22    0  817    1   10   10]
 [   2    7    8   20    1    4    3   32    9    3    8    6   12    1   33    0    2    0  893    3   11]
 [   2    7    7    0    2    5    8   17    1    1    2   44   13    5    1   13    9    1    3  932   15]
 [ 351  301  379  200  403  209  154  211  167  235  187  414  497  459  399  366  487  147  316  471 7579]]

2024-06-06 01:02:24,876 - ==> Best [Top1: 72.708   Top5: 95.282   Sparsity:0.00   Params: 169472 on epoch: 65]
2024-06-06 01:02:24,876 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:02:24,884 - 

2024-06-06 01:02:24,884 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:02:31,436 - Epoch: [72][  100/ 1218]    Overall Loss 0.655156    Objective Loss 0.655156                                        LR 0.001000    Time 0.065500    
2024-06-06 01:02:36,158 - Epoch: [72][  200/ 1218]    Overall Loss 0.646579    Objective Loss 0.646579                                        LR 0.001000    Time 0.056353    
2024-06-06 01:02:40,742 - Epoch: [72][  300/ 1218]    Overall Loss 0.643978    Objective Loss 0.643978                                        LR 0.001000    Time 0.052842    
2024-06-06 01:02:45,350 - Epoch: [72][  400/ 1218]    Overall Loss 0.648773    Objective Loss 0.648773                                        LR 0.001000    Time 0.051146    
2024-06-06 01:02:49,900 - Epoch: [72][  500/ 1218]    Overall Loss 0.645271    Objective Loss 0.645271                                        LR 0.001000    Time 0.050014    
2024-06-06 01:02:54,601 - Epoch: [72][  600/ 1218]    Overall Loss 0.649073    Objective Loss 0.649073                                        LR 0.001000    Time 0.049510    
2024-06-06 01:02:59,279 - Epoch: [72][  700/ 1218]    Overall Loss 0.649615    Objective Loss 0.649615                                        LR 0.001000    Time 0.049117    
2024-06-06 01:03:03,809 - Epoch: [72][  800/ 1218]    Overall Loss 0.648579    Objective Loss 0.648579                                        LR 0.001000    Time 0.048638    
2024-06-06 01:03:08,423 - Epoch: [72][  900/ 1218]    Overall Loss 0.649588    Objective Loss 0.649588                                        LR 0.001000    Time 0.048359    
2024-06-06 01:03:13,008 - Epoch: [72][ 1000/ 1218]    Overall Loss 0.649646    Objective Loss 0.649646                                        LR 0.001000    Time 0.048106    
2024-06-06 01:03:17,714 - Epoch: [72][ 1100/ 1218]    Overall Loss 0.648923    Objective Loss 0.648923                                        LR 0.001000    Time 0.048010    
2024-06-06 01:03:22,266 - Epoch: [72][ 1200/ 1218]    Overall Loss 0.650264    Objective Loss 0.650264                                        LR 0.001000    Time 0.047801    
2024-06-06 01:03:23,058 - Epoch: [72][ 1218/ 1218]    Overall Loss 0.649866    Objective Loss 0.649866    Top1 74.816626    Top5 95.599022    LR 0.001000    Time 0.047744    
2024-06-06 01:03:23,249 - --- validate (epoch=72)-----------
2024-06-06 01:03:23,249 - 34633 samples (256 per mini-batch)
2024-06-06 01:03:28,951 - Epoch: [72][  100/  136]    Loss 0.580251    Top1 72.503906    Top5 95.109375    
2024-06-06 01:03:30,626 - Epoch: [72][  136/  136]    Loss 0.590704    Top1 72.526203    Top5 95.111599    
2024-06-06 01:03:30,821 - ==> Top1: 72.526    Top5: 95.112    Loss: 0.591

2024-06-06 01:03:30,822 - ==> Confusion:
[[ 721    2    4    2   18    2    0    3    9  129    2    2    1    3    8    0    5    2    3    1   14]
 [  11  930    0    2   19   14    5   20    9    5    3    9    2    2    6    2    2    1    9    2   10]
 [   7    6  785   15   10    2   43   10    4   13   10    7    4    5    3    7    9    1   10    4   15]
 [   4    4   26  803    7    7    8    5    3    3   18    3    7    5   52    2    4   10   34    0   11]
 [  38   24    3    1  893   15    3    0    2   28    2    2    0    5   13    1    9    0    4    1   10]
 [   5   62    4    3   24  762    6   47    1   15    1   21   13   28    9    1    5    2   13   11   10]
 [   7   10   29    4    2    7  954    8    1    2    6    2    5    3    0   11    6    5    5    7   12]
 [   7   33   12    2    7   38    6  839    2    4    7   17    9    1    2    3    4    1   58    8   17]
 [  12    7    0    0    2    2    1    1  774   97   15    3    6   17   39    1    1    0   12    1   11]
 [  61    2    2    0    4    1    2    1   30  853    2    0    0   16    9    4    0    1    2    1   10]
 [   4    8    8   10    0    4    7    3   20    8  927    1    0   12   15    0    0    1   23    1   12]
 [   7    7    2    0    5   12    4   12    2    1    2  846   21   13    1    9    8   24    6   18   11]
 [   8    4    1    7    0    4    2    6    5    1    2  117  723    5    6    8    8   59    7    8   14]
 [   7    2    4    0    5   18    0    2   18   36   10   22    1  823   11    3    6    5    1    6   21]
 [  16    9    1   15   17    5    1    0   26   19    5    2    3    6  943    0    3    3   15    0    9]
 [   5    3    6    1    4    2   10    0    1    3    1   29    9    7    0  927   23   20    1    2   12]
 [   4   29    6    2   16   10    2    2    9    4    3   10    1    6    3   10  937    2    1    4   11]
 [   3    0    1    2    2    2    1    1    4    8    0   27   29    9    6   20    1  872    4    4    9]
 [   4   17    7   15    9    7    2   21    8    1    7    1    2    3   29    0    4    2  899    5   15]
 [   5   15    4    2    4   11   17   20    0    3    3   30    7    9    0   10   11    4    4  909   20]
 [ 303  490  275  177  368  214  160  218  156  286  228  341  404  370  388  188  517  173  329  350 7997]]

2024-06-06 01:03:30,823 - ==> Best [Top1: 72.708   Top5: 95.282   Sparsity:0.00   Params: 169472 on epoch: 65]
2024-06-06 01:03:30,823 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:03:30,831 - 

2024-06-06 01:03:30,831 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:03:36,880 - Epoch: [73][  100/ 1218]    Overall Loss 0.648002    Objective Loss 0.648002                                        LR 0.001000    Time 0.060473    
2024-06-06 01:03:41,575 - Epoch: [73][  200/ 1218]    Overall Loss 0.646150    Objective Loss 0.646150                                        LR 0.001000    Time 0.053702    
2024-06-06 01:03:46,095 - Epoch: [73][  300/ 1218]    Overall Loss 0.648525    Objective Loss 0.648525                                        LR 0.001000    Time 0.050863    
2024-06-06 01:03:50,707 - Epoch: [73][  400/ 1218]    Overall Loss 0.651060    Objective Loss 0.651060                                        LR 0.001000    Time 0.049672    
2024-06-06 01:03:55,340 - Epoch: [73][  500/ 1218]    Overall Loss 0.652099    Objective Loss 0.652099                                        LR 0.001000    Time 0.049001    
2024-06-06 01:04:00,311 - Epoch: [73][  600/ 1218]    Overall Loss 0.650385    Objective Loss 0.650385                                        LR 0.001000    Time 0.049116    
2024-06-06 01:04:04,977 - Epoch: [73][  700/ 1218]    Overall Loss 0.650207    Objective Loss 0.650207                                        LR 0.001000    Time 0.048763    
2024-06-06 01:04:09,768 - Epoch: [73][  800/ 1218]    Overall Loss 0.650457    Objective Loss 0.650457                                        LR 0.001000    Time 0.048654    
2024-06-06 01:04:14,562 - Epoch: [73][  900/ 1218]    Overall Loss 0.650035    Objective Loss 0.650035                                        LR 0.001000    Time 0.048573    
2024-06-06 01:04:19,369 - Epoch: [73][ 1000/ 1218]    Overall Loss 0.648871    Objective Loss 0.648871                                        LR 0.001000    Time 0.048521    
2024-06-06 01:04:23,946 - Epoch: [73][ 1100/ 1218]    Overall Loss 0.648002    Objective Loss 0.648002                                        LR 0.001000    Time 0.048269    
2024-06-06 01:04:28,729 - Epoch: [73][ 1200/ 1218]    Overall Loss 0.646847    Objective Loss 0.646847                                        LR 0.001000    Time 0.048231    
2024-06-06 01:04:29,500 - Epoch: [73][ 1218/ 1218]    Overall Loss 0.647498    Objective Loss 0.647498    Top1 70.904645    Top5 93.398533    LR 0.001000    Time 0.048151    
2024-06-06 01:04:29,661 - --- validate (epoch=73)-----------
2024-06-06 01:04:29,661 - 34633 samples (256 per mini-batch)
2024-06-06 01:04:35,304 - Epoch: [73][  100/  136]    Loss 0.591505    Top1 72.355469    Top5 94.718750    
2024-06-06 01:04:37,024 - Epoch: [73][  136/  136]    Loss 0.590942    Top1 72.468455    Top5 94.713135    
2024-06-06 01:04:37,216 - ==> Top1: 72.468    Top5: 94.713    Loss: 0.591

2024-06-06 01:04:37,217 - ==> Confusion:
[[ 764    1    6    0   21    2    2    1   15   60    3    2    4    5   13    3    6    5    1    2   15]
 [   4  867    5    2   16   39   11   20    6    2   10    7    5    4   10    2   14    5   19    4   11]
 [   9    2  825   10    2    2   30    9    3    6    5    4    7    6    3   10    7    2    5    5   18]
 [   1    3   31  835    5    6    6    2    5    1   13    2   23    4   28    1    6    8   24    1   11]
 [  39    9    5    3  869   18    2    1    7   12    1    4    1    6   22   10   15    3   12    1   14]
 [   0   35    9   10   16  792   10   37    5    6    4   15    8   30    4    5   13    7   11    9   17]
 [   3    4   45    3    1    2  966    5    2    0    5    3    5    3    0    6    1    5    4   15    8]
 [   5   13   25    4    2   49    8  840    1    1    5   10    7    4    4    1    3    6   54   22   13]
 [  11    3    1    1    3    5    2    3  813   48   18    0    7   29   29    2    4    6    7    1    9]
 [ 112    2    2    1   11    4    1    3   71  720    3    1    0   27   21    3    2    4    3    0   10]
 [   2    1   20   18    2    7   13    8   19    2  897    2    4   15   10    0    2    1   27    4   10]
 [   2    1    3    1    0    9    6    7    2    1    0  810   60    9    1   27    7   37    2   16   10]
 [   4    1    2    6    0    3    1    2    6    1    3   64  814    3    8    6    6   51    3    4    7]
 [   0    1    5    0    9   19    2    4   27   21   12   20   13  821    8    9    7    5    2    5   11]
 [   9    1    7   20    7    0    0    2   35    8   10    0    8    5  940    2    5    5   29    0    5]
 [   3    1    7    2    3    2    7    1    1    0    1   15   17    1    2  951   20   18    3    3    8]
 [   2   11    5    3    8    6    2    2   10    1    2    8    7    7    3   15  945    6    7    4   18]
 [   2    0    2    3    2    2    1    1    4    0    0   20   54    5    3   20    2  875    3    2    4]
 [   2    7   12   23    3    4    0   25    9    1    4    4   13    2   24    0    1    1  907    4   12]
 [   1    4    5    0    2    9   23    7    2    0    2   26   20   12    0    7   13    5    4  929   17]
 [ 254  277  330  241  275  225  210  231  207  111  252  249  610  425  367  312  506  228  353  351 7918]]

2024-06-06 01:04:37,219 - ==> Best [Top1: 72.708   Top5: 95.282   Sparsity:0.00   Params: 169472 on epoch: 65]
2024-06-06 01:04:37,219 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:04:37,228 - 

2024-06-06 01:04:37,228 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:04:43,364 - Epoch: [74][  100/ 1218]    Overall Loss 0.633874    Objective Loss 0.633874                                        LR 0.001000    Time 0.061336    
2024-06-06 01:04:48,156 - Epoch: [74][  200/ 1218]    Overall Loss 0.630966    Objective Loss 0.630966                                        LR 0.001000    Time 0.054621    
2024-06-06 01:04:52,775 - Epoch: [74][  300/ 1218]    Overall Loss 0.633267    Objective Loss 0.633267                                        LR 0.001000    Time 0.051803    
2024-06-06 01:04:57,571 - Epoch: [74][  400/ 1218]    Overall Loss 0.634316    Objective Loss 0.634316                                        LR 0.001000    Time 0.050837    
2024-06-06 01:05:02,139 - Epoch: [74][  500/ 1218]    Overall Loss 0.636644    Objective Loss 0.636644                                        LR 0.001000    Time 0.049803    
2024-06-06 01:05:06,763 - Epoch: [74][  600/ 1218]    Overall Loss 0.639571    Objective Loss 0.639571                                        LR 0.001000    Time 0.049207    
2024-06-06 01:05:11,410 - Epoch: [74][  700/ 1218]    Overall Loss 0.638559    Objective Loss 0.638559                                        LR 0.001000    Time 0.048814    
2024-06-06 01:05:16,037 - Epoch: [74][  800/ 1218]    Overall Loss 0.637187    Objective Loss 0.637187                                        LR 0.001000    Time 0.048493    
2024-06-06 01:05:20,757 - Epoch: [74][  900/ 1218]    Overall Loss 0.640093    Objective Loss 0.640093                                        LR 0.001000    Time 0.048347    
2024-06-06 01:05:25,338 - Epoch: [74][ 1000/ 1218]    Overall Loss 0.640628    Objective Loss 0.640628                                        LR 0.001000    Time 0.048092    
2024-06-06 01:05:29,890 - Epoch: [74][ 1100/ 1218]    Overall Loss 0.641890    Objective Loss 0.641890                                        LR 0.001000    Time 0.047857    
2024-06-06 01:05:34,574 - Epoch: [74][ 1200/ 1218]    Overall Loss 0.641646    Objective Loss 0.641646                                        LR 0.001000    Time 0.047770    
2024-06-06 01:05:35,471 - Epoch: [74][ 1218/ 1218]    Overall Loss 0.641480    Objective Loss 0.641480    Top1 73.349633    Top5 94.376528    LR 0.001000    Time 0.047801    
2024-06-06 01:05:35,655 - --- validate (epoch=74)-----------
2024-06-06 01:05:35,655 - 34633 samples (256 per mini-batch)
2024-06-06 01:05:41,375 - Epoch: [74][  100/  136]    Loss 0.590824    Top1 71.824219    Top5 94.566406    
2024-06-06 01:05:43,099 - Epoch: [74][  136/  136]    Loss 0.588621    Top1 71.948719    Top5 94.623625    
2024-06-06 01:05:43,283 - ==> Top1: 71.949    Top5: 94.624    Loss: 0.589

2024-06-06 01:05:43,284 - ==> Confusion:
[[ 794    2    4    3   22    0    0    2    6   57    1    3    3    4    7    2    2    5    2    2   10]
 [   8  898    2    1   38   37    3   10    9    1    2    9    5    3    5    2    9    0    4    8    9]
 [  12    6  804   13    8    2   30    9    4    4    3    3   10    8    5   17    7    1    5    8   11]
 [   4    1   32  810    5    7    2    1    6    2   19    4   16    3   45    6    3   11   25    5    9]
 [  43    8    3    0  925    9    1    0    3    9    0    5    2    2    6    7    9    3    3    3   13]
 [   6   48    3    5   26  792    5   25    7    7    2   25   11   30    3    5   10    4    6   12   11]
 [   6   13   32    3    3    5  952    4    0    0    3    6    1    4    0   16    3    4    5   18    8]
 [   7   33   15    1    9   55    7  803    6    3    5    9    8    5    1    3    2    6   62   24   13]
 [  22   11    0    2    3    1    0    1  791   74   16    1    5   21   28    0    3    3    9    2    9]
 [  99    1    0    0   16    5    2    0   37  789    1    2    2   19   11    1    0    3    3    3    7]
 [   3    8   11   13    5    7    7    4   19    1  901    1    3   13   24    0    5    1   25    0   13]
 [   3    5    3    0    1   15    4    3    1    0    3  845   26   19    0    9    9   26    4   22   13]
 [   2    1    3    4    0    6    0    2    6    0    0  111  766    6    2    6   10   41    6   10   13]
 [   9    4    3    2   11   13    0    3   27   30    8   15    7  820    9    8    4    6    1    8   13]
 [  15    8    5   14   18    0    0    0   39   12    1    2    3    2  931    2    4    9   18    1   14]
 [   3    3    6    2    9    1    8    0    1    0    0   19   13    3    2  952    6   21    1    6   10]
 [   7    9    3    3   13   12    5    0    7    2    1   12    7    3    4   11  934    6    9    9   15]
 [   2    1    1    2    1    0    1    0    2    2    1   39   34    4    5   16    4  876    2    3    9]
 [   8   17    8   23   11    7    1   17    9    0    8    7    6    1   29    0    3    2  888    3   10]
 [   2    9    4    2    6   14   23   11    1    0    3   30   15    9    1    9    9    3    4  919   14]
 [ 351  424  300  167  524  273  144  160  220  174  224  305  445  398  401  258  459  218  321  439 7727]]

2024-06-06 01:05:43,286 - ==> Best [Top1: 72.708   Top5: 95.282   Sparsity:0.00   Params: 169472 on epoch: 65]
2024-06-06 01:05:43,286 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:05:43,294 - 

2024-06-06 01:05:43,294 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:05:49,428 - Epoch: [75][  100/ 1218]    Overall Loss 0.631485    Objective Loss 0.631485                                        LR 0.001000    Time 0.061320    
2024-06-06 01:05:54,214 - Epoch: [75][  200/ 1218]    Overall Loss 0.646213    Objective Loss 0.646213                                        LR 0.001000    Time 0.054581    
2024-06-06 01:05:58,975 - Epoch: [75][  300/ 1218]    Overall Loss 0.648401    Objective Loss 0.648401                                        LR 0.001000    Time 0.052252    
2024-06-06 01:06:03,733 - Epoch: [75][  400/ 1218]    Overall Loss 0.648366    Objective Loss 0.648366                                        LR 0.001000    Time 0.051079    
2024-06-06 01:06:08,386 - Epoch: [75][  500/ 1218]    Overall Loss 0.648025    Objective Loss 0.648025                                        LR 0.001000    Time 0.050167    
2024-06-06 01:06:13,045 - Epoch: [75][  600/ 1218]    Overall Loss 0.648119    Objective Loss 0.648119                                        LR 0.001000    Time 0.049568    
2024-06-06 01:06:17,783 - Epoch: [75][  700/ 1218]    Overall Loss 0.648318    Objective Loss 0.648318                                        LR 0.001000    Time 0.049252    
2024-06-06 01:06:22,487 - Epoch: [75][  800/ 1218]    Overall Loss 0.647793    Objective Loss 0.647793                                        LR 0.001000    Time 0.048973    
2024-06-06 01:06:27,059 - Epoch: [75][  900/ 1218]    Overall Loss 0.646468    Objective Loss 0.646468                                        LR 0.001000    Time 0.048609    
2024-06-06 01:06:31,634 - Epoch: [75][ 1000/ 1218]    Overall Loss 0.648448    Objective Loss 0.648448                                        LR 0.001000    Time 0.048322    
2024-06-06 01:06:36,229 - Epoch: [75][ 1100/ 1218]    Overall Loss 0.647851    Objective Loss 0.647851                                        LR 0.001000    Time 0.048105    
2024-06-06 01:06:40,851 - Epoch: [75][ 1200/ 1218]    Overall Loss 0.646143    Objective Loss 0.646143                                        LR 0.001000    Time 0.047947    
2024-06-06 01:06:41,681 - Epoch: [75][ 1218/ 1218]    Overall Loss 0.645960    Objective Loss 0.645960    Top1 71.638142    Top5 94.376528    LR 0.001000    Time 0.047919    
2024-06-06 01:06:41,867 - --- validate (epoch=75)-----------
2024-06-06 01:06:41,868 - 34633 samples (256 per mini-batch)
2024-06-06 01:06:47,758 - Epoch: [75][  100/  136]    Loss 0.587406    Top1 72.050781    Top5 94.773438    
2024-06-06 01:06:49,421 - Epoch: [75][  136/  136]    Loss 0.582643    Top1 72.156614    Top5 94.759334    
2024-06-06 01:06:49,615 - ==> Top1: 72.157    Top5: 94.759    Loss: 0.583

2024-06-06 01:06:49,616 - ==> Confusion:
[[ 801    0    6    3    7    2    0    3    4   64    0    5    3    3    7    6    2    3    2    0   10]
 [   5  886    1    1   15   40    9   35    5    3    4    8    5    1    7    1    6    0   14    5   12]
 [   9    6  790   21    3    6   37   21    1    6   10    4    8    7    4    7    5    3    4    7   11]
 [   6    2   23  852    2   10    4    4    0    3   20    3   18    4   31    1    0    8   20    0    5]
 [  36   14    6    0  901   26    4    3    4   13    0    6    0    4    9    3    5    6    4    7    3]
 [   5   32    3    5   14  840    5   50    3    4    5   15    9   18    4    1    4    2    5   11    8]
 [   3    7   26    6    1    9  964   10    1    1    3    4    8    3    0    7    1    7    4   12    9]
 [   4   16   11    7    0   44    7  889    3    4    5   16   11    3    2    1    2    7   28   14    3]
 [  21    7    2    4    0    7    0    4  773   59   16    0    8   26   42    0    1    3   15    1   13]
 [ 134    2    2    0    9    5    2    2   51  734    1    2    4   23   11    0    1    5    6    0    7]
 [   5    3    8   21    5    2    6    8   24    3  910    1    5   21   17    1    0    2   18    2    2]
 [   0    2    1    0    1   22    2    9    0    2    2  812   63   12    1   13    2   39    1   17   10]
 [   1    0    1    8    0   11    1    5    1    0    2   68  802    4    3    8    2   53   14    8    3]
 [   6    3    2    2    6   28    0    2   29   22    7   19    4  835    7    1    4   10    0    5    9]
 [  15    5    1   18    9   13    0    1   26   12    8    1    7    4  943    2    3    5   17    2    6]
 [   3    3    8    3    4    2    9    4    1    1    1   34   19    7    2  915    9   29    1    3    8]
 [   7   17    2    3    8   15    3    2    4    2    1   13   13   11    3   11  918    7    3   10   19]
 [   1    1    2    1    0    4    2    2    1    1    0   10   39    5    5    9    1  902    5    5    9]
 [   2   12   10   23    1    4    0   43    7    1    5    1   17    1   20    1    0    4  894    5    7]
 [   2    7    2    1    2   17   18   17    0    1    0   46   28    9    2    2    2    7    8  908    9]
 [ 370  341  266  238  260  411  175  322  197  156  265  317  562  435  417  211  301  275  290  402 7721]]

2024-06-06 01:06:49,617 - ==> Best [Top1: 72.708   Top5: 95.282   Sparsity:0.00   Params: 169472 on epoch: 65]
2024-06-06 01:06:49,617 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:06:49,625 - 

2024-06-06 01:06:49,625 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:06:55,786 - Epoch: [76][  100/ 1218]    Overall Loss 0.645866    Objective Loss 0.645866                                        LR 0.001000    Time 0.061583    
2024-06-06 01:07:00,444 - Epoch: [76][  200/ 1218]    Overall Loss 0.643202    Objective Loss 0.643202                                        LR 0.001000    Time 0.054074    
2024-06-06 01:07:05,050 - Epoch: [76][  300/ 1218]    Overall Loss 0.647910    Objective Loss 0.647910                                        LR 0.001000    Time 0.051398    
2024-06-06 01:07:09,607 - Epoch: [76][  400/ 1218]    Overall Loss 0.645777    Objective Loss 0.645777                                        LR 0.001000    Time 0.049936    
2024-06-06 01:07:14,328 - Epoch: [76][  500/ 1218]    Overall Loss 0.643715    Objective Loss 0.643715                                        LR 0.001000    Time 0.049388    
2024-06-06 01:07:18,943 - Epoch: [76][  600/ 1218]    Overall Loss 0.643946    Objective Loss 0.643946                                        LR 0.001000    Time 0.048845    
2024-06-06 01:07:23,689 - Epoch: [76][  700/ 1218]    Overall Loss 0.642149    Objective Loss 0.642149                                        LR 0.001000    Time 0.048645    
2024-06-06 01:07:28,392 - Epoch: [76][  800/ 1218]    Overall Loss 0.644186    Objective Loss 0.644186                                        LR 0.001000    Time 0.048440    
2024-06-06 01:07:32,941 - Epoch: [76][  900/ 1218]    Overall Loss 0.643430    Objective Loss 0.643430                                        LR 0.001000    Time 0.048111    
2024-06-06 01:07:37,482 - Epoch: [76][ 1000/ 1218]    Overall Loss 0.642992    Objective Loss 0.642992                                        LR 0.001000    Time 0.047840    
2024-06-06 01:07:42,031 - Epoch: [76][ 1100/ 1218]    Overall Loss 0.643087    Objective Loss 0.643087                                        LR 0.001000    Time 0.047624    
2024-06-06 01:07:46,591 - Epoch: [76][ 1200/ 1218]    Overall Loss 0.642579    Objective Loss 0.642579                                        LR 0.001000    Time 0.047454    
2024-06-06 01:07:47,426 - Epoch: [76][ 1218/ 1218]    Overall Loss 0.642568    Objective Loss 0.642568    Top1 70.904645    Top5 96.332518    LR 0.001000    Time 0.047437    
2024-06-06 01:07:47,606 - --- validate (epoch=76)-----------
2024-06-06 01:07:47,606 - 34633 samples (256 per mini-batch)
2024-06-06 01:07:53,072 - Epoch: [76][  100/  136]    Loss 0.565992    Top1 73.757812    Top5 95.445312    
2024-06-06 01:07:54,777 - Epoch: [76][  136/  136]    Loss 0.568716    Top1 73.721595    Top5 95.452314    
2024-06-06 01:07:54,961 - ==> Top1: 73.722    Top5: 95.452    Loss: 0.569

2024-06-06 01:07:54,963 - ==> Confusion:
[[ 769    2    6    1   13    2    1    2   15   73    2    1    1    1    6    6    1    3    7    1   18]
 [   3  888    1    0   27   33    9   26    8    3    3    6    2    1    3    0   11    3   18    8   10]
 [   6    3  811   15    5    1   37   13    0    5    6    3    3    5    4    7   13    0   13    8   12]
 [   3    3   26  862    5    3    4    5    1    4   16    2    8    2   28    1    2    5   21    3   12]
 [  40   12    5    4  892   17    2    4    5   13    1    3    0    3   13    9   11    1    4    1   14]
 [   8   43    2   11   18  798    4   46    1    7    5   24   10   14    3    3   10    4    8   13   11]
 [   3    3   30    2    0    8  971    9    3    2    7    2    0    0    0   11    6    3    2   16    8]
 [   3   23   14    5    4   44   10  859    5    1    6   14    2    1    2    2    3    3   41   25   10]
 [  12    5    0    5    1    2    0    4  810   57   14    3    9   12   30    1    3    2   14    1   17]
 [ 105    2    4    0    8    7    0    2   76  749    3    2    0   15    8    2    1    5    3    4    5]
 [   3    2    9   23    3    4   15    8   25    1  906    1    1   15   11    0    1    0   29    0    7]
 [   3    3    4    2    1   10    6    6    2    0    1  808   53    6    2   24    4   31    3   34    8]
 [   2    0    0    7    0    4    2    5    2    0    1   72  785    1    7   15    4   50   14    5   19]
 [   7    1    5    2    7   26    1    5   36   24   16   10    6  792    9   11    5    8    5   14   11]
 [  16    6    3   27    6    0    1    1   31    9    4    2    5    2  946    1    2    1   24    0   11]
 [   3    3    7    4    2    2    5    2    0    0    0   21    9    0    0  956   12   22    6    5    7]
 [   4    8   10    4   10    5    4    3    6    1    2   10    8    2    0   15  932    3    6   14   25]
 [   3    3    0    6    0    2    4    2    2    2    0   26   37    6    1   17    1  870    9    2   12]
 [   3   10    9   25    4    4    3   24    7    1   10    0    8    2   14    0    2    0  914    6   12]
 [   2   10    3    1    3   12   24    7    1    0    0   26   11    1    1    9    7    3   14  945    8]
 [ 295  336  305  205  307  232  189  268  220  147  211  231  455  254  326  245  422  170  394  451 8269]]

2024-06-06 01:07:54,964 - ==> Best [Top1: 73.722   Top5: 95.452   Sparsity:0.00   Params: 169472 on epoch: 76]
2024-06-06 01:07:54,964 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:07:54,979 - 

2024-06-06 01:07:54,979 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:08:01,349 - Epoch: [77][  100/ 1218]    Overall Loss 0.639865    Objective Loss 0.639865                                        LR 0.001000    Time 0.063678    
2024-06-06 01:08:06,010 - Epoch: [77][  200/ 1218]    Overall Loss 0.640630    Objective Loss 0.640630                                        LR 0.001000    Time 0.055134    
2024-06-06 01:08:10,977 - Epoch: [77][  300/ 1218]    Overall Loss 0.642471    Objective Loss 0.642471                                        LR 0.001000    Time 0.053308    
2024-06-06 01:08:16,073 - Epoch: [77][  400/ 1218]    Overall Loss 0.641818    Objective Loss 0.641818                                        LR 0.001000    Time 0.052715    
2024-06-06 01:08:20,672 - Epoch: [77][  500/ 1218]    Overall Loss 0.640534    Objective Loss 0.640534                                        LR 0.001000    Time 0.051368    
2024-06-06 01:08:25,375 - Epoch: [77][  600/ 1218]    Overall Loss 0.642747    Objective Loss 0.642747                                        LR 0.001000    Time 0.050642    
2024-06-06 01:08:29,923 - Epoch: [77][  700/ 1218]    Overall Loss 0.643118    Objective Loss 0.643118                                        LR 0.001000    Time 0.049901    
2024-06-06 01:08:34,517 - Epoch: [77][  800/ 1218]    Overall Loss 0.641422    Objective Loss 0.641422                                        LR 0.001000    Time 0.049404    
2024-06-06 01:08:39,299 - Epoch: [77][  900/ 1218]    Overall Loss 0.644519    Objective Loss 0.644519                                        LR 0.001000    Time 0.049226    
2024-06-06 01:08:43,825 - Epoch: [77][ 1000/ 1218]    Overall Loss 0.644024    Objective Loss 0.644024                                        LR 0.001000    Time 0.048825    
2024-06-06 01:08:48,515 - Epoch: [77][ 1100/ 1218]    Overall Loss 0.644661    Objective Loss 0.644661                                        LR 0.001000    Time 0.048649    
2024-06-06 01:08:53,275 - Epoch: [77][ 1200/ 1218]    Overall Loss 0.644851    Objective Loss 0.644851                                        LR 0.001000    Time 0.048560    
2024-06-06 01:08:54,180 - Epoch: [77][ 1218/ 1218]    Overall Loss 0.644781    Objective Loss 0.644781    Top1 71.638142    Top5 95.843521    LR 0.001000    Time 0.048585    
2024-06-06 01:08:54,369 - --- validate (epoch=77)-----------
2024-06-06 01:08:54,370 - 34633 samples (256 per mini-batch)
2024-06-06 01:09:00,058 - Epoch: [77][  100/  136]    Loss 0.572480    Top1 73.128906    Top5 95.421875    
2024-06-06 01:09:01,735 - Epoch: [77][  136/  136]    Loss 0.579011    Top1 72.890018    Top5 95.316606    
2024-06-06 01:09:01,913 - ==> Top1: 72.890    Top5: 95.317    Loss: 0.579

2024-06-06 01:09:01,914 - ==> Confusion:
[[ 802    1    3    0   13    2    0    2   11   52    1    1    1    5   14    2    3    1    2    1   14]
 [   4  898    5    3   40   26    5   10   10    2    5    7    0    0    7    1   11    2    8    8   11]
 [  13    1  813   16    5    2   25   21    1    6   10    2    3    7   11    6    5    3    4    6   10]
 [   5    2   21  860    6    9    1    2    2    1   14    0    7    2   55    3    1    2    6    3   14]
 [  42   13    4    3  917    9    0    1    2   15    0    2    1    1   19    3    7    2    3    0   10]
 [  10   43    4    9   32  798    6   21    6    5    3   19    7   32    7    3    9    1    5   15    8]
 [   9   11   39    4    3    7  952    6    1    2    1    2    0    2    3   11    4    5    3   11   10]
 [   9   34   18    4    5   67    6  821    4    1    9   13    4    9    4    1    1    0   37   19   11]
 [  21    4    0    0    1    2    0    1  832   46   10    4    1   28   26    1    6    1    6    2   10]
 [ 124    1    0    1    9    4    0    2   74  728    0    2    0   27   14    1    3    1    0    1    9]
 [   4    5   17   17    3    4    7    8   22    2  891    2    2   27   19    0    2    0   20    1   11]
 [   5    3    1    0    5   21    3    3    1    0    2  815   20   16    3   29    9   26    4   33   12]
 [   1    3    5   17    1    6    2    7    4    0    2   95  755    9    4    9   13   34   10    4   14]
 [  10    4    4    2    9   14    1    3   24   17    5   10    2  858   11    6    3    6    0    3    9]
 [  19    5    4    8   12    3    0    3   44    8    6    1    4    8  945    0    2    7    6    1   12]
 [   2    2    6    2    6    2    3    0    0    1    0   10    7    4    1  976   17    7    0    6   14]
 [  10   11   10    3   10   11    5    1    8    1    3    6    2    5    4   11  940    3    2    7   19]
 [   6    0    1    7    2    1    2    1    2    3    0   30   37   10   10   33    3  838    2    3   14]
 [   9   12   16   24    0   10    3   27   16    1    7    4    4    2   41    0    3    1  864    4   10]
 [   6   10    3    2    2   22   20   11    0    1    2   23    9    3    1   14   19    6    4  918   12]
 [ 472  299  372  240  429  282  122  159  227  157  195  221  393  462  458  200  468  154  219  380 8023]]

2024-06-06 01:09:01,916 - ==> Best [Top1: 73.722   Top5: 95.452   Sparsity:0.00   Params: 169472 on epoch: 76]
2024-06-06 01:09:01,916 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:09:01,925 - 

2024-06-06 01:09:01,925 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:09:08,370 - Epoch: [78][  100/ 1218]    Overall Loss 0.637044    Objective Loss 0.637044                                        LR 0.001000    Time 0.064430    
2024-06-06 01:09:12,886 - Epoch: [78][  200/ 1218]    Overall Loss 0.641508    Objective Loss 0.641508                                        LR 0.001000    Time 0.054787    
2024-06-06 01:09:17,640 - Epoch: [78][  300/ 1218]    Overall Loss 0.639947    Objective Loss 0.639947                                        LR 0.001000    Time 0.052363    
2024-06-06 01:09:22,452 - Epoch: [78][  400/ 1218]    Overall Loss 0.644460    Objective Loss 0.644460                                        LR 0.001000    Time 0.051300    
2024-06-06 01:09:27,118 - Epoch: [78][  500/ 1218]    Overall Loss 0.644576    Objective Loss 0.644576                                        LR 0.001000    Time 0.050369    
2024-06-06 01:09:31,856 - Epoch: [78][  600/ 1218]    Overall Loss 0.645202    Objective Loss 0.645202                                        LR 0.001000    Time 0.049867    
2024-06-06 01:09:36,511 - Epoch: [78][  700/ 1218]    Overall Loss 0.644390    Objective Loss 0.644390                                        LR 0.001000    Time 0.049391    
2024-06-06 01:09:41,112 - Epoch: [78][  800/ 1218]    Overall Loss 0.645083    Objective Loss 0.645083                                        LR 0.001000    Time 0.048966    
2024-06-06 01:09:45,958 - Epoch: [78][  900/ 1218]    Overall Loss 0.645035    Objective Loss 0.645035                                        LR 0.001000    Time 0.048908    
2024-06-06 01:09:50,850 - Epoch: [78][ 1000/ 1218]    Overall Loss 0.644843    Objective Loss 0.644843                                        LR 0.001000    Time 0.048907    
2024-06-06 01:09:55,916 - Epoch: [78][ 1100/ 1218]    Overall Loss 0.645256    Objective Loss 0.645256                                        LR 0.001000    Time 0.049066    
2024-06-06 01:10:00,763 - Epoch: [78][ 1200/ 1218]    Overall Loss 0.645468    Objective Loss 0.645468                                        LR 0.001000    Time 0.049015    
2024-06-06 01:10:01,538 - Epoch: [78][ 1218/ 1218]    Overall Loss 0.645044    Objective Loss 0.645044    Top1 74.327628    Top5 96.088020    LR 0.001000    Time 0.048926    
2024-06-06 01:10:01,705 - --- validate (epoch=78)-----------
2024-06-06 01:10:01,706 - 34633 samples (256 per mini-batch)
2024-06-06 01:10:07,298 - Epoch: [78][  100/  136]    Loss 0.600447    Top1 72.859375    Top5 94.984375    
2024-06-06 01:10:09,076 - Epoch: [78][  136/  136]    Loss 0.598303    Top1 72.884243    Top5 95.059625    
2024-06-06 01:10:09,273 - ==> Top1: 72.884    Top5: 95.060    Loss: 0.598

2024-06-06 01:10:09,274 - ==> Confusion:
[[ 734    0    2    0    6    1    1    1   17  127    1    2    3    4    8    1    1    2    8    1   11]
 [   4  865    2    1   29   13   11   31   12    5    7    9    2    2    9    2   10    3   29    1   16]
 [  10    2  771   20    2    0   50   17    2    9    8    7    4    7    7   13    6    3   12    5   15]
 [   8    1   19  829    4    2    3    2    7    1   17    2    6    6   45    2    1   12   33    1   15]
 [  33   10    5    3  894    7    3    3    6   31    1    5    0    2   12   10   10    2    8    0    9]
 [   6   47    4    7   24  711   10   69    8   10    4   32   15   22    7    2    7   10   19   12   17]
 [   2    2   21    4    2    5  981    8    3    1    2    4    2    2    3    6    2    7    8   11   10]
 [   5   20   12    2    2   18   14  875    6    5    5   20    5    1    3    2    3    6   47   11   15]
 [  15    5    2    0    0    2    1    1  855   54    5    1    3   16   18    0    2    4   12    0    6]
 [  66    2    1    0    4    5    0    0   76  809    1    0    0   14    3    4    1    5    5    0    5]
 [   3    1    2    9    2    3    4    6   38    3  908    3    0   15   22    0    0    1   37    1    6]
 [   2    4    0    0    1    4    4    9    1    3    0  836   42    7    1   18    5   46    5   13   10]
 [   0    2    1    9    0    1    4    3    5    0    1   77  752    4    7   13    5   74   16    8   13]
 [   4    1    4    0    6   14    2   12   31   31    8   18    7  821    8    6    2    9    2    9    6]
 [  13    1    3   12    8    1    1    0   70   19    3    2    2    4  916    1    1   10   24    0    7]
 [   1    0    8    4    8    0   16    0    1    1    0   17   14    0    1  947    7   30    1    4    6]
 [   9    7    4    6    6    9    4    2   12    4    4   11    7    4    4   24  914    3    2   11   25]
 [   0    1    0    2    0    1    4    4    2    2    0   14   26    3    5   11    1  916    3    3    7]
 [   1    7    6   19    5    1    2   22   15    5    6    4    5    2   27    2    2    2  914    5    6]
 [   0   10    5    1    2    7   26   15    3    0    1   46   16   11    1   11    7    8   11  895   12]
 [ 305  236  271  206  275  135  246  281  329  276  209  294  444  306  446  205  288  317  433  331 8099]]

2024-06-06 01:10:09,276 - ==> Best [Top1: 73.722   Top5: 95.452   Sparsity:0.00   Params: 169472 on epoch: 76]
2024-06-06 01:10:09,276 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:10:09,283 - 

2024-06-06 01:10:09,283 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:10:15,498 - Epoch: [79][  100/ 1218]    Overall Loss 0.642026    Objective Loss 0.642026                                        LR 0.001000    Time 0.062127    
2024-06-06 01:10:20,253 - Epoch: [79][  200/ 1218]    Overall Loss 0.643128    Objective Loss 0.643128                                        LR 0.001000    Time 0.054830    
2024-06-06 01:10:24,797 - Epoch: [79][  300/ 1218]    Overall Loss 0.643793    Objective Loss 0.643793                                        LR 0.001000    Time 0.051695    
2024-06-06 01:10:29,344 - Epoch: [79][  400/ 1218]    Overall Loss 0.640769    Objective Loss 0.640769                                        LR 0.001000    Time 0.050133    
2024-06-06 01:10:34,015 - Epoch: [79][  500/ 1218]    Overall Loss 0.642451    Objective Loss 0.642451                                        LR 0.001000    Time 0.049445    
2024-06-06 01:10:38,598 - Epoch: [79][  600/ 1218]    Overall Loss 0.642206    Objective Loss 0.642206                                        LR 0.001000    Time 0.048839    
2024-06-06 01:10:43,233 - Epoch: [79][  700/ 1218]    Overall Loss 0.642282    Objective Loss 0.642282                                        LR 0.001000    Time 0.048481    
2024-06-06 01:10:47,832 - Epoch: [79][  800/ 1218]    Overall Loss 0.640523    Objective Loss 0.640523                                        LR 0.001000    Time 0.048168    
2024-06-06 01:10:52,377 - Epoch: [79][  900/ 1218]    Overall Loss 0.642941    Objective Loss 0.642941                                        LR 0.001000    Time 0.047864    
2024-06-06 01:10:57,768 - Epoch: [79][ 1000/ 1218]    Overall Loss 0.643155    Objective Loss 0.643155                                        LR 0.001000    Time 0.048466    
2024-06-06 01:11:02,305 - Epoch: [79][ 1100/ 1218]    Overall Loss 0.643626    Objective Loss 0.643626                                        LR 0.001000    Time 0.048183    
2024-06-06 01:11:07,070 - Epoch: [79][ 1200/ 1218]    Overall Loss 0.644054    Objective Loss 0.644054                                        LR 0.001000    Time 0.048138    
2024-06-06 01:11:07,949 - Epoch: [79][ 1218/ 1218]    Overall Loss 0.644188    Objective Loss 0.644188    Top1 72.371638    Top5 95.599022    LR 0.001000    Time 0.048147    
2024-06-06 01:11:08,135 - --- validate (epoch=79)-----------
2024-06-06 01:11:08,135 - 34633 samples (256 per mini-batch)
2024-06-06 01:11:13,615 - Epoch: [79][  100/  136]    Loss 0.589593    Top1 72.652344    Top5 94.925781    
2024-06-06 01:11:15,290 - Epoch: [79][  136/  136]    Loss 0.591790    Top1 72.760084    Top5 95.019201    
2024-06-06 01:11:15,461 - ==> Top1: 72.760    Top5: 95.019    Loss: 0.592

2024-06-06 01:11:15,462 - ==> Confusion:
[[ 804    1    6    3   11    2    0    2    4   52    1    1    0    7    9    5    6    6    2    1    8]
 [   3  905    5    3   33   35    6    4    2    0    6    2    5    1    6    3    5    4   19    9    7]
 [  13    3  804   20    2    2   37   11    1    4    4    5    3    8    5   12    4    3   10    6   13]
 [   3    3   23  856    1    8    3    1    2    3   21    1   15    3   29    5    3    7    9    1   19]
 [  34   14    3    4  894   15    0    1    5   22    0    4    1    6   13   11   11    2    4    1    9]
 [   7   38    3    8   19  800    8   30    3    7    3   16   12   37    3    5    3    9    6   15   11]
 [   2    7   36    1    1    3  953    7    3    2    6    3    5    1    0   18    4   11    3    8   12]
 [   4   28   26    5    6   61   10  790    1    0   12   19    9    5    3    3    0    5   53   25   12]
 [  23    8    0    3    4    1    0    2  777   76   19    3    3   29   33    0    6    2    9    1    3]
 [ 128    2    5    2    4    4    1    0   38  744    1    2    1   31   12    6    1    3    5    1   10]
 [   4    8    7   22    5    3    8    6   14    1  911    2    5   13   16    2    3    2   22    8    2]
 [   3    3    2    0    2   12    3    3    0    3    2  797   56   11    1   22    4   49    2   24   12]
 [   0    4    0    9    0    3    2    2    2    0    2   73  789    7    1    6    2   59    9   11   14]
 [   7    3    4    3    5   11    3    3    8   20   15   12    7  843    7    7    9    9    1   15    9]
 [  16    3    6   18   14    2    0    0   28    8   11    0    5   11  928    2    4    8   22    2   10]
 [   3    1    1    2    2    1    8    0    0    0    0   19   17    2    0  966   13   20    1    4    6]
 [   3   14    4    7   10   13    1    0    5    2    5   13    4    5    3   21  934    5    3    7   13]
 [   4    0    0    3    1    2    2    1    3    2    0   18   35    7    7   20    0  882    1    6   11]
 [   2    8   11   24    6    3    2   13   13    1   10    7    4    2   22    1    0    2  910    6   11]
 [   4   11    6    0    1   11   17   10    1    0    1   29   15    6    1   11    9   10    5  918   22]
 [ 359  322  338  246  324  247  173  148  137  129  286  241  494  444  336  331  446  252  305  381 7993]]

2024-06-06 01:11:15,463 - ==> Best [Top1: 73.722   Top5: 95.452   Sparsity:0.00   Params: 169472 on epoch: 76]
2024-06-06 01:11:15,464 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:11:15,471 - 

2024-06-06 01:11:15,472 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:11:21,623 - Epoch: [80][  100/ 1218]    Overall Loss 0.653772    Objective Loss 0.653772                                        LR 0.001000    Time 0.061487    
2024-06-06 01:11:26,435 - Epoch: [80][  200/ 1218]    Overall Loss 0.648469    Objective Loss 0.648469                                        LR 0.001000    Time 0.054798    
2024-06-06 01:11:31,015 - Epoch: [80][  300/ 1218]    Overall Loss 0.648560    Objective Loss 0.648560                                        LR 0.001000    Time 0.051793    
2024-06-06 01:11:35,799 - Epoch: [80][  400/ 1218]    Overall Loss 0.648177    Objective Loss 0.648177                                        LR 0.001000    Time 0.050799    
2024-06-06 01:11:40,442 - Epoch: [80][  500/ 1218]    Overall Loss 0.647096    Objective Loss 0.647096                                        LR 0.001000    Time 0.049922    
2024-06-06 01:11:45,071 - Epoch: [80][  600/ 1218]    Overall Loss 0.646153    Objective Loss 0.646153                                        LR 0.001000    Time 0.049314    
2024-06-06 01:11:49,739 - Epoch: [80][  700/ 1218]    Overall Loss 0.644304    Objective Loss 0.644304                                        LR 0.001000    Time 0.048935    
2024-06-06 01:11:54,483 - Epoch: [80][  800/ 1218]    Overall Loss 0.644372    Objective Loss 0.644372                                        LR 0.001000    Time 0.048746    
2024-06-06 01:11:59,181 - Epoch: [80][  900/ 1218]    Overall Loss 0.643383    Objective Loss 0.643383                                        LR 0.001000    Time 0.048547    
2024-06-06 01:12:04,012 - Epoch: [80][ 1000/ 1218]    Overall Loss 0.644593    Objective Loss 0.644593                                        LR 0.001000    Time 0.048521    
2024-06-06 01:12:08,930 - Epoch: [80][ 1100/ 1218]    Overall Loss 0.644729    Objective Loss 0.644729                                        LR 0.001000    Time 0.048580    
2024-06-06 01:12:13,481 - Epoch: [80][ 1200/ 1218]    Overall Loss 0.645022    Objective Loss 0.645022                                        LR 0.001000    Time 0.048322    
2024-06-06 01:12:14,266 - Epoch: [80][ 1218/ 1218]    Overall Loss 0.645417    Objective Loss 0.645417    Top1 73.105134    Top5 96.821516    LR 0.001000    Time 0.048253    
2024-06-06 01:12:14,473 - --- validate (epoch=80)-----------
2024-06-06 01:12:14,473 - 34633 samples (256 per mini-batch)
2024-06-06 01:12:20,174 - Epoch: [80][  100/  136]    Loss 0.606845    Top1 71.531250    Top5 94.507812    
2024-06-06 01:12:21,874 - Epoch: [80][  136/  136]    Loss 0.602982    Top1 71.723501    Top5 94.652499    
2024-06-06 01:12:22,047 - ==> Top1: 71.724    Top5: 94.652    Loss: 0.603

2024-06-06 01:12:22,048 - ==> Confusion:
[[ 791    1    4    0    9    4    0    2   20   61    0    2    2    6    4    1    3    6    5    4    6]
 [   1  858    2    2   20   40    4   33    4    2    5   11    2    2   10    3    8    5   29    8   14]
 [  13    1  818    7    3    8   13   26    4    3    8    8    1    7    5    8   10    8    5    5    9]
 [   6    2   32  787    4    6    4    6    5    1   45    3   13    3   40    1    2   12   33    1   10]
 [  46   21    4    1  880   19    1    5    5   10    4    4    1    3   15    2   11    2   11    2    7]
 [   4   30    2    4   12  823    4   54    9    2    4   29    8   14    3    4    3    4    8   12   10]
 [   2    7   79    2    0    9  900   11    2    1    4   12    5    2    0    5    4   10    3   12   16]
 [   4   18   16    0    1   34    3  875    5    1    7   21    6    4    3    1    0    3   46   13   16]
 [  10    6    2    0    2    7    0    4  815   46   30    3    4   20   23    0    3    5   13    4    5]
 [  99    4    3    1    9    9    0    7   99  710    3    4    1   21   14    1    0    6    0    1    9]
 [   4    5   15    4    1    5    3    9   19    2  919    2    1   21   13    0    0    1   26    4   10]
 [   1    2    4    0    1   11    3   10    3    0    2  850   48    5    0   14    2   32    3   17    3]
 [   2    0    3    4    0    4    1    5    2    0    4   94  791    3    7    5    2   47    8    7    6]
 [   4    1    4    0    5   16    0    6   21    9   14   30    8  832    6    4    7    9    1   15    9]
 [   9    6    3    7   13    4    0    0   45    5    9    3    6    8  929    0    2    6   29    2   12]
 [   4    2    6    1    2    1   18    1    1    1    0   46   21    2    0  903   13   22    7    5   10]
 [   7    8    3    2   11   13    0    1    7    0    2   16    4    5    1   11  943    5   10   12   11]
 [   2    2    4    2    2    5    1    1    2    1    0   42   34    0    5   10    0  875    7    3    7]
 [   2    7   15    5    2    3    1   18    9    4    9    4    4    2   25    0    2    5  920    6   15]
 [   0    8    7    0    3    8   21   23    0    1    3   54   16    5    0    8   11    6   12  892   10]
 [ 321  324  339  139  304  322  116  307  234   97  330  424  524  365  358  164  489  247  398  401 7729]]

2024-06-06 01:12:22,050 - ==> Best [Top1: 73.722   Top5: 95.452   Sparsity:0.00   Params: 169472 on epoch: 76]
2024-06-06 01:12:22,050 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:12:22,058 - 

2024-06-06 01:12:22,058 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:12:28,312 - Epoch: [81][  100/ 1218]    Overall Loss 0.644709    Objective Loss 0.644709                                        LR 0.001000    Time 0.062523    
2024-06-06 01:12:33,090 - Epoch: [81][  200/ 1218]    Overall Loss 0.643198    Objective Loss 0.643198                                        LR 0.001000    Time 0.055142    
2024-06-06 01:12:37,908 - Epoch: [81][  300/ 1218]    Overall Loss 0.645671    Objective Loss 0.645671                                        LR 0.001000    Time 0.052814    
2024-06-06 01:12:42,470 - Epoch: [81][  400/ 1218]    Overall Loss 0.644291    Objective Loss 0.644291                                        LR 0.001000    Time 0.051012    
2024-06-06 01:12:47,373 - Epoch: [81][  500/ 1218]    Overall Loss 0.640977    Objective Loss 0.640977                                        LR 0.001000    Time 0.050612    
2024-06-06 01:12:51,959 - Epoch: [81][  600/ 1218]    Overall Loss 0.642420    Objective Loss 0.642420                                        LR 0.001000    Time 0.049818    
2024-06-06 01:12:56,694 - Epoch: [81][  700/ 1218]    Overall Loss 0.642721    Objective Loss 0.642721                                        LR 0.001000    Time 0.049461    
2024-06-06 01:13:01,339 - Epoch: [81][  800/ 1218]    Overall Loss 0.643582    Objective Loss 0.643582                                        LR 0.001000    Time 0.049083    
2024-06-06 01:13:06,714 - Epoch: [81][  900/ 1218]    Overall Loss 0.644620    Objective Loss 0.644620                                        LR 0.001000    Time 0.049600    
2024-06-06 01:13:11,899 - Epoch: [81][ 1000/ 1218]    Overall Loss 0.644050    Objective Loss 0.644050                                        LR 0.001000    Time 0.049824    
2024-06-06 01:13:16,841 - Epoch: [81][ 1100/ 1218]    Overall Loss 0.643795    Objective Loss 0.643795                                        LR 0.001000    Time 0.049785    
2024-06-06 01:13:21,636 - Epoch: [81][ 1200/ 1218]    Overall Loss 0.643399    Objective Loss 0.643399                                        LR 0.001000    Time 0.049631    
2024-06-06 01:13:22,536 - Epoch: [81][ 1218/ 1218]    Overall Loss 0.643417    Objective Loss 0.643417    Top1 74.327628    Top5 95.110024    LR 0.001000    Time 0.049636    
2024-06-06 01:13:22,705 - --- validate (epoch=81)-----------
2024-06-06 01:13:22,706 - 34633 samples (256 per mini-batch)
2024-06-06 01:13:28,321 - Epoch: [81][  100/  136]    Loss 0.590343    Top1 73.437500    Top5 95.175781    
2024-06-06 01:13:29,996 - Epoch: [81][  136/  136]    Loss 0.590688    Top1 73.689833    Top5 95.152023    
2024-06-06 01:13:30,168 - ==> Top1: 73.690    Top5: 95.152    Loss: 0.591

2024-06-06 01:13:30,169 - ==> Confusion:
[[ 718    0    5    1   17    6    2    4   10  122    1    2    3    5    6    1    2    7    2    4   13]
 [   0  878    4    2   20   39   10   27    1    4    7    4    1    4    7    0    3    4   26    6   16]
 [   4    4  806    6   13    2   29   22    1    8    7    6    8    6    5    8    8    1    8    7   11]
 [   4    7   36  828    1    8    3    4    2    3   25    1   17    3   16    3    1    7   25    5   17]
 [  27    4    1    1  894   16    2    5    2   25    4    5    3    9    7    7   11    4   12    1   14]
 [   5   24    3    1   13  831    2   51    2    5    4   20    6   30    3    2    4    4    9   13   11]
 [   0    5   36    2    2    7  953    7    1    1    6    6    4    1    0    6    3    3    8   27    8]
 [   3    7    9    2    2   33    8  890    1    4    7   12    8    7    0    0    0    1   49   23   11]
 [  10    7    2    2    1    5    0    1  770   80   31    4    4   38   12    0    4    1   11    4   15]
 [  63    0    3    0    9    6    1    3   44  807    1    3    2   37    4    0    1    3    2    2   10]
 [   1    4   17   14    2    5    3    7   11    3  920    3    1   20   11    0    0    0   28    3   11]
 [   2    0    1    0    4   13    2   10    1    0    0  819   46   13    1    9    3   26    8   44    9]
 [   2    0    5    8    2    7    2    5    3    0    1   93  762    6    1    6    4   49    9   14   16]
 [   1    2    2    0    6   17    2    3   13   19   14   17    5  862    0    0    0    6    4   17   11]
 [  12    7    5   25   18    5    1    3   55   10   17    1    3   21  856    1    7    8   27    1   15]
 [   2    2    7    1    7    1   16    1    0    3    0   36   11    2    0  907   25   14    2   11   18]
 [   4   13    3    1   16    8    2    2    4    1    8    8   11   12    0   15  925    4    3   17   15]
 [   2    4    1    0    1    2    2    5    3    2    0   40   28    8    2   15    1  872    4    3   10]
 [   4    4    5   14    4    7    2   34    6    1    9    5    6    0   15    1    2    5  924    3    7]
 [   0    5    5    0    1   13   15   21    1    3    1   21   12    4    0    6    5    5   10  946   14]
 [ 213  231  286  133  342  285  178  298  140  189  270  276  447  487  212  185  370  171  320  546 8353]]

2024-06-06 01:13:30,171 - ==> Best [Top1: 73.722   Top5: 95.452   Sparsity:0.00   Params: 169472 on epoch: 76]
2024-06-06 01:13:30,171 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:13:30,182 - 

2024-06-06 01:13:30,182 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:13:36,227 - Epoch: [82][  100/ 1218]    Overall Loss 0.656196    Objective Loss 0.656196                                        LR 0.001000    Time 0.060425    
2024-06-06 01:13:40,909 - Epoch: [82][  200/ 1218]    Overall Loss 0.653601    Objective Loss 0.653601                                        LR 0.001000    Time 0.053610    
2024-06-06 01:13:45,600 - Epoch: [82][  300/ 1218]    Overall Loss 0.649312    Objective Loss 0.649312                                        LR 0.001000    Time 0.051372    
2024-06-06 01:13:50,142 - Epoch: [82][  400/ 1218]    Overall Loss 0.647826    Objective Loss 0.647826                                        LR 0.001000    Time 0.049880    
2024-06-06 01:13:54,720 - Epoch: [82][  500/ 1218]    Overall Loss 0.648471    Objective Loss 0.648471                                        LR 0.001000    Time 0.049056    
2024-06-06 01:13:59,267 - Epoch: [82][  600/ 1218]    Overall Loss 0.644969    Objective Loss 0.644969                                        LR 0.001000    Time 0.048456    
2024-06-06 01:14:04,029 - Epoch: [82][  700/ 1218]    Overall Loss 0.645544    Objective Loss 0.645544                                        LR 0.001000    Time 0.048334    
2024-06-06 01:14:08,624 - Epoch: [82][  800/ 1218]    Overall Loss 0.645204    Objective Loss 0.645204                                        LR 0.001000    Time 0.048034    
2024-06-06 01:14:13,276 - Epoch: [82][  900/ 1218]    Overall Loss 0.642946    Objective Loss 0.642946                                        LR 0.001000    Time 0.047863    
2024-06-06 01:14:17,859 - Epoch: [82][ 1000/ 1218]    Overall Loss 0.643444    Objective Loss 0.643444                                        LR 0.001000    Time 0.047658    
2024-06-06 01:14:22,670 - Epoch: [82][ 1100/ 1218]    Overall Loss 0.642384    Objective Loss 0.642384                                        LR 0.001000    Time 0.047698    
2024-06-06 01:14:27,514 - Epoch: [82][ 1200/ 1218]    Overall Loss 0.643307    Objective Loss 0.643307                                        LR 0.001000    Time 0.047758    
2024-06-06 01:14:28,289 - Epoch: [82][ 1218/ 1218]    Overall Loss 0.643570    Objective Loss 0.643570    Top1 69.437653    Top5 93.643032    LR 0.001000    Time 0.047688    
2024-06-06 01:14:28,500 - --- validate (epoch=82)-----------
2024-06-06 01:14:28,500 - 34633 samples (256 per mini-batch)
2024-06-06 01:14:34,105 - Epoch: [82][  100/  136]    Loss 0.592491    Top1 71.671875    Top5 94.585938    
2024-06-06 01:14:35,744 - Epoch: [82][  136/  136]    Loss 0.591153    Top1 71.928507    Top5 94.736234    
2024-06-06 01:14:35,932 - ==> Top1: 71.929    Top5: 94.736    Loss: 0.591

2024-06-06 01:14:35,934 - ==> Confusion:
[[ 800    4    5    0   14    1    0    4   12   54    0    3    1    8    9    1    1    2    4    2    6]
 [   4  906    4    2   24   27   12   20    1    2    5    3    1    1    5    1    9    5   16    5   10]
 [  13    6  790   15    7    3   32   18    1    5    7    4    3    4    4   14   12    2   10    8   12]
 [  13    3   36  813    5    6    0    4    4    0   28    2   15    3   36    1    7    7   25    1    7]
 [  56   15    2    1  894    6    0    3    4   13    1    2    3    4   12   10    9    0    7    2   10]
 [  10   48    8    8   15  772    4   65    6    3    5   17   11   21    5    4    8    4    8   13    8]
 [   2    5   32    6    3    4  962   12    4    2    2    5    2    1    0    8    2    4    6   15    9]
 [   6   21   15    2    2   39    5  871    5    2    5    7    6    2    4    0    2    4   43   23   13]
 [  24    6    1    0    0    3    0    2  804   55   17    1    4   15   36    0    3    3   20    3    5]
 [ 121    0    6    0    8    5    3    5   54  737    1    0    2   27   20    2    0    2    4    0    4]
 [   4    4   12   12    2    8    9   11   21    1  895    2    3    9   15    1    5    2   37    5    6]
 [   4    2    0    0    3   13    8   17    4    1    0  792   54   16    2   15    4   33    5   28   10]
 [   5    2    5    5    0    4    4    5    6    1    6   55  790    5    3    9    7   57    9    6   11]
 [   5    0    8    3    9   15    4    9   21   16   13   13   17  814   15    4    4    8    0   14    9]
 [  15    2    3   20   14    2    0    1   35    6    6    0    2    5  942    1    0    5   28    1   10]
 [   4    2    4    1    5    0   11    1    0    0    0   24   15    1    1  937   17   23    3    2   15]
 [  15   13    6    2    9    9    7    2    5    0    1   11    6    6    1   11  935    3    4    8   18]
 [   3    2    0    4    0    2    3    1    2    2    1   12   39    7    6   14    4  886    3    6    8]
 [   5    6    6   18    3    3    2   24   11    0    8    1    6    2   15    2    1    0  933    3    9]
 [   3   11    5    1    1    4   26   20    2    1    1   24   17    4    0    8    4    5   10  930   11]
 [ 422  335  303  204  338  272  184  326  218  164  215  204  510  373  403  192  483  194  409  475 7708]]

2024-06-06 01:14:35,935 - ==> Best [Top1: 73.722   Top5: 95.452   Sparsity:0.00   Params: 169472 on epoch: 76]
2024-06-06 01:14:35,935 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:14:35,943 - 

2024-06-06 01:14:35,943 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:14:42,036 - Epoch: [83][  100/ 1218]    Overall Loss 0.634523    Objective Loss 0.634523                                        LR 0.001000    Time 0.060914    
2024-06-06 01:14:46,774 - Epoch: [83][  200/ 1218]    Overall Loss 0.635311    Objective Loss 0.635311                                        LR 0.001000    Time 0.054136    
2024-06-06 01:14:51,487 - Epoch: [83][  300/ 1218]    Overall Loss 0.639660    Objective Loss 0.639660                                        LR 0.001000    Time 0.051797    
2024-06-06 01:14:56,310 - Epoch: [83][  400/ 1218]    Overall Loss 0.640988    Objective Loss 0.640988                                        LR 0.001000    Time 0.050901    
2024-06-06 01:15:01,244 - Epoch: [83][  500/ 1218]    Overall Loss 0.640453    Objective Loss 0.640453                                        LR 0.001000    Time 0.050583    
2024-06-06 01:15:05,939 - Epoch: [83][  600/ 1218]    Overall Loss 0.640242    Objective Loss 0.640242                                        LR 0.001000    Time 0.049971    
2024-06-06 01:15:10,853 - Epoch: [83][  700/ 1218]    Overall Loss 0.641404    Objective Loss 0.641404                                        LR 0.001000    Time 0.049851    
2024-06-06 01:15:15,559 - Epoch: [83][  800/ 1218]    Overall Loss 0.642630    Objective Loss 0.642630                                        LR 0.001000    Time 0.049499    
2024-06-06 01:15:20,234 - Epoch: [83][  900/ 1218]    Overall Loss 0.641844    Objective Loss 0.641844                                        LR 0.001000    Time 0.049191    
2024-06-06 01:15:24,895 - Epoch: [83][ 1000/ 1218]    Overall Loss 0.641203    Objective Loss 0.641203                                        LR 0.001000    Time 0.048932    
2024-06-06 01:15:29,589 - Epoch: [83][ 1100/ 1218]    Overall Loss 0.640490    Objective Loss 0.640490                                        LR 0.001000    Time 0.048749    
2024-06-06 01:15:34,501 - Epoch: [83][ 1200/ 1218]    Overall Loss 0.641019    Objective Loss 0.641019                                        LR 0.001000    Time 0.048778    
2024-06-06 01:15:35,442 - Epoch: [83][ 1218/ 1218]    Overall Loss 0.641179    Objective Loss 0.641179    Top1 71.149144    Top5 95.110024    LR 0.001000    Time 0.048830    
2024-06-06 01:15:35,642 - --- validate (epoch=83)-----------
2024-06-06 01:15:35,642 - 34633 samples (256 per mini-batch)
2024-06-06 01:15:41,347 - Epoch: [83][  100/  136]    Loss 0.575253    Top1 71.347656    Top5 94.402344    
2024-06-06 01:15:43,051 - Epoch: [83][  136/  136]    Loss 0.579810    Top1 71.437646    Top5 94.386856    
2024-06-06 01:15:43,233 - ==> Top1: 71.438    Top5: 94.387    Loss: 0.580

2024-06-06 01:15:43,234 - ==> Confusion:
[[ 777    3    8    0   12    4    0    0   10   70    2    4    1    9    9    4    3    3    2    2    8]
 [   5  859    3    3   32   43    5   21    5    7   12    5    2    5   13    3   13    2   14    4    7]
 [  13    2  799   13    6    3   26   13    1    6   14    7    8    8    4    9   11    1    7    7   12]
 [   6    0   24  839    2    9    2    6    3    0   23    1   17    3   39    4    7   11   11    0    9]
 [  31    8    4    0  898   18    1    0    3   19    2    5    3   10   14   10   16    0    6    1    5]
 [   5   20    4    5   16  787    5   51    6   10    7   32   14   41    5    2    7    9    4    9    4]
 [   1    2   30    4    3    5  958    7    1    2   11    7    7    6    2   10    2   10    2   11    5]
 [   2   12   21    0    4   45    9  875    5    0    7   11    9   10    3    1    5    6   30   14    8]
 [  17    2    1    1    6    2    0    1  783   73   10    5    5   38   32    1    7    4    9    0    5]
 [  91    1    1    0    9    3    1    2   51  783    4    1    0   26    9    3    1    5    2    1    7]
 [   4    2   10   18    0    7    4    9   19    4  925    1    2   17   17    0    2    1   15    1    6]
 [   0    3    3    0    0   10    1    6    4    4    1  816   49   17    4   24    6   30    4   18   11]
 [   0    1    3   10    0    4    2    4    1    0    3   75  786    5    3    8    5   53    7   12   13]
 [   2    2    3    0    3   11    0    2   19   32    7   17    7  866    3    3    4    3    1    8    8]
 [  10    5    3   17   11    3    0    0   41    9   13    3    9   10  922    1    1    5   19    1   15]
 [   5    2    4    1    5    0    9    1    1    0    1   23   16    1    1  940   16   21    2    6   11]
 [   6    8    8    2    4   15    2    2    2    2    1   14    8    7    2   15  936    6    5    8   19]
 [   2    0    0    2    0    3    4    3    1    2    1   18   38    8    5   19    3  875    2    4   15]
 [   2    3   16   23    6    6    2   29   15    2   16    4    5    6   33    1    2    1  876    3    7]
 [   2    3    6    0    0   12   10   22    2    0    1   31   18    6    1   10   17   14    5  918   10]
 [ 295  241  376  165  335  307  180  264  213  246  309  272  560  550  352  317  535  237  302  353 7523]]

2024-06-06 01:15:43,235 - ==> Best [Top1: 73.722   Top5: 95.452   Sparsity:0.00   Params: 169472 on epoch: 76]
2024-06-06 01:15:43,235 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:15:43,243 - 

2024-06-06 01:15:43,243 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:15:49,630 - Epoch: [84][  100/ 1218]    Overall Loss 0.639421    Objective Loss 0.639421                                        LR 0.001000    Time 0.063855    
2024-06-06 01:15:54,227 - Epoch: [84][  200/ 1218]    Overall Loss 0.636396    Objective Loss 0.636396                                        LR 0.001000    Time 0.054902    
2024-06-06 01:15:58,936 - Epoch: [84][  300/ 1218]    Overall Loss 0.638468    Objective Loss 0.638468                                        LR 0.001000    Time 0.052289    
2024-06-06 01:16:03,637 - Epoch: [84][  400/ 1218]    Overall Loss 0.636423    Objective Loss 0.636423                                        LR 0.001000    Time 0.050965    
2024-06-06 01:16:08,344 - Epoch: [84][  500/ 1218]    Overall Loss 0.635897    Objective Loss 0.635897                                        LR 0.001000    Time 0.050184    
2024-06-06 01:16:13,001 - Epoch: [84][  600/ 1218]    Overall Loss 0.637003    Objective Loss 0.637003                                        LR 0.001000    Time 0.049578    
2024-06-06 01:16:17,587 - Epoch: [84][  700/ 1218]    Overall Loss 0.637963    Objective Loss 0.637963                                        LR 0.001000    Time 0.049044    
2024-06-06 01:16:22,186 - Epoch: [84][  800/ 1218]    Overall Loss 0.638617    Objective Loss 0.638617                                        LR 0.001000    Time 0.048660    
2024-06-06 01:16:26,720 - Epoch: [84][  900/ 1218]    Overall Loss 0.640237    Objective Loss 0.640237                                        LR 0.001000    Time 0.048288    
2024-06-06 01:16:31,313 - Epoch: [84][ 1000/ 1218]    Overall Loss 0.641161    Objective Loss 0.641161                                        LR 0.001000    Time 0.048051    
2024-06-06 01:16:35,888 - Epoch: [84][ 1100/ 1218]    Overall Loss 0.642214    Objective Loss 0.642214                                        LR 0.001000    Time 0.047840    
2024-06-06 01:16:40,409 - Epoch: [84][ 1200/ 1218]    Overall Loss 0.642810    Objective Loss 0.642810                                        LR 0.001000    Time 0.047620    
2024-06-06 01:16:41,261 - Epoch: [84][ 1218/ 1218]    Overall Loss 0.643113    Objective Loss 0.643113    Top1 72.860636    Top5 95.110024    LR 0.001000    Time 0.047615    
2024-06-06 01:16:41,437 - --- validate (epoch=84)-----------
2024-06-06 01:16:41,438 - 34633 samples (256 per mini-batch)
2024-06-06 01:16:46,954 - Epoch: [84][  100/  136]    Loss 0.603539    Top1 72.722656    Top5 95.109375    
2024-06-06 01:16:48,626 - Epoch: [84][  136/  136]    Loss 0.591112    Top1 72.895793    Top5 95.221321    
2024-06-06 01:16:48,791 - ==> Top1: 72.896    Top5: 95.221    Loss: 0.591

2024-06-06 01:16:48,792 - ==> Confusion:
[[ 792    1    4    0   16    4    1    2   20   45    2    3    3    9    8    3    2    4    7    0    5]
 [   3  922    3    1   24   25    4   12    5    3    1    8    3    1    5    2   11    4   18    2    6]
 [   8    3  819    6    7    3   47   12    1    2    3    6    4    7    2    8    2    3    6    8   13]
 [   6    6   43  818    4    7    7    6    3    2   16    4    9    2   24    1    4   13   32    3    6]
 [  30   22    3    3  892   19    1    0    1   12    4    3    3    6   13   10   11    3    4    3   11]
 [   5   48    2   12   10  795    7   37    3    2    4   26    8   26    4    5    9    8    7   11   14]
 [   3    9   36    3    2    9  940    9    2    0    4    4    2    2    0   15    5    7    8   11   15]
 [   3   30   13    2    5   39    7  857    2    0    2   15    7    1    2    2    1    2   59   19    9]
 [  18    9    0    1    6    3    1    2  792   36   18    4    6   33   40    0    5    7   11    4    6]
 [ 139    1    5    2    8    2    1    2   78  682    0    2    1   24   17    1    4    7    6    0   19]
 [   0   12   19   11    2    7   10    5   18    0  902    5    1   14   11    1    3    1   22    5   15]
 [   1    1    2    0    1   16    3    7    3    0    1  849   31   12    3   13    7   32    4   18    7]
 [   2    2    5    8    1    8    1    5    4    0    4  116  734    2    3   12    6   58    7    5   12]
 [   8    2    4    0    8    7    2    5   14   12   11   22    9  832    7    5   11    5    5    6   26]
 [  25   10    4   17   14    3    2    1   30    6    9    0    8   11  910    0    4    7   20    0   17]
 [   3    4   10    3    4    2    5    0    0    0    1   29   13    2    0  936   16   19    1    2   16]
 [  10   16    4    2    6    8    3    2    5    0    1   15    6    1    3   16  938    5    4   10   17]
 [   4    0    2    5    0    1    1    1    1    1    0   34   37    5    3   12    0  882    2    2   12]
 [   3    7   13   18    5    4    1   29    8    0    7    9    6    0   18    0    1    4  908    4   13]
 [   4    9    5    1    1    8   20   17    0    0    2   26   12    4    0   11   15    7    8  924   14]
 [ 310  384  307  162  292  259  182  216  132   89  220  359  491  401  315  239  418  297  342  395 8122]]

2024-06-06 01:16:48,794 - ==> Best [Top1: 73.722   Top5: 95.452   Sparsity:0.00   Params: 169472 on epoch: 76]
2024-06-06 01:16:48,794 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:16:48,802 - 

2024-06-06 01:16:48,802 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:16:54,856 - Epoch: [85][  100/ 1218]    Overall Loss 0.652599    Objective Loss 0.652599                                        LR 0.001000    Time 0.060520    
2024-06-06 01:16:59,392 - Epoch: [85][  200/ 1218]    Overall Loss 0.645960    Objective Loss 0.645960                                        LR 0.001000    Time 0.052933    
2024-06-06 01:17:04,028 - Epoch: [85][  300/ 1218]    Overall Loss 0.639539    Objective Loss 0.639539                                        LR 0.001000    Time 0.050735    
2024-06-06 01:17:08,883 - Epoch: [85][  400/ 1218]    Overall Loss 0.641616    Objective Loss 0.641616                                        LR 0.001000    Time 0.050185    
2024-06-06 01:17:13,612 - Epoch: [85][  500/ 1218]    Overall Loss 0.639023    Objective Loss 0.639023                                        LR 0.001000    Time 0.049602    
2024-06-06 01:17:18,314 - Epoch: [85][  600/ 1218]    Overall Loss 0.639579    Objective Loss 0.639579                                        LR 0.001000    Time 0.049169    
2024-06-06 01:17:23,056 - Epoch: [85][  700/ 1218]    Overall Loss 0.641506    Objective Loss 0.641506                                        LR 0.001000    Time 0.048916    
2024-06-06 01:17:27,763 - Epoch: [85][  800/ 1218]    Overall Loss 0.641127    Objective Loss 0.641127                                        LR 0.001000    Time 0.048684    
2024-06-06 01:17:32,605 - Epoch: [85][  900/ 1218]    Overall Loss 0.642764    Objective Loss 0.642764                                        LR 0.001000    Time 0.048652    
2024-06-06 01:17:37,170 - Epoch: [85][ 1000/ 1218]    Overall Loss 0.641061    Objective Loss 0.641061                                        LR 0.001000    Time 0.048350    
2024-06-06 01:17:41,809 - Epoch: [85][ 1100/ 1218]    Overall Loss 0.641656    Objective Loss 0.641656                                        LR 0.001000    Time 0.048170    
2024-06-06 01:17:46,610 - Epoch: [85][ 1200/ 1218]    Overall Loss 0.642286    Objective Loss 0.642286                                        LR 0.001000    Time 0.048155    
2024-06-06 01:17:47,533 - Epoch: [85][ 1218/ 1218]    Overall Loss 0.642387    Objective Loss 0.642387    Top1 74.083130    Top5 95.599022    LR 0.001000    Time 0.048201    
2024-06-06 01:17:47,718 - --- validate (epoch=85)-----------
2024-06-06 01:17:47,718 - 34633 samples (256 per mini-batch)
2024-06-06 01:17:53,485 - Epoch: [85][  100/  136]    Loss 0.584192    Top1 73.636719    Top5 95.175781    
2024-06-06 01:17:55,172 - Epoch: [85][  136/  136]    Loss 0.582936    Top1 73.608986    Top5 95.189559    
2024-06-06 01:17:55,360 - ==> Top1: 73.609    Top5: 95.190    Loss: 0.583

2024-06-06 01:17:55,362 - ==> Confusion:
[[ 783    0    4    0   12    3    1    0   11   65    1    1    1    2   13    4    5    2    8    2   13]
 [   4  876    4    2   24   43    7   27    4    3    5    4    1    3    6    4    5    0   25    3   13]
 [  13    2  792   16    6    2   43   19    0    6    5    3    3    4    3   10    9    1   13    9   11]
 [   4    1   21  835    2   10    2    1    5    3   23    2    8    5   39    4    0    5   25    1   20]
 [  46    8    2    0  897    8    3    0    0   20    5    1    2   12   12    4   13    0   10    0   11]
 [   4   29    3    3   25  809    6   40    6   13    2   13    7   40    1    2    8    1    6   11   14]
 [   4    4   18    4    4    7  988    8    1    1    6    3    3    3    2    8    3    0    1   10    8]
 [  10   12   12    7    5   44   14  844    2    5   14    7    6   10    2    1    0    4   47   24    7]
 [  19    3    2    1    2    5    1    3  801   63   21    2    4    8   29    1    1    1   22    2   11]
 [ 108    0    4    1    8    3    0    2   47  779    1    0    0   21   12    0    2    1    2    2    8]
 [   4    6   13   12    2    1    9    3   10    3  929    0    1   16   17    1    2    1   21    6    7]
 [   2    3    4    0    5   30    6    9    3    5    2  777   36   26    3   18    7   16    7   36   16]
 [   3    0    2    5    1    9    4    5    3    1    2   71  777   17    2   13    4   30   11   15   20]
 [   6    0    2    0    5   10    1    1   13   27   17    7    4  872   12    1    0    3    3    5   12]
 [  22    2    2   18    7    4    1    0   44    8    3    2    4    7  929    1    2    3   29    1    9]
 [   4    1    5    4    5    1   18    0    2    4    0   11   11    9    1  933   20   16    2    3   16]
 [   7   18    5    0    8   13    6    0    7    3    4    9    5   11    7   11  927    2    6    8   15]
 [   3    3    2   10    2    0    5    5    2    7    1   36   43   17    3   26    3  814    6    6   11]
 [   3    7    6   18    4    2    2   31    6    2    7    3    4    4   19    3    3    5  914    8    7]
 [   2    8    6    0    4   12   24   12    0    1    3   24    7   10    1    9    7    3    3  940   12]
 [ 362  254  340  190  338  279  250  211  186  216  250  190  387  451  346  199  276  139  360  431 8277]]

2024-06-06 01:17:55,364 - ==> Best [Top1: 73.722   Top5: 95.452   Sparsity:0.00   Params: 169472 on epoch: 76]
2024-06-06 01:17:55,364 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:17:55,375 - 

2024-06-06 01:17:55,375 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:18:01,370 - Epoch: [86][  100/ 1218]    Overall Loss 0.641360    Objective Loss 0.641360                                        LR 0.001000    Time 0.059924    
2024-06-06 01:18:05,933 - Epoch: [86][  200/ 1218]    Overall Loss 0.642566    Objective Loss 0.642566                                        LR 0.001000    Time 0.052760    
2024-06-06 01:18:10,611 - Epoch: [86][  300/ 1218]    Overall Loss 0.636212    Objective Loss 0.636212                                        LR 0.001000    Time 0.050762    
2024-06-06 01:18:15,329 - Epoch: [86][  400/ 1218]    Overall Loss 0.635194    Objective Loss 0.635194                                        LR 0.001000    Time 0.049863    
2024-06-06 01:18:20,290 - Epoch: [86][  500/ 1218]    Overall Loss 0.637756    Objective Loss 0.637756                                        LR 0.001000    Time 0.049807    
2024-06-06 01:18:24,872 - Epoch: [86][  600/ 1218]    Overall Loss 0.641259    Objective Loss 0.641259                                        LR 0.001000    Time 0.049140    
2024-06-06 01:18:29,650 - Epoch: [86][  700/ 1218]    Overall Loss 0.642405    Objective Loss 0.642405                                        LR 0.001000    Time 0.048943    
2024-06-06 01:18:34,280 - Epoch: [86][  800/ 1218]    Overall Loss 0.642936    Objective Loss 0.642936                                        LR 0.001000    Time 0.048610    
2024-06-06 01:18:38,995 - Epoch: [86][  900/ 1218]    Overall Loss 0.643502    Objective Loss 0.643502                                        LR 0.001000    Time 0.048447    
2024-06-06 01:18:43,589 - Epoch: [86][ 1000/ 1218]    Overall Loss 0.643447    Objective Loss 0.643447                                        LR 0.001000    Time 0.048194    
2024-06-06 01:18:48,201 - Epoch: [86][ 1100/ 1218]    Overall Loss 0.643344    Objective Loss 0.643344                                        LR 0.001000    Time 0.048004    
2024-06-06 01:18:52,795 - Epoch: [86][ 1200/ 1218]    Overall Loss 0.643655    Objective Loss 0.643655                                        LR 0.001000    Time 0.047831    
2024-06-06 01:18:53,569 - Epoch: [86][ 1218/ 1218]    Overall Loss 0.643480    Objective Loss 0.643480    Top1 70.904645    Top5 94.132029    LR 0.001000    Time 0.047759    
2024-06-06 01:18:53,748 - --- validate (epoch=86)-----------
2024-06-06 01:18:53,748 - 34633 samples (256 per mini-batch)
2024-06-06 01:18:59,370 - Epoch: [86][  100/  136]    Loss 0.572321    Top1 72.363281    Top5 94.828125    
2024-06-06 01:19:01,024 - Epoch: [86][  136/  136]    Loss 0.583529    Top1 72.098865    Top5 94.655386    
2024-06-06 01:19:01,233 - ==> Top1: 72.099    Top5: 94.655    Loss: 0.584

2024-06-06 01:19:01,234 - ==> Confusion:
[[ 774    1    7    2   15    2    0    2    8   82    2    4    0    4   11    1    0    2    3    1   10]
 [   4  862    5    6   33   25    7   29    9    3    3    5    1    1    5    2   16    3   24    6   14]
 [   7    1  808   18    8    1   40   15    3    6   11    3    2    4    3    2    5    1   14   11    7]
 [   5    2   18  865    3    8    5    4    2    2   15    2    5    2   30    4    3    4   30    2    5]
 [  30    9    4    0  919   11    2    4    5   16    4    1    3    1   11    3   13    0    7    2    9]
 [   4   47    2   12   22  796    5   48   13    7    4   11    6   14    4    3    9    1   13   12   10]
 [   2    4   31    4    3    5  981    7    1    2    6    4    1    1    0    2    6    2    4    8   12]
 [   1   19   20    5    3   43   10  865    7    1    4    9    3    1    0    1    2    3   63   14    3]
 [  11    6    0    2    1    0    2    2  817   55   19    8    0   12   30    1    5    3   22    0    6]
 [  73    2    0    1    9    5    1    0   82  783    4    2    0   12   10    0    1    3    7    0    6]
 [   1    6   11   21    3    4    5    8   20    1  922    0    0    2   15    0    2    2   25    4   12]
 [   3    2    2    1    1   29    8    9    6    3    3  802   33   12    0   16   13   25    4   28   11]
 [   1    1    0   16    1   11    2    3    3    0    3   99  746    1    4    7   11   48   19    6   13]
 [   8    2    3    4    9   21    2    5   39   32   21   18    4  789    4    1    7    2    6   13   11]
 [  15    1    3   35   18    5    1    1   46   13    8    2    2    2  892    1    3    3   37    2    8]
 [   7    3   12    5   12    3   11    1    0    3    0   15    9    3    0  910   24   26    6    7    9]
 [   4   10    6    4   11   10    2    1    7    1    3    9    4    2    3   13  949    2    8   12   11]
 [   4    3    0    6    3    3    1    1    0    3    0   24   27    3    7    9    2  893    5    5    6]
 [   1    7   10   20    2    4    1   19    6    1    8    4    4    1   10    2    2    2  943    4    7]
 [   1    6    9    2    0   13   21   18    0    1    0   29   11    4    1    5   12    3   11  931   10]
 [ 321  290  374  288  364  267  215  281  266  209  236  254  394  304  349  155  597  173  486  386 7723]]

2024-06-06 01:19:01,235 - ==> Best [Top1: 73.722   Top5: 95.452   Sparsity:0.00   Params: 169472 on epoch: 76]
2024-06-06 01:19:01,235 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:19:01,243 - 

2024-06-06 01:19:01,243 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:19:07,733 - Epoch: [87][  100/ 1218]    Overall Loss 0.659897    Objective Loss 0.659897                                        LR 0.001000    Time 0.064879    
2024-06-06 01:19:12,309 - Epoch: [87][  200/ 1218]    Overall Loss 0.646455    Objective Loss 0.646455                                        LR 0.001000    Time 0.055311    
2024-06-06 01:19:17,130 - Epoch: [87][  300/ 1218]    Overall Loss 0.645221    Objective Loss 0.645221                                        LR 0.001000    Time 0.052936    
2024-06-06 01:19:21,924 - Epoch: [87][  400/ 1218]    Overall Loss 0.646390    Objective Loss 0.646390                                        LR 0.001000    Time 0.051684    
2024-06-06 01:19:26,733 - Epoch: [87][  500/ 1218]    Overall Loss 0.643650    Objective Loss 0.643650                                        LR 0.001000    Time 0.050961    
2024-06-06 01:19:31,541 - Epoch: [87][  600/ 1218]    Overall Loss 0.641528    Objective Loss 0.641528                                        LR 0.001000    Time 0.050479    
2024-06-06 01:19:36,390 - Epoch: [87][  700/ 1218]    Overall Loss 0.639895    Objective Loss 0.639895                                        LR 0.001000    Time 0.050191    
2024-06-06 01:19:41,005 - Epoch: [87][  800/ 1218]    Overall Loss 0.640530    Objective Loss 0.640530                                        LR 0.001000    Time 0.049684    
2024-06-06 01:19:45,731 - Epoch: [87][  900/ 1218]    Overall Loss 0.639393    Objective Loss 0.639393                                        LR 0.001000    Time 0.049412    
2024-06-06 01:19:50,411 - Epoch: [87][ 1000/ 1218]    Overall Loss 0.638337    Objective Loss 0.638337                                        LR 0.001000    Time 0.049150    
2024-06-06 01:19:55,187 - Epoch: [87][ 1100/ 1218]    Overall Loss 0.639392    Objective Loss 0.639392                                        LR 0.001000    Time 0.049022    
2024-06-06 01:19:59,979 - Epoch: [87][ 1200/ 1218]    Overall Loss 0.640016    Objective Loss 0.640016                                        LR 0.001000    Time 0.048929    
2024-06-06 01:20:00,781 - Epoch: [87][ 1218/ 1218]    Overall Loss 0.640190    Objective Loss 0.640190    Top1 72.371638    Top5 96.332518    LR 0.001000    Time 0.048864    
2024-06-06 01:20:00,950 - --- validate (epoch=87)-----------
2024-06-06 01:20:00,951 - 34633 samples (256 per mini-batch)
2024-06-06 01:20:06,443 - Epoch: [87][  100/  136]    Loss 0.576233    Top1 72.988281    Top5 95.035156    
2024-06-06 01:20:08,130 - Epoch: [87][  136/  136]    Loss 0.585900    Top1 72.846707    Top5 94.967228    
2024-06-06 01:20:08,323 - ==> Top1: 72.847    Top5: 94.967    Loss: 0.586

2024-06-06 01:20:08,324 - ==> Confusion:
[[ 758    0    4    2   10    9    1    8   13   86    0    2    0    6    8    4    1    3    3    0   13]
 [   4  898    5    1   12   32    7   34    8    1    7    9    2    2    8    1    4    7    9    3    9]
 [   9    4  774   15    7    5   47   31    2    5    8   12    2   12    5   10    3    2    8    4    5]
 [   3    5   22  809    6   13    4   14    1    0   17    5   14    5   52    2    4    7   22    3    8]
 [  34   20    1    2  892   18    1    9    4   13    2    4    1   13    6    6   10    3    6    4    5]
 [   2   37    1    0    8  816    1   55    4    6    2   20    8   38    7    2    2    3    7   12   12]
 [   1    4   25    1    1    8  964   12    0    2    5    8    5    3    2    9    3    4    3   18    8]
 [   0   14    9    1    3   44    3  910    2    6    4   11    3    8    5    0    1    4   19   18   12]
 [  16    4    1    1    1    7    0    8  807   56   12    5    6   26   23    0    5    3   11    1    9]
 [  76    1    3    1    7    4    2    4   60  781    5    2    0   29    8    2    0    3    1    4    8]
 [   1    1   10   16    4    9    4   16   20    1  899    1    2   26   20    0    3    2   17    2   10]
 [   1    0    5    0    1   23    1   15    1    1    0  807   44   24    0   14    6   32    1   30    5]
 [   1    4    3    2    0   10    0   10    1    1    3   86  786    3    3    6    4   46    8    6   12]
 [   2    2    1    0    5   21    0    5   12   22    5    8    5  880    4    7    2    5    2   10    3]
 [  12    3    5    7    7    4    1    5   39   10    5    4    6   10  948    0    3    7   11    1   10]
 [   0    1    7    2    5    4   11    0    0    1    1   25   13    5    0  943   12   21    1    4   10]
 [   0   22    3    1   11   14    6    7    3    0    4    9    6   14    2   18  915    1    2   14   20]
 [   1    0    2    1    0    5    0    3    2    5    0   26   43   10    3   12    0  877    6    3    6]
 [   0    6    5   10    3    6    0   46    6    1   10    5    8    4   27    2    1    4  899    3   12]
 [   1    4    5    0    2   22   14   22    1    0    0   28   10    7    0    9    6    9    3  933   12]
 [ 237  343  292  124  258  341  175  417  155  193  197  325  486  578  361  227  302  244  274  468 7935]]

2024-06-06 01:20:08,325 - ==> Best [Top1: 73.722   Top5: 95.452   Sparsity:0.00   Params: 169472 on epoch: 76]
2024-06-06 01:20:08,325 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:20:08,333 - 

2024-06-06 01:20:08,333 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:20:14,255 - Epoch: [88][  100/ 1218]    Overall Loss 0.638748    Objective Loss 0.638748                                        LR 0.001000    Time 0.059194    
2024-06-06 01:20:18,824 - Epoch: [88][  200/ 1218]    Overall Loss 0.641577    Objective Loss 0.641577                                        LR 0.001000    Time 0.052432    
2024-06-06 01:20:23,491 - Epoch: [88][  300/ 1218]    Overall Loss 0.636574    Objective Loss 0.636574                                        LR 0.001000    Time 0.050507    
2024-06-06 01:20:28,179 - Epoch: [88][  400/ 1218]    Overall Loss 0.635481    Objective Loss 0.635481                                        LR 0.001000    Time 0.049596    
2024-06-06 01:20:32,742 - Epoch: [88][  500/ 1218]    Overall Loss 0.636994    Objective Loss 0.636994                                        LR 0.001000    Time 0.048799    
2024-06-06 01:20:37,466 - Epoch: [88][  600/ 1218]    Overall Loss 0.638426    Objective Loss 0.638426                                        LR 0.001000    Time 0.048536    
2024-06-06 01:20:42,294 - Epoch: [88][  700/ 1218]    Overall Loss 0.636223    Objective Loss 0.636223                                        LR 0.001000    Time 0.048496    
2024-06-06 01:20:46,902 - Epoch: [88][  800/ 1218]    Overall Loss 0.636333    Objective Loss 0.636333                                        LR 0.001000    Time 0.048193    
2024-06-06 01:20:51,537 - Epoch: [88][  900/ 1218]    Overall Loss 0.636441    Objective Loss 0.636441                                        LR 0.001000    Time 0.047985    
2024-06-06 01:20:56,237 - Epoch: [88][ 1000/ 1218]    Overall Loss 0.636547    Objective Loss 0.636547                                        LR 0.001000    Time 0.047886    
2024-06-06 01:21:00,975 - Epoch: [88][ 1100/ 1218]    Overall Loss 0.635707    Objective Loss 0.635707                                        LR 0.001000    Time 0.047838    
2024-06-06 01:21:05,881 - Epoch: [88][ 1200/ 1218]    Overall Loss 0.636650    Objective Loss 0.636650                                        LR 0.001000    Time 0.047938    
2024-06-06 01:21:06,726 - Epoch: [88][ 1218/ 1218]    Overall Loss 0.636258    Objective Loss 0.636258    Top1 71.638142    Top5 94.865526    LR 0.001000    Time 0.047923    
2024-06-06 01:21:06,912 - --- validate (epoch=88)-----------
2024-06-06 01:21:06,912 - 34633 samples (256 per mini-batch)
2024-06-06 01:21:12,584 - Epoch: [88][  100/  136]    Loss 0.589290    Top1 71.863281    Top5 94.277344    
2024-06-06 01:21:14,349 - Epoch: [88][  136/  136]    Loss 0.589521    Top1 71.873647    Top5 94.418618    
2024-06-06 01:21:14,536 - ==> Top1: 71.874    Top5: 94.419    Loss: 0.590

2024-06-06 01:21:14,537 - ==> Confusion:
[[ 800    1    4    1    6    1    2    0    8   70    1    2    4    1    7    0    2    4    3    1   13]
 [   9  891    4    0   27   40    4   18    1    4    5    5    3    3    7    4   12    1   11    6    8]
 [  15    4  803   20    5    2   33   12    4   12    7    4    4   10    4    6    4    3    5    5    8]
 [   7    2   20  854    6   10    4    2    2    0   27    1    9    4   39    3    1    6   10    4    5]
 [  51   10    1    0  886   22    2    7    2   27    2    3    0    2   10    3    9    4    3    3    7]
 [  10   34    4    4   17  802    4   40    2   13    4   15    9   23    6    4    8    4   11   14   15]
 [   2    2   23    1    5    4  977    6    1    4    4    3    5    1    1   13    4    7    2   15    6]
 [   6   27   17    2    4   50    8  857    3    2    7   13    6    1    6    1    4   10   26   18    9]
 [  21    9    0    2    3    4    1    2  788   90    6    2    2   17   25    3    3    5    6    3   10]
 [ 105    2    4    0    6    2    0    2   44  798    0    0    3   11    8    2    0    6    0    2    6]
 [   5    7    6   18    3    7    6    9   27    5  911    1    0   14   20    1    1    3   17    0    3]
 [   4    6    3    2    3   20    3    8    1    3    1  790   46   18    2   18    4   47    3   21    8]
 [   3    3    1   21    1    6    4    2    2    0    4   68  755    4    8   13    7   59    8   10   16]
 [   6    0    0    0    7   10    1   10   28   44    8    6    7  826    4    2    6   10    2   10   14]
 [  18    5    4   21    8    4    1    1   41   17    3    3    4    8  929    3    3    2   17    0    6]
 [   4    1    3    5    3    2   11    0    0    5    0   16   14    3    3  947    9   23    3    3   11]
 [   4   15    4    4   14   10    3    0    6    4    3    7    5    6    2   17  937    8    3   11    9]
 [   2    3    0    8    0    1    2    1    2    4    1   16   32    4    7   19    3  890    5    3    2]
 [   3   10   11   34    6    3    2   29   12    5    3    2    4    1   42    1    1    1  875    5    8]
 [   2    6    1    1    4   14   11   13    2    2    3   26   18    8    2   13   11   12    5  919   15]
 [ 486  311  311  263  350  308  166  260  207  242  218  220  469  365  425  297  458  272  244  403 7657]]

2024-06-06 01:21:14,539 - ==> Best [Top1: 73.722   Top5: 95.452   Sparsity:0.00   Params: 169472 on epoch: 76]
2024-06-06 01:21:14,539 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:21:14,547 - 

2024-06-06 01:21:14,547 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:21:20,603 - Epoch: [89][  100/ 1218]    Overall Loss 0.649750    Objective Loss 0.649750                                        LR 0.001000    Time 0.060534    
2024-06-06 01:21:25,392 - Epoch: [89][  200/ 1218]    Overall Loss 0.642580    Objective Loss 0.642580                                        LR 0.001000    Time 0.054203    
2024-06-06 01:21:30,094 - Epoch: [89][  300/ 1218]    Overall Loss 0.645000    Objective Loss 0.645000                                        LR 0.001000    Time 0.051802    
2024-06-06 01:21:34,937 - Epoch: [89][  400/ 1218]    Overall Loss 0.647005    Objective Loss 0.647005                                        LR 0.001000    Time 0.050954    
2024-06-06 01:21:39,747 - Epoch: [89][  500/ 1218]    Overall Loss 0.642103    Objective Loss 0.642103                                        LR 0.001000    Time 0.050381    
2024-06-06 01:21:44,451 - Epoch: [89][  600/ 1218]    Overall Loss 0.640570    Objective Loss 0.640570                                        LR 0.001000    Time 0.049819    
2024-06-06 01:21:49,180 - Epoch: [89][  700/ 1218]    Overall Loss 0.638354    Objective Loss 0.638354                                        LR 0.001000    Time 0.049455    
2024-06-06 01:21:53,842 - Epoch: [89][  800/ 1218]    Overall Loss 0.637409    Objective Loss 0.637409                                        LR 0.001000    Time 0.049098    
2024-06-06 01:21:58,447 - Epoch: [89][  900/ 1218]    Overall Loss 0.637993    Objective Loss 0.637993                                        LR 0.001000    Time 0.048758    
2024-06-06 01:22:03,081 - Epoch: [89][ 1000/ 1218]    Overall Loss 0.637621    Objective Loss 0.637621                                        LR 0.001000    Time 0.048515    
2024-06-06 01:22:07,748 - Epoch: [89][ 1100/ 1218]    Overall Loss 0.638755    Objective Loss 0.638755                                        LR 0.001000    Time 0.048346    
2024-06-06 01:22:12,625 - Epoch: [89][ 1200/ 1218]    Overall Loss 0.638664    Objective Loss 0.638664                                        LR 0.001000    Time 0.048380    
2024-06-06 01:22:13,415 - Epoch: [89][ 1218/ 1218]    Overall Loss 0.639014    Objective Loss 0.639014    Top1 68.459658    Top5 92.420538    LR 0.001000    Time 0.048312    
2024-06-06 01:22:13,580 - --- validate (epoch=89)-----------
2024-06-06 01:22:13,580 - 34633 samples (256 per mini-batch)
2024-06-06 01:22:19,122 - Epoch: [89][  100/  136]    Loss 0.579097    Top1 71.863281    Top5 94.507812    
2024-06-06 01:22:20,852 - Epoch: [89][  136/  136]    Loss 0.573561    Top1 71.960269    Top5 94.537002    
2024-06-06 01:22:21,032 - ==> Top1: 71.960    Top5: 94.537    Loss: 0.574

2024-06-06 01:22:21,033 - ==> Confusion:
[[ 760    1    5    1   26    3    0    2   12   82    4    2    3    3    6    3    3    6    1    1    7]
 [   5  912    1    3   22   35    4   19    3    1    4    5    5    2    7    3    4    4   12    5    7]
 [  21    8  787   10    5    7   39   15    3    4   11    7    6    6    4    9    5    5    2    7    9]
 [   1    9   27  835    4    9    5    6    5    3   25    2    8    0   29    2    5    7   23    3    8]
 [  22   16    2    1  935   12    1    2    2   12    4    4    3    7   10    2    5    2    2    5    5]
 [   9   44    4    5   16  806    4   41   11    4    2   22   11   22    2    3    4    4    5   18    6]
 [   2   10   23    0    2    3  977    7    0    1    5    5    4    1    0    9    1    3    4   18   11]
 [   5   26   17    1    5   45    5  870   10    1    5    8    3    2    1    1    0    3   38   24    7]
 [  19    8    3    3    5    3    1    3  811   55   21    8    2   20   13    0    2    2    9    3   11]
 [  84    4    2    1   11    7    0    1   59  770    3    1    2   29    6    2    2    3    3    6    5]
 [   3    8    6    9    5    4    8    9   22    4  921    1    2   18   13    0    3    2   19    4    3]
 [   5    6    1    2    2   15    5    6    1    0    1  844   25    6    0   17   11   24    1   28   11]
 [   1    1    0    5    1    7    1    5    1    0    3  107  761    2    6    6   12   44    8   10   14]
 [   3    4    1    2    6   14    2    4   24   17   11   18    9  840    7    6    5    8    0    8   12]
 [  14    6    2   14   18    6    0    1   48    9   13    5    6    9  907    2    5    5   18    3    7]
 [   0    6    4    3    5    1   13    0    0    0    1   27   11    1    0  957   10   17    0    4    6]
 [   6   15    7    1   15   10    4    2    3    1    3   11   10    4    3   18  933    2    0    9   15]
 [   4    2    2    4    1    2    2    2    4    2    0   23   49    8    3   29    2  849    3    9    5]
 [   7   14   10   15    5    8    1   34    8    3    5    8    5    2   23    0    2    4  895    5    4]
 [   0    7    3    4    6    9   19   13    5    0    2   37    9    4    0    8    8    4    5  937    8]
 [ 393  511  317  169  379  310  185  252  208  139  290  352  430  479  256  249  419  161  286  532 7615]]

2024-06-06 01:22:21,035 - ==> Best [Top1: 73.722   Top5: 95.452   Sparsity:0.00   Params: 169472 on epoch: 76]
2024-06-06 01:22:21,035 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:22:21,044 - 

2024-06-06 01:22:21,044 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:22:27,144 - Epoch: [90][  100/ 1218]    Overall Loss 0.634055    Objective Loss 0.634055                                        LR 0.001000    Time 0.060976    
2024-06-06 01:22:32,182 - Epoch: [90][  200/ 1218]    Overall Loss 0.639017    Objective Loss 0.639017                                        LR 0.001000    Time 0.055666    
2024-06-06 01:22:36,940 - Epoch: [90][  300/ 1218]    Overall Loss 0.636916    Objective Loss 0.636916                                        LR 0.001000    Time 0.052964    
2024-06-06 01:22:41,573 - Epoch: [90][  400/ 1218]    Overall Loss 0.636868    Objective Loss 0.636868                                        LR 0.001000    Time 0.051301    
2024-06-06 01:22:46,147 - Epoch: [90][  500/ 1218]    Overall Loss 0.637793    Objective Loss 0.637793                                        LR 0.001000    Time 0.050186    
2024-06-06 01:22:50,792 - Epoch: [90][  600/ 1218]    Overall Loss 0.635625    Objective Loss 0.635625                                        LR 0.001000    Time 0.049561    
2024-06-06 01:22:55,486 - Epoch: [90][  700/ 1218]    Overall Loss 0.635946    Objective Loss 0.635946                                        LR 0.001000    Time 0.049184    
2024-06-06 01:23:00,201 - Epoch: [90][  800/ 1218]    Overall Loss 0.636304    Objective Loss 0.636304                                        LR 0.001000    Time 0.048924    
2024-06-06 01:23:05,273 - Epoch: [90][  900/ 1218]    Overall Loss 0.637362    Objective Loss 0.637362                                        LR 0.001000    Time 0.049122    
2024-06-06 01:23:10,111 - Epoch: [90][ 1000/ 1218]    Overall Loss 0.639337    Objective Loss 0.639337                                        LR 0.001000    Time 0.049046    
2024-06-06 01:23:15,181 - Epoch: [90][ 1100/ 1218]    Overall Loss 0.639760    Objective Loss 0.639760                                        LR 0.001000    Time 0.049194    
2024-06-06 01:23:19,855 - Epoch: [90][ 1200/ 1218]    Overall Loss 0.638884    Objective Loss 0.638884                                        LR 0.001000    Time 0.048989    
2024-06-06 01:23:20,635 - Epoch: [90][ 1218/ 1218]    Overall Loss 0.638733    Objective Loss 0.638733    Top1 74.327628    Top5 95.843521    LR 0.001000    Time 0.048904    
2024-06-06 01:23:20,834 - --- validate (epoch=90)-----------
2024-06-06 01:23:20,835 - 34633 samples (256 per mini-batch)
2024-06-06 01:23:26,286 - Epoch: [90][  100/  136]    Loss 0.580573    Top1 74.050781    Top5 95.335938    
2024-06-06 01:23:27,948 - Epoch: [90][  136/  136]    Loss 0.581023    Top1 73.839979    Top5 95.310831    
2024-06-06 01:23:28,128 - ==> Top1: 73.840    Top5: 95.311    Loss: 0.581

2024-06-06 01:23:28,129 - ==> Confusion:
[[ 760    0    4    2   13    0    1    2   14   87    1    3    4    5    4    4    3    4    4    5   11]
 [   3  912    5    2   27   23    6   23   11    3    3    5    4    2    2    3    4    4   10    4    7]
 [  14    3  787   14    7    4   43   18    0    6    7    7    4    5    4    9    8    6    4    7   13]
 [   3    2   26  845    2    4    3    3    7    0   16    1   19    2   30    4    4   12   17    2   14]
 [  34   16    5    1  892   15    0    4    5   18    3    6    2    4   10    6   14    0    8    5    6]
 [   8   48    1    3   16  789    1   43    7    7    2   16   13   33    5    5    9    5    7   10   15]
 [   0    6   24    1    1    5  961   17    3    0    4    3    4    0    1    5    5    3    1   20   22]
 [   6   19   12    6    2   37    9  853    2    4    9   15    8    1    3    1    3    2   49   22   14]
 [  11    8    1    3    0    4    1    2  808   61   14    1    5   29   13    1    2   10   15    3   10]
 [  65    2    1    0    8    3    1    0   66  795    2    2    2   27    5    2    1    4    2    3   10]
 [   4    9   17   10    3    3   10    6   24    4  904    2    1   24    5    2    1    0   22    3   10]
 [   1    4    4    1    6   16    1    7    4    1    3  832   39   11    0   15    3   23    0   27   13]
 [   0    1    2    6    3    7    0    8    1    0    2   84  790    1    3    6    5   48   12    5   11]
 [   4    4    3    2    2   13    1    6   18   13   10   20   10  857    4    5    4    4    1    5   15]
 [  22    9    4   13   15    3    1    4   66   14    5    2    7   10  875    2    6    4   20    2   14]
 [   4    5    5    5    3    2    5    0    1    0    1   30   20    3    0  924   14   21    3    5   15]
 [   6   14    3    3   11    9    3    1    4    0    1   12    8    6    3    9  943    4    5    7   20]
 [   2    2    1    5    1    2    1    3    2    1    0   32   45    6    2   17    5  859    2    4   13]
 [   7   13    7   24    7    4    3   22   12    2    9    3   10    1   15    1    3    1  904    1    9]
 [   0    4    2    4    5   16   13   14    1    0    1   33   22    9    0    7   12    5    8  916   16]
 [ 303  367  281  147  302  240  156  268  196  136  226  285  494  403  234  207  415  196  269  440 8367]]

2024-06-06 01:23:28,131 - ==> Best [Top1: 73.840   Top5: 95.311   Sparsity:0.00   Params: 169472 on epoch: 90]
2024-06-06 01:23:28,131 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:23:28,146 - 

2024-06-06 01:23:28,146 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:23:34,262 - Epoch: [91][  100/ 1218]    Overall Loss 0.639791    Objective Loss 0.639791                                        LR 0.001000    Time 0.061128    
2024-06-06 01:23:39,096 - Epoch: [91][  200/ 1218]    Overall Loss 0.640098    Objective Loss 0.640098                                        LR 0.001000    Time 0.054728    
2024-06-06 01:23:44,109 - Epoch: [91][  300/ 1218]    Overall Loss 0.645173    Objective Loss 0.645173                                        LR 0.001000    Time 0.053187    
2024-06-06 01:23:49,222 - Epoch: [91][  400/ 1218]    Overall Loss 0.647784    Objective Loss 0.647784                                        LR 0.001000    Time 0.052670    
2024-06-06 01:23:53,963 - Epoch: [91][  500/ 1218]    Overall Loss 0.643594    Objective Loss 0.643594                                        LR 0.001000    Time 0.051614    
2024-06-06 01:23:58,659 - Epoch: [91][  600/ 1218]    Overall Loss 0.644920    Objective Loss 0.644920                                        LR 0.001000    Time 0.050835    
2024-06-06 01:24:03,349 - Epoch: [91][  700/ 1218]    Overall Loss 0.644079    Objective Loss 0.644079                                        LR 0.001000    Time 0.050271    
2024-06-06 01:24:08,010 - Epoch: [91][  800/ 1218]    Overall Loss 0.643041    Objective Loss 0.643041                                        LR 0.001000    Time 0.049811    
2024-06-06 01:24:12,791 - Epoch: [91][  900/ 1218]    Overall Loss 0.641939    Objective Loss 0.641939                                        LR 0.001000    Time 0.049587    
2024-06-06 01:24:17,776 - Epoch: [91][ 1000/ 1218]    Overall Loss 0.640648    Objective Loss 0.640648                                        LR 0.001000    Time 0.049611    
2024-06-06 01:24:22,661 - Epoch: [91][ 1100/ 1218]    Overall Loss 0.641207    Objective Loss 0.641207                                        LR 0.001000    Time 0.049540    
2024-06-06 01:24:27,406 - Epoch: [91][ 1200/ 1218]    Overall Loss 0.640847    Objective Loss 0.640847                                        LR 0.001000    Time 0.049365    
2024-06-06 01:24:28,184 - Epoch: [91][ 1218/ 1218]    Overall Loss 0.640641    Objective Loss 0.640641    Top1 74.327628    Top5 95.599022    LR 0.001000    Time 0.049273    
2024-06-06 01:24:28,363 - --- validate (epoch=91)-----------
2024-06-06 01:24:28,364 - 34633 samples (256 per mini-batch)
2024-06-06 01:24:33,932 - Epoch: [91][  100/  136]    Loss 0.590597    Top1 71.996094    Top5 94.589844    
2024-06-06 01:24:35,650 - Epoch: [91][  136/  136]    Loss 0.588043    Top1 71.928507    Top5 94.614963    
2024-06-06 01:24:35,831 - ==> Top1: 71.929    Top5: 94.615    Loss: 0.588

2024-06-06 01:24:35,832 - ==> Confusion:
[[ 733    2   11    0    7    2    1    3   24  108    1    2    1    3   11    1    1    2    4    6    8]
 [   0  884    6    3   23   25    8   26   11    1   10    2    4    2    7    1    4    2   28    8    8]
 [   5    3  825    4    5    1   31   18    5    8    8    4    1    9    4    7    5    0    8    7   12]
 [   5    1   42  807    2    6    5    3    7    2   31    1   15    8   43    1    1    3   26    1    6]
 [  32   17   10    2  870   16    3    2    7   20    4    4    1    9   18    4   10    0    5    5   15]
 [   5   54    6    3   15  763    5   70    4    9    1   20    9   28    3    2    8    2   13   14    9]
 [   1    4   31    0    1    6  976   17    2    2    7    5    1    2    0    3    1    2    5   14    6]
 [   3   19   18    1    2   32    6  897    5    3    6    6    5    1    4    0    2    3   46   11    7]
 [  12    6    3    0    0    1    0    5  792   71   19    2    5   24   33    1    3    1   17    2    5]
 [  66    1    3    0    3    1    0    4   65  807    2    1    2   27    7    2    1    3    3    1    2]
 [   2    5   17   12    0    3    3    4   24    1  914    1    2   18   18    0    3    1   21    4   11]
 [   2    3    5    0    0   13    4   11    4    2    2  819   39   17    1   14    9   20    2   36    8]
 [   0    2    3   11    0    6    0   12    5    0    5   98  760    7    6    6    6   27   12   16   13]
 [   3    0    1    0    7   14    1    9   20   19    9    8    5  874    7    2    1    0    3   10    8]
 [   9    3    8   11    9    2    0    2   41   13    9    2    5   12  925    0    1    2   31    2   11]
 [   4    3   24    0    4    2   20    2    1    2    1   32   10    5    1  896   16   12    7   12   12]
 [   4   16   12    1    8   12    3    1   11    1    1    9    5    8    4    7  921    2    8   19   19]
 [   2    2    1    7    1    3    0    4    4    5    1   36   54   12   10   13    2  828    5    8    7]
 [   3    3   15   17    2    2    1   26    9    0   10    2    4    1   19    1    2    0  930    5    6]
 [   2    4    7    0    0   11   26   25    0    0    5   20    9    9    0    4    5    5   10  938    8]
 [ 292  279  475  114  262  223  216  376  199  225  294  247  461  509  381  155  426  119  436  492 7751]]

2024-06-06 01:24:35,834 - ==> Best [Top1: 73.840   Top5: 95.311   Sparsity:0.00   Params: 169472 on epoch: 90]
2024-06-06 01:24:35,834 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:24:35,842 - 

2024-06-06 01:24:35,842 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:24:42,256 - Epoch: [92][  100/ 1218]    Overall Loss 0.625453    Objective Loss 0.625453                                        LR 0.001000    Time 0.064125    
2024-06-06 01:24:46,832 - Epoch: [92][  200/ 1218]    Overall Loss 0.624200    Objective Loss 0.624200                                        LR 0.001000    Time 0.054930    
2024-06-06 01:24:51,381 - Epoch: [92][  300/ 1218]    Overall Loss 0.628487    Objective Loss 0.628487                                        LR 0.001000    Time 0.051778    
2024-06-06 01:24:56,028 - Epoch: [92][  400/ 1218]    Overall Loss 0.630108    Objective Loss 0.630108                                        LR 0.001000    Time 0.050447    
2024-06-06 01:25:00,753 - Epoch: [92][  500/ 1218]    Overall Loss 0.632787    Objective Loss 0.632787                                        LR 0.001000    Time 0.049804    
2024-06-06 01:25:05,326 - Epoch: [92][  600/ 1218]    Overall Loss 0.631346    Objective Loss 0.631346                                        LR 0.001000    Time 0.049122    
2024-06-06 01:25:10,000 - Epoch: [92][  700/ 1218]    Overall Loss 0.631914    Objective Loss 0.631914                                        LR 0.001000    Time 0.048778    
2024-06-06 01:25:14,582 - Epoch: [92][  800/ 1218]    Overall Loss 0.634604    Objective Loss 0.634604                                        LR 0.001000    Time 0.048407    
2024-06-06 01:25:19,143 - Epoch: [92][  900/ 1218]    Overall Loss 0.635606    Objective Loss 0.635606                                        LR 0.001000    Time 0.048094    
2024-06-06 01:25:23,802 - Epoch: [92][ 1000/ 1218]    Overall Loss 0.634898    Objective Loss 0.634898                                        LR 0.001000    Time 0.047942    
2024-06-06 01:25:28,402 - Epoch: [92][ 1100/ 1218]    Overall Loss 0.634679    Objective Loss 0.634679                                        LR 0.001000    Time 0.047763    
2024-06-06 01:25:33,033 - Epoch: [92][ 1200/ 1218]    Overall Loss 0.634964    Objective Loss 0.634964                                        LR 0.001000    Time 0.047641    
2024-06-06 01:25:33,832 - Epoch: [92][ 1218/ 1218]    Overall Loss 0.634783    Objective Loss 0.634783    Top1 75.305623    Top5 97.310513    LR 0.001000    Time 0.047593    
2024-06-06 01:25:34,022 - --- validate (epoch=92)-----------
2024-06-06 01:25:34,022 - 34633 samples (256 per mini-batch)
2024-06-06 01:25:39,638 - Epoch: [92][  100/  136]    Loss 0.572315    Top1 73.968750    Top5 95.550781    
2024-06-06 01:25:41,340 - Epoch: [92][  136/  136]    Loss 0.571554    Top1 73.975688    Top5 95.593798    
2024-06-06 01:25:41,512 - ==> Top1: 73.976    Top5: 95.594    Loss: 0.572

2024-06-06 01:25:41,514 - ==> Confusion:
[[ 810    2    2    0   15    5    0    2    7   49    1    0    1    2    3    1    2    4    0    3   22]
 [   1  904    2    1   21   41    7   28    4    4    3    4    1    1    1    1   10    3    4    6   16]
 [  12    4  778   17    4    1   55   18    2    8    5    8    5    6    1    6    9    2    4   10   15]
 [   8    3   25  856    4    9    6    5    3    1   18    6    8    2   16    1    7    9   11    3   15]
 [  39   12    5    4  905   19    3    5    4   11    3    3    1    4    4    2    8    2    6    2   12]
 [   6   34    1    8    6  832    7   54    7    5    4   22    8    8    2    1   10    3    5    9   11]
 [   2    6   20    3    2    9  974    9    1    2    6    3    4    0    0    4    3    7    0   18   13]
 [   9   18    8    4    2   53   11  871    6    3    7   17    1    2    0    0    2    4   25   19   15]
 [  16    4    2    7    3    2    0    1  812   62   19    7    3   10   17    0    5    5   12    2   13]
 [ 124    1    2    1   11    0    2    2   57  746    0    2    0   18    7    1    2    4    4    1   16]
 [   2    4   10   17    3    4    7   10   23    2  922    0    4   10    7    0    4    1   19    7    8]
 [   4    1    4    0    0   20    3    9    3    0    1  815   41    9    0    9    8   38    1   36    9]
 [   2    1    2    7    0    3    0    5    2    0    1  110  784    3    1    5    6   47    4    9    3]
 [  11    2    3    2    5   24    1    3   32   25    6   13    3  808    4    4    6   10    0   19   20]
 [  18    5    6   37   19    4    0    2   58   16    9    2    4    6  854    0    3   12   21    2   20]
 [   2    4    4    4   10    3   11    1    3    2    0   37   14    3    0  900   14   31    2    8   13]
 [   6   12    8    1    7   18    3    0   12    2    0    9    1    2    1    8  935    1    4   14   28]
 [   3    3    0    1    0    3    2    3    5    1    0   27   33    2    2    8    0  893    1    9    9]
 [   2   13   13   27    3    9    1   42    8    3   14    5    4    2    3    1    1    0  889    6   12]
 [   4    6    2    2    3   16   18   20    1    0    2   28   10    5    1    5   12    2    2  931   18]
 [ 361  297  285  200  297  308  174  280  188  141  253  267  482  303  198  140  403  200  195  560 8400]]

2024-06-06 01:25:41,516 - ==> Best [Top1: 73.976   Top5: 95.594   Sparsity:0.00   Params: 169472 on epoch: 92]
2024-06-06 01:25:41,516 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:25:41,529 - 

2024-06-06 01:25:41,529 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:25:47,578 - Epoch: [93][  100/ 1218]    Overall Loss 0.639074    Objective Loss 0.639074                                        LR 0.001000    Time 0.060463    
2024-06-06 01:25:52,151 - Epoch: [93][  200/ 1218]    Overall Loss 0.641144    Objective Loss 0.641144                                        LR 0.001000    Time 0.053090    
2024-06-06 01:25:56,840 - Epoch: [93][  300/ 1218]    Overall Loss 0.635191    Objective Loss 0.635191                                        LR 0.001000    Time 0.051017    
2024-06-06 01:26:01,718 - Epoch: [93][  400/ 1218]    Overall Loss 0.636269    Objective Loss 0.636269                                        LR 0.001000    Time 0.050451    
2024-06-06 01:26:06,581 - Epoch: [93][  500/ 1218]    Overall Loss 0.635288    Objective Loss 0.635288                                        LR 0.001000    Time 0.050084    
2024-06-06 01:26:11,152 - Epoch: [93][  600/ 1218]    Overall Loss 0.636238    Objective Loss 0.636238                                        LR 0.001000    Time 0.049352    
2024-06-06 01:26:15,779 - Epoch: [93][  700/ 1218]    Overall Loss 0.635521    Objective Loss 0.635521                                        LR 0.001000    Time 0.048909    
2024-06-06 01:26:20,448 - Epoch: [93][  800/ 1218]    Overall Loss 0.635578    Objective Loss 0.635578                                        LR 0.001000    Time 0.048629    
2024-06-06 01:26:25,260 - Epoch: [93][  900/ 1218]    Overall Loss 0.634260    Objective Loss 0.634260                                        LR 0.001000    Time 0.048571    
2024-06-06 01:26:29,877 - Epoch: [93][ 1000/ 1218]    Overall Loss 0.635670    Objective Loss 0.635670                                        LR 0.001000    Time 0.048329    
2024-06-06 01:26:34,592 - Epoch: [93][ 1100/ 1218]    Overall Loss 0.635670    Objective Loss 0.635670                                        LR 0.001000    Time 0.048221    
2024-06-06 01:26:39,227 - Epoch: [93][ 1200/ 1218]    Overall Loss 0.636915    Objective Loss 0.636915                                        LR 0.001000    Time 0.048063    
2024-06-06 01:26:40,079 - Epoch: [93][ 1218/ 1218]    Overall Loss 0.636973    Objective Loss 0.636973    Top1 71.393643    Top5 95.599022    LR 0.001000    Time 0.048052    
2024-06-06 01:26:40,247 - --- validate (epoch=93)-----------
2024-06-06 01:26:40,248 - 34633 samples (256 per mini-batch)
2024-06-06 01:26:45,841 - Epoch: [93][  100/  136]    Loss 0.567222    Top1 73.925781    Top5 95.292969    
2024-06-06 01:26:47,495 - Epoch: [93][  136/  136]    Loss 0.569220    Top1 73.822655    Top5 95.284844    
2024-06-06 01:26:47,694 - ==> Top1: 73.823    Top5: 95.285    Loss: 0.569

2024-06-06 01:26:47,695 - ==> Confusion:
[[ 728    5    5    0   20    5    1    0   15  113    1    3    3    5   13    2    4    2    0    0    6]
 [   2  878    3    1   31   33    8   23    4    3   10    4    3    2   11    4   10    3   18    2   10]
 [   3    2  827   11    6    1   37    9    0    7    9    5    4    7    7    8    6    0    6    3   12]
 [   2    3   23  846    6    7    4    5    3    1   30    3   10    3   37    3    5    6   13    3    3]
 [  28    8    2    6  924    9    0    1    4   15    3    0    2    2   17    5   10    3    4    2    9]
 [   4   35    2    6   16  821   11   30    5    7    5   21    3   25    8    5    9    5    7    5   13]
 [   0    1   34    8    2    7  959    7    0    1   12    7    6    2    1   11    2    2    4   12    8]
 [   4   18   24    2    5   56   16  831    5    2    9   21    4    2    2    1    1    2   35   21   16]
 [  15    7    1    1    2    1    0    3  804   66   12    2    2   18   46    0    5    4    9    2    2]
 [  48    4    0    0    6    3    0    3   70  811    4    0    0   22   12    6    2    3    3    1    3]
 [   0    8   10   20    2    7    7    6   14    1  931    3    4   14   14    1    3    0   11    3    5]
 [   0    1    4    1    3   19    5    7    0    2    1  837   37   10    2   19    4   36    3   14    6]
 [   2    0    7    8    1    8    1    3    5    0    4   94  742    5    5   13    8   59    6    6   18]
 [   2    0    3    1    7   13    1    3   17   15   11   15   13  853    8    5    7    4    2   13    8]
 [   8    2    1   22    8    2    0    1   47   13   13    2    2    9  931    1    1    5   16    3   11]
 [   1    1    8    0    3    2   14    1    0    1    1   24   11    3    2  941   17   19    3    5    9]
 [   2   10    3    3   10    8    4    2    6    1    4   11    0    8    5   19  941    3    4    6   22]
 [   0    2    1    4    1    2    3    0    0    4    1   23   32    5    4   22    1  879    1    6   14]
 [   0   10   15   19    4    3    2   22    7    1   15    4    8    3   28    1    2    0  901    4    9]
 [   4   11    5    2    7   11   21   14    4    1    5   44   12    4    0   10   12    4    5  900   12]
 [ 285  251  349  183  342  266  165  197  183  204  354  258  411  434  398  226  430  206  233  275 8282]]

2024-06-06 01:26:47,696 - ==> Best [Top1: 73.976   Top5: 95.594   Sparsity:0.00   Params: 169472 on epoch: 92]
2024-06-06 01:26:47,697 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:26:47,705 - 

2024-06-06 01:26:47,705 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:26:53,888 - Epoch: [94][  100/ 1218]    Overall Loss 0.642363    Objective Loss 0.642363                                        LR 0.001000    Time 0.061808    
2024-06-06 01:26:58,524 - Epoch: [94][  200/ 1218]    Overall Loss 0.631470    Objective Loss 0.631470                                        LR 0.001000    Time 0.054073    
2024-06-06 01:27:03,437 - Epoch: [94][  300/ 1218]    Overall Loss 0.629738    Objective Loss 0.629738                                        LR 0.001000    Time 0.052419    
2024-06-06 01:27:08,111 - Epoch: [94][  400/ 1218]    Overall Loss 0.633044    Objective Loss 0.633044                                        LR 0.001000    Time 0.050996    
2024-06-06 01:27:12,764 - Epoch: [94][  500/ 1218]    Overall Loss 0.633015    Objective Loss 0.633015                                        LR 0.001000    Time 0.050099    
2024-06-06 01:27:17,471 - Epoch: [94][  600/ 1218]    Overall Loss 0.635147    Objective Loss 0.635147                                        LR 0.001000    Time 0.049591    
2024-06-06 01:27:22,354 - Epoch: [94][  700/ 1218]    Overall Loss 0.636067    Objective Loss 0.636067                                        LR 0.001000    Time 0.049480    
2024-06-06 01:27:27,043 - Epoch: [94][  800/ 1218]    Overall Loss 0.637391    Objective Loss 0.637391                                        LR 0.001000    Time 0.049153    
2024-06-06 01:27:31,630 - Epoch: [94][  900/ 1218]    Overall Loss 0.637614    Objective Loss 0.637614                                        LR 0.001000    Time 0.048786    
2024-06-06 01:27:36,175 - Epoch: [94][ 1000/ 1218]    Overall Loss 0.638784    Objective Loss 0.638784                                        LR 0.001000    Time 0.048450    
2024-06-06 01:27:40,746 - Epoch: [94][ 1100/ 1218]    Overall Loss 0.638398    Objective Loss 0.638398                                        LR 0.001000    Time 0.048200    
2024-06-06 01:27:45,540 - Epoch: [94][ 1200/ 1218]    Overall Loss 0.638294    Objective Loss 0.638294                                        LR 0.001000    Time 0.048177    
2024-06-06 01:27:46,322 - Epoch: [94][ 1218/ 1218]    Overall Loss 0.638630    Objective Loss 0.638630    Top1 71.393643    Top5 93.887531    LR 0.001000    Time 0.048106    
2024-06-06 01:27:46,511 - --- validate (epoch=94)-----------
2024-06-06 01:27:46,511 - 34633 samples (256 per mini-batch)
2024-06-06 01:27:52,224 - Epoch: [94][  100/  136]    Loss 0.566286    Top1 72.910156    Top5 95.039062    
2024-06-06 01:27:53,901 - Epoch: [94][  136/  136]    Loss 0.571925    Top1 72.765859    Top5 94.897930    
2024-06-06 01:27:54,070 - ==> Top1: 72.766    Top5: 94.898    Loss: 0.572

2024-06-06 01:27:54,072 - ==> Confusion:
[[ 770    1    2    1   13    3    1    4   11   81    1    2    0    5   10    6    2    3    2    2   11]
 [   1  893    3    1   29   29   11   17    7    3    1   11    2    3    8    5   12    5   11    4    7]
 [   7    5  746   21    6    4   57    9    2   11    7    6    2    8   12   10   11    5    9   12   20]
 [   5    4   20  859    5    9    8    6    4    3   16    4    5    3   29    5    4    6    9    3    9]
 [  27   13    3    2  927    7    1    1    5   20    2    1    0    2   12   10    7    2    4    2    6]
 [   5   28    4    3   28  815    4   38    3    2    0   26    6   23    7    2   13    3    6   18    9]
 [   4    8   22    1    4    6  977    4    0    0    4   10    6    0    0   11    3    5    0   15    6]
 [   2   20   17    2    8   60   15  823    1    3    5   21    8    3    3    1    3    4   41   25   12]
 [  14    7    0    2    6    4    0    0  792   73    7    3    6   24   34    1    6    4   10    1    8]
 [  80    2    1    1   14    4    1    2   35  808    3    1    0   22   12    0    2    6    1    0    6]
 [   3    7    8   26    5    8   15    2   24    3  891    1    2   23   14    0    2    0   17    8    5]
 [   3    2    2    1    4    9    4    6    1    1    0  852   33   14    1   19    3   24    4   21    7]
 [   2    2    3   15    0    3    1    3    0    0    2  123  728    5    5   11   10   52    8   14    8]
 [   6    1    3    0    6   15    1    2   14   21    6   15    8  860    6    5    5    8    2    6   11]
 [  13    5    2   22   12    5    0    0   34   16    4    2    6    8  937    1    4    6    8    0   13]
 [   1    1    4    0    5    0    3    0    1    0    0   21    7    4    0  973   16   12    2    5   11]
 [   2   15    4    4   11    9    3    1    3    1    0   10    5    6    2   18  938    7    2   10   21]
 [   4    3    3    5    1    1    5    3    0    3    0   34   19    4    3   25    3  882    1    2    4]
 [   4   13    7   35    6    8    5   24   13    2    3    8    6    2   38    0    1    2  868    3   10]
 [   3    6    0    5    3    9   19   10    3    2    4   34   11    3    2    8   10    3    3  940   10]
 [ 282  322  209  187  448  264  211  192  165  217  179  361  415  469  382  301  546  182  233  444 7923]]

2024-06-06 01:27:54,073 - ==> Best [Top1: 73.976   Top5: 95.594   Sparsity:0.00   Params: 169472 on epoch: 92]
2024-06-06 01:27:54,073 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:27:54,081 - 

2024-06-06 01:27:54,081 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:28:00,230 - Epoch: [95][  100/ 1218]    Overall Loss 0.623809    Objective Loss 0.623809                                        LR 0.001000    Time 0.061470    
2024-06-06 01:28:05,040 - Epoch: [95][  200/ 1218]    Overall Loss 0.629147    Objective Loss 0.629147                                        LR 0.001000    Time 0.054778    
2024-06-06 01:28:09,843 - Epoch: [95][  300/ 1218]    Overall Loss 0.634624    Objective Loss 0.634624                                        LR 0.001000    Time 0.052522    
2024-06-06 01:28:14,661 - Epoch: [95][  400/ 1218]    Overall Loss 0.631157    Objective Loss 0.631157                                        LR 0.001000    Time 0.051432    
2024-06-06 01:28:19,515 - Epoch: [95][  500/ 1218]    Overall Loss 0.631898    Objective Loss 0.631898                                        LR 0.001000    Time 0.050849    
2024-06-06 01:28:24,189 - Epoch: [95][  600/ 1218]    Overall Loss 0.631378    Objective Loss 0.631378                                        LR 0.001000    Time 0.050163    
2024-06-06 01:28:28,808 - Epoch: [95][  700/ 1218]    Overall Loss 0.631378    Objective Loss 0.631378                                        LR 0.001000    Time 0.049592    
2024-06-06 01:28:33,382 - Epoch: [95][  800/ 1218]    Overall Loss 0.632409    Objective Loss 0.632409                                        LR 0.001000    Time 0.049108    
2024-06-06 01:28:38,125 - Epoch: [95][  900/ 1218]    Overall Loss 0.633083    Objective Loss 0.633083                                        LR 0.001000    Time 0.048920    
2024-06-06 01:28:42,786 - Epoch: [95][ 1000/ 1218]    Overall Loss 0.634051    Objective Loss 0.634051                                        LR 0.001000    Time 0.048687    
2024-06-06 01:28:47,444 - Epoch: [95][ 1100/ 1218]    Overall Loss 0.634444    Objective Loss 0.634444                                        LR 0.001000    Time 0.048493    
2024-06-06 01:28:52,373 - Epoch: [95][ 1200/ 1218]    Overall Loss 0.634946    Objective Loss 0.634946                                        LR 0.001000    Time 0.048558    
2024-06-06 01:28:53,152 - Epoch: [95][ 1218/ 1218]    Overall Loss 0.634177    Objective Loss 0.634177    Top1 75.305623    Top5 95.354523    LR 0.001000    Time 0.048480    
2024-06-06 01:28:53,333 - --- validate (epoch=95)-----------
2024-06-06 01:28:53,333 - 34633 samples (256 per mini-batch)
2024-06-06 01:28:58,834 - Epoch: [95][  100/  136]    Loss 0.585180    Top1 72.546875    Top5 95.035156    
2024-06-06 01:29:00,637 - Epoch: [95][  136/  136]    Loss 0.582495    Top1 72.604164    Top5 94.918142    
2024-06-06 01:29:00,836 - ==> Top1: 72.604    Top5: 94.918    Loss: 0.582

2024-06-06 01:29:00,837 - ==> Confusion:
[[ 706    2    9    1   16    3    1    0   21  116    0    4    3    3   23    5    3    3    3    2    7]
 [   4  858    1    2   23   27    6   26    9    4    8   10    1    2   15    3   10    3   22   15   14]
 [  10    6  791   13    7    1   39   14    1    6    6    6    3    7   10   11    5    3   10    8   13]
 [   2    1   22  854    9    9    0    3    1    2   23    0   11    4   31    2    4   11   17    4    6]
 [  19    8    1    1  912   11    1    4    2   19    1    2    4    6   27    6   10    3    9    3    5]
 [   5   44    2    6   17  790   11   48    4    8    2   21   13   16   10    3    6    2    7   17   11]
 [   0    4   29    3    3    5  974    3    0    1    4    4    4    3    1   15    1    7    4   16    5]
 [   2   14   20    5    4   39   13  845    4    2    6   14   12    5    3    1    0    1   48   28   11]
 [  10    5    0    0    5    3    0    1  811   60   16    5    8   19   31    2    3    3    9    1   10]
 [  58    1    0    1    7    2    1    0   76  800    2    2    2   23   14    3    1    3    2    2    1]
 [   1    5   14   24    3    4   11    9   27    2  902    4    1   11   13    0    4    0   20    5    4]
 [   1    3    2    0    2   13    1    6    2    2    0  804   55    9    2   11   11   38    4   34   11]
 [   1    1    4    5    1    4    6    3    2    1    1   74  793    5    3   11    6   53    3   10    8]
 [   2    1    4    2   11   18    3    3   27   17   10   13    6  828   15    2    4    8    1   11   15]
 [   6    4    4   18    7    2    1    3   35   11    5    1    3    8  946    4    4    8   17    2    9]
 [   3    0    3    2    3    1   11    0    0    0    0   25    7    2    0  967   11   18    1    6    6]
 [   1   15    3    4   12   10    7    0    8    1    6    8    9    7    2   10  931    3    3   16   16]
 [   1    4    2    5    1    2    3    1    6    3    0   13   33    4    4   18    3  882    1    9   10]
 [   0    6    6   31    6    5    2   17    8    0    8    3    7    0   31    0    3    0  909    7    9]
 [   2    7    5    1    1    4   17   17    0    3    1   22    9    3    1    6    9    5    4  958   13]
 [ 249  316  298  210  323  231  166  203  221  208  272  252  503  399  459  279  402  213  322  522 7884]]

2024-06-06 01:29:00,839 - ==> Best [Top1: 73.976   Top5: 95.594   Sparsity:0.00   Params: 169472 on epoch: 92]
2024-06-06 01:29:00,839 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:29:00,846 - 

2024-06-06 01:29:00,846 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:29:06,894 - Epoch: [96][  100/ 1218]    Overall Loss 0.633183    Objective Loss 0.633183                                        LR 0.001000    Time 0.060451    
2024-06-06 01:29:11,467 - Epoch: [96][  200/ 1218]    Overall Loss 0.631340    Objective Loss 0.631340                                        LR 0.001000    Time 0.053081    
2024-06-06 01:29:16,079 - Epoch: [96][  300/ 1218]    Overall Loss 0.631735    Objective Loss 0.631735                                        LR 0.001000    Time 0.050756    
2024-06-06 01:29:20,751 - Epoch: [96][  400/ 1218]    Overall Loss 0.630089    Objective Loss 0.630089                                        LR 0.001000    Time 0.049743    
2024-06-06 01:29:25,423 - Epoch: [96][  500/ 1218]    Overall Loss 0.630285    Objective Loss 0.630285                                        LR 0.001000    Time 0.049135    
2024-06-06 01:29:30,022 - Epoch: [96][  600/ 1218]    Overall Loss 0.629224    Objective Loss 0.629224                                        LR 0.001000    Time 0.048608    
2024-06-06 01:29:34,786 - Epoch: [96][  700/ 1218]    Overall Loss 0.630181    Objective Loss 0.630181                                        LR 0.001000    Time 0.048466    
2024-06-06 01:29:39,346 - Epoch: [96][  800/ 1218]    Overall Loss 0.631013    Objective Loss 0.631013                                        LR 0.001000    Time 0.048106    
2024-06-06 01:29:43,973 - Epoch: [96][  900/ 1218]    Overall Loss 0.632295    Objective Loss 0.632295                                        LR 0.001000    Time 0.047900    
2024-06-06 01:29:48,784 - Epoch: [96][ 1000/ 1218]    Overall Loss 0.632555    Objective Loss 0.632555                                        LR 0.001000    Time 0.047920    
2024-06-06 01:29:53,377 - Epoch: [96][ 1100/ 1218]    Overall Loss 0.633107    Objective Loss 0.633107                                        LR 0.001000    Time 0.047737    
2024-06-06 01:29:57,972 - Epoch: [96][ 1200/ 1218]    Overall Loss 0.633635    Objective Loss 0.633635                                        LR 0.001000    Time 0.047586    
2024-06-06 01:29:58,801 - Epoch: [96][ 1218/ 1218]    Overall Loss 0.634000    Objective Loss 0.634000    Top1 71.882641    Top5 95.599022    LR 0.001000    Time 0.047563    
2024-06-06 01:29:58,966 - --- validate (epoch=96)-----------
2024-06-06 01:29:58,966 - 34633 samples (256 per mini-batch)
2024-06-06 01:30:04,601 - Epoch: [96][  100/  136]    Loss 0.576091    Top1 73.066406    Top5 95.332031    
2024-06-06 01:30:06,226 - Epoch: [96][  136/  136]    Loss 0.580246    Top1 73.071926    Top5 95.244420    
2024-06-06 01:30:06,417 - ==> Top1: 73.072    Top5: 95.244    Loss: 0.580

2024-06-06 01:30:06,418 - ==> Confusion:
[[ 769    0    2    4   11    9    2    7   10   85    1    2    1    4    9    2    2    3    4    1    3]
 [   1  839    1    3   25   79    7   32    5    2    5    5    0    2    3    4   13    5    9    8   15]
 [  17    1  764   37   11    9   32   15    1    7    6    3    2   10    4    7    4    2   14    7   17]
 [   6    1    9  859    3   17    4    5    1    3   13    1    4    3   38    3    4    6   19    2   15]
 [  28   11    2    2  917   20    1    6    3   15    0    2    2    3   15    3    8    1    4    1   10]
 [  11   11    4    2   15  876    8   32    3    5    3   10    8   22    3    1    5    4    3    7   10]
 [   4    5   25    8    2   21  966    7    0    2    9    1    2    0    0    3    5    4    4    9    9]
 [   2   12   12   11    1   82    5  866    1    4    5   10    2    5    4    1    0    5   22   18    9]
 [  13    8    1    2    3    5    0    2  808   73   10    3    4   21   24    0    1    4   12    2    6]
 [  73    1    0    0    2    2    0    1   48  827    3    1    1   12   11    0    1    5    2    2    9]
 [   3    7    6   34    3    8    2   11   24    4  884    0    4   21   19    0    2    1   16    1   14]
 [   3    4    2    0    5   38    9   11    1    1    3  780   33   15    1   17    1   39    2   40    6]
 [   1    2    3    9    1   12    2    9    2    2    0   85  737    4    5   13    4   78    6   10   10]
 [   3    1    3    0   10   36    5    1   21   42    6    7    1  830    7    5    2    6    0    7    8]
 [  13    3    2   13   18    2    0    4   37   11    2    3    2    7  947    1    3    3   19    1    7]
 [   5    2    6    6    8    5    7    1    0    0    1   22    5    6    3  936   12   21    1   10    9]
 [   4    6    7    8   14   20    3    3   11    3    3    7    1    2    1    7  927    5    5   11   24]
 [   2    1    2    5    2    3    2    2    2    2    0   24   17    9    4   16    1  894    2    2   13]
 [   4   13    4   42    5    3    2   41    9    0    6    2    3    1   22    0    0    1  890    5    5]
 [   1    6    3    2    3   29   18   27    1    0    1   11   12    4    0    6   15    1    3  933   12]
 [ 319  248  252  300  321  443  152  305  162  224  196  201  385  403  379  200  376  260  252  497 8057]]

2024-06-06 01:30:06,420 - ==> Best [Top1: 73.976   Top5: 95.594   Sparsity:0.00   Params: 169472 on epoch: 92]
2024-06-06 01:30:06,420 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:30:06,428 - 

2024-06-06 01:30:06,428 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:30:12,728 - Epoch: [97][  100/ 1218]    Overall Loss 0.627264    Objective Loss 0.627264                                        LR 0.001000    Time 0.062978    
2024-06-06 01:30:17,515 - Epoch: [97][  200/ 1218]    Overall Loss 0.630385    Objective Loss 0.630385                                        LR 0.001000    Time 0.055412    
2024-06-06 01:30:22,582 - Epoch: [97][  300/ 1218]    Overall Loss 0.629354    Objective Loss 0.629354                                        LR 0.001000    Time 0.053826    
2024-06-06 01:30:27,317 - Epoch: [97][  400/ 1218]    Overall Loss 0.628857    Objective Loss 0.628857                                        LR 0.001000    Time 0.052203    
2024-06-06 01:30:31,967 - Epoch: [97][  500/ 1218]    Overall Loss 0.632840    Objective Loss 0.632840                                        LR 0.001000    Time 0.051058    
2024-06-06 01:30:36,624 - Epoch: [97][  600/ 1218]    Overall Loss 0.638096    Objective Loss 0.638096                                        LR 0.001000    Time 0.050307    
2024-06-06 01:30:41,368 - Epoch: [97][  700/ 1218]    Overall Loss 0.641477    Objective Loss 0.641477                                        LR 0.001000    Time 0.049895    
2024-06-06 01:30:46,026 - Epoch: [97][  800/ 1218]    Overall Loss 0.642884    Objective Loss 0.642884                                        LR 0.001000    Time 0.049470    
2024-06-06 01:30:50,841 - Epoch: [97][  900/ 1218]    Overall Loss 0.643341    Objective Loss 0.643341                                        LR 0.001000    Time 0.049322    
2024-06-06 01:30:55,645 - Epoch: [97][ 1000/ 1218]    Overall Loss 0.640269    Objective Loss 0.640269                                        LR 0.001000    Time 0.049191    
2024-06-06 01:31:00,341 - Epoch: [97][ 1100/ 1218]    Overall Loss 0.639494    Objective Loss 0.639494                                        LR 0.001000    Time 0.048987    
2024-06-06 01:31:04,993 - Epoch: [97][ 1200/ 1218]    Overall Loss 0.640548    Objective Loss 0.640548                                        LR 0.001000    Time 0.048780    
2024-06-06 01:31:05,786 - Epoch: [97][ 1218/ 1218]    Overall Loss 0.640373    Objective Loss 0.640373    Top1 73.594132    Top5 95.599022    LR 0.001000    Time 0.048710    
2024-06-06 01:31:05,973 - --- validate (epoch=97)-----------
2024-06-06 01:31:05,973 - 34633 samples (256 per mini-batch)
2024-06-06 01:31:11,772 - Epoch: [97][  100/  136]    Loss 0.575782    Top1 73.589844    Top5 95.027344    
2024-06-06 01:31:13,432 - Epoch: [97][  136/  136]    Loss 0.574797    Top1 73.597436    Top5 95.097162    
2024-06-06 01:31:13,624 - ==> Top1: 73.597    Top5: 95.097    Loss: 0.575

2024-06-06 01:31:13,625 - ==> Confusion:
[[ 791    1    2    2   14    1    0    3    8   67    1    2    1    4    8    2    2    7    1    1   13]
 [   4  912    5    1   22   26    2   17   10    1    4    5    1    2   13    0   10    7   10    4    7]
 [  19    5  793    8    6    2   34    8    4   10    9    4    2    5    6    9    9    0   10    6   21]
 [   4    3   17  803   10   11    4    2    3    3   24    3    8    1   63    3    2    7   35    1    9]
 [  34   18    1    1  920    7    2    2    0   12    0    5    1    4   13    8    9    3    7    1    6]
 [   5   45    3    3   18  810    1   46    4    7    0   25    8   24    7    3   10    5    5    9    5]
 [   6    5   35    0    3    9  943   11    3    3    5   10    2    2    2   15    5    6    3   11    7]
 [   6   26   10    3    3   44    4  868    2    2    5   13    5    5    1    0    3    2   51   12   12]
 [  19    8    1    0    1    3    0    3  805   62    6    3    6   19   38    1    5    5    9    1    7]
 [ 115    0    0    0    4    4    0    4   69  743    2    1    0   16   20    0    1    6    2    2   12]
 [   0    6    6   10    2   10    2    7   36    7  897    3    1   23   18    0    5    0   20    1   10]
 [   3    1    1    1    1   14    3    5    1    0    1  875   28    8    0   16    3   22    2   12   14]
 [   0    2    2    3    3    6    5    2    5    0    2   95  759    2    6    8    5   58   12    9   11]
 [   3    1    1    1   11   17    1    4   18   26    2   13    9  841    9    4    6   13    2    4   15]
 [  14    3    1    3    8    2    0    2   40   10    4    4    4   10  962    1    1    5   14    0   10]
 [   3    3    3    3    4    2    5    0    0    0    0   26    9    1    3  958   13   20    4    3    6]
 [   9   11    3    3   10    6    1    1    7    2    1   11    6    5    4   18  945    2    2    5   20]
 [   1    1    1    1    0    2    1    1    4    2    0   33   30    4    6   13    3  881    9    1   11]
 [   4    4    6    5    5    4    1   18   17    0    4    7    3    1   33    3    1    1  930    2    9]
 [   5   12    4    0    4   22   17   17    4    0    2   61    9    5    1   11   18    5    6  865   20]
 [ 396  305  224  115  343  276  134  218  229  155  230  325  454  343  463  238  476  207  328  285 8188]]

2024-06-06 01:31:13,626 - ==> Best [Top1: 73.976   Top5: 95.594   Sparsity:0.00   Params: 169472 on epoch: 92]
2024-06-06 01:31:13,626 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:31:13,634 - 

2024-06-06 01:31:13,634 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:31:19,812 - Epoch: [98][  100/ 1218]    Overall Loss 0.652966    Objective Loss 0.652966                                        LR 0.001000    Time 0.061755    
2024-06-06 01:31:24,462 - Epoch: [98][  200/ 1218]    Overall Loss 0.645164    Objective Loss 0.645164                                        LR 0.001000    Time 0.054122    
2024-06-06 01:31:29,221 - Epoch: [98][  300/ 1218]    Overall Loss 0.638297    Objective Loss 0.638297                                        LR 0.001000    Time 0.051938    
2024-06-06 01:31:33,883 - Epoch: [98][  400/ 1218]    Overall Loss 0.638169    Objective Loss 0.638169                                        LR 0.001000    Time 0.050603    
2024-06-06 01:31:38,587 - Epoch: [98][  500/ 1218]    Overall Loss 0.637242    Objective Loss 0.637242                                        LR 0.001000    Time 0.049886    
2024-06-06 01:31:43,328 - Epoch: [98][  600/ 1218]    Overall Loss 0.637224    Objective Loss 0.637224                                        LR 0.001000    Time 0.049472    
2024-06-06 01:31:48,083 - Epoch: [98][  700/ 1218]    Overall Loss 0.639177    Objective Loss 0.639177                                        LR 0.001000    Time 0.049195    
2024-06-06 01:31:52,619 - Epoch: [98][  800/ 1218]    Overall Loss 0.638514    Objective Loss 0.638514                                        LR 0.001000    Time 0.048713    
2024-06-06 01:31:57,516 - Epoch: [98][  900/ 1218]    Overall Loss 0.638591    Objective Loss 0.638591                                        LR 0.001000    Time 0.048740    
2024-06-06 01:32:02,213 - Epoch: [98][ 1000/ 1218]    Overall Loss 0.637912    Objective Loss 0.637912                                        LR 0.001000    Time 0.048561    
2024-06-06 01:32:06,790 - Epoch: [98][ 1100/ 1218]    Overall Loss 0.638675    Objective Loss 0.638675                                        LR 0.001000    Time 0.048306    
2024-06-06 01:32:11,328 - Epoch: [98][ 1200/ 1218]    Overall Loss 0.638763    Objective Loss 0.638763                                        LR 0.001000    Time 0.048060    
2024-06-06 01:32:12,180 - Epoch: [98][ 1218/ 1218]    Overall Loss 0.639394    Objective Loss 0.639394    Top1 72.127139    Top5 92.909535    LR 0.001000    Time 0.048049    
2024-06-06 01:32:12,361 - --- validate (epoch=98)-----------
2024-06-06 01:32:12,361 - 34633 samples (256 per mini-batch)
2024-06-06 01:32:17,897 - Epoch: [98][  100/  136]    Loss 0.588893    Top1 73.339844    Top5 95.066406    
2024-06-06 01:32:19,586 - Epoch: [98][  136/  136]    Loss 0.592013    Top1 73.222071    Top5 95.062513    
2024-06-06 01:32:19,753 - ==> Top1: 73.222    Top5: 95.063    Loss: 0.592

2024-06-06 01:32:19,755 - ==> Confusion:
[[ 809    4    2    0    3    2    0    2   10   62    0    4    2    4    5    3    3    4    2    1    9]
 [   6  924    4    0   15   35    2   19    9    1    3    1    1    2    6    1    8    1    9    5   11]
 [  18   10  803    7    6    3   27   17    3    6    7    4    3   10    5    4   12    1    9    3   12]
 [   6    2   39  797    2   13    2    4    2    3   14    1   10   11   57    1    5    9   22    3   13]
 [  67   17    3    0  840   15    1    4    6   30    0    4    1    4   20   10    9    1    7    3   12]
 [  15   48    2    2   15  797    3   39    8    9    2   18    4   42    4    0    7    3    5   12    8]
 [   3    6   39    4    3   11  938   16    1    1    4    1    0    2    1    8    8    5    1   13   21]
 [   5   20   12    3    1   61    2  865    5    5   11    9    5    7    3    3    1    2   32   16    9]
 [  15    7    1    1    1    1    0    0  850   38   12    2    6   23   18    0    3    2   15    2    5]
 [ 107    2    2    2    4    4    0    2  112  722    0    0    3   13    8    0    2    3    5    1    9]
 [   4    9   14   15    1    8    4    8   27    1  897    0    1   23   13    0    3    0   22    2   12]
 [   5    2    4    0    0   20    2    8    1    3    3  822   35   26    3    8    8   39    5    8    9]
 [   1    2    1    8    0    8    2    6    5    0    1   79  759    8    1   11    7   66   13    5   12]
 [   6    0    2    0    4   15    1    3   23   18    8    5    5  869   12    3    6    6    2    2   11]
 [  20    2    4   11    7    1    0    3   71   12    4    2    4   12  906    1    4    4   18    1   11]
 [   3    1    6    1    3    4    5    1    1    1    1   26   11    7    2  933   21   25    3    1   10]
 [   9   15    2    1    6   11    3    3    7    0    0   11    2   10    4   11  943    2    1    9   22]
 [   5    3    0    2    0    2    2    1    1    1    1   24   25    7    2   11    2  901    1    4   10]
 [   4   16    7   16    2    4    1   33   16    1    6    5    4    2   33    0    2    4  886    4   12]
 [   2   11    2    3    2   16   16   31    4    1    0   46    6    8    1    8   17    9    5  892    8]
 [ 400  408  241  114  223  289  116  255  255  189  179  218  391  485  355  216  517  217  261  397 8206]]

2024-06-06 01:32:19,756 - ==> Best [Top1: 73.976   Top5: 95.594   Sparsity:0.00   Params: 169472 on epoch: 92]
2024-06-06 01:32:19,756 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:32:19,764 - 

2024-06-06 01:32:19,764 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:32:26,006 - Epoch: [99][  100/ 1218]    Overall Loss 0.658636    Objective Loss 0.658636                                        LR 0.001000    Time 0.062399    
2024-06-06 01:32:31,044 - Epoch: [99][  200/ 1218]    Overall Loss 0.647756    Objective Loss 0.647756                                        LR 0.001000    Time 0.056381    
2024-06-06 01:32:35,748 - Epoch: [99][  300/ 1218]    Overall Loss 0.647578    Objective Loss 0.647578                                        LR 0.001000    Time 0.053262    
2024-06-06 01:32:40,543 - Epoch: [99][  400/ 1218]    Overall Loss 0.645108    Objective Loss 0.645108                                        LR 0.001000    Time 0.051930    
2024-06-06 01:32:45,362 - Epoch: [99][  500/ 1218]    Overall Loss 0.640439    Objective Loss 0.640439                                        LR 0.001000    Time 0.051178    
2024-06-06 01:32:49,970 - Epoch: [99][  600/ 1218]    Overall Loss 0.639288    Objective Loss 0.639288                                        LR 0.001000    Time 0.050326    
2024-06-06 01:32:54,625 - Epoch: [99][  700/ 1218]    Overall Loss 0.639891    Objective Loss 0.639891                                        LR 0.001000    Time 0.049784    
2024-06-06 01:32:59,190 - Epoch: [99][  800/ 1218]    Overall Loss 0.639193    Objective Loss 0.639193                                        LR 0.001000    Time 0.049265    
2024-06-06 01:33:03,782 - Epoch: [99][  900/ 1218]    Overall Loss 0.641088    Objective Loss 0.641088                                        LR 0.001000    Time 0.048891    
2024-06-06 01:33:08,501 - Epoch: [99][ 1000/ 1218]    Overall Loss 0.640898    Objective Loss 0.640898                                        LR 0.001000    Time 0.048719    
2024-06-06 01:33:13,127 - Epoch: [99][ 1100/ 1218]    Overall Loss 0.639577    Objective Loss 0.639577                                        LR 0.001000    Time 0.048494    
2024-06-06 01:33:17,678 - Epoch: [99][ 1200/ 1218]    Overall Loss 0.639962    Objective Loss 0.639962                                        LR 0.001000    Time 0.048244    
2024-06-06 01:33:18,446 - Epoch: [99][ 1218/ 1218]    Overall Loss 0.639694    Objective Loss 0.639694    Top1 75.550122    Top5 96.821516    LR 0.001000    Time 0.048161    
2024-06-06 01:33:18,615 - --- validate (epoch=99)-----------
2024-06-06 01:33:18,615 - 34633 samples (256 per mini-batch)
2024-06-06 01:33:24,184 - Epoch: [99][  100/  136]    Loss 0.582746    Top1 72.246094    Top5 95.011719    
2024-06-06 01:33:25,869 - Epoch: [99][  136/  136]    Loss 0.585497    Top1 72.266336    Top5 94.978777    
2024-06-06 01:33:26,062 - ==> Top1: 72.266    Top5: 94.979    Loss: 0.585

2024-06-06 01:33:26,063 - ==> Confusion:
[[ 757    0    5    0    9    3    0    1   12   94    1    1    3    9   10    5    1    2    4    3   11]
 [   2  848    7    7   31   54    4   32    9    2    2    7    2    4   10    4   10    2   21    3    2]
 [  12    2  825   11    9    2   27    3    1    7    8    9    4    5    5    8    7    1    9    8    7]
 [   4    0   33  839    3    8    3    5    1    3   21    2   16    3   35    2    2    9   13    1   13]
 [  25    8    8    2  905   17    2    2    7   17    1    2    2    4   24    6    7    0    7    2    6]
 [   8   21    6    4   16  815   13   51    6    5    6   11    9   25    6    5    9    1    5   16    5]
 [   3    5   36    2    5   10  967    4    1    1    4    5    3    3    1    9    3    1    2   11   10]
 [   2    7   22    7    5   58   12  838    1    6    4    8    8    8    3    1    0    6   46   26    9]
 [   8    4    2    3    3    5    0    4  772   94   16    2    5   24   35    0    3    8    6    2    6]
 [  64    0    5    0    8    6    2    1   44  793    1    1    3   35   22    2    0    3    2    2    7]
 [   1    6   15   18    2    4    5    6   19    4  920    0    0   11   17    1    3    1   19    3    9]
 [   4    2    6    0    2   15    9   14    3    2    1  778   55   24    1   12    3   23    2   47    8]
 [   0    1    2    5    0    7    2    5    4    1    3   60  799   14    3    9    4   45    5   15   11]
 [   5    2    2    1    4   21    0    3   13   24   11   11   10  843    7    1    3    5    2   11   22]
 [  12    2    6   19   12    4    0    0   34   13   11    2    3   15  926    0    0    7   17    0   15]
 [   1    4    4    1    4    1   12    2    0    1    1   19    8    5    0  933   18   29    2    8   13]
 [   6   12   14    4    9   14    6    2    3    2    3    7   11    8    4   19  912    3    2   13   18]
 [   1    1    3    1    0    3    3    4    3    3    0   17   34    3    6   16    1  885    0    8   13]
 [   5    5    8   23    4    7    2   26   14    2    6    0    9    1   30    2    1    4  894    3   12]
 [   0    3    7    3    4   12   27   14    1    2    0   15    9    9    2    7    6    4    3  950   10]
 [ 305  222  430  188  336  382  203  225  175  220  224  201  512  475  427  217  417  217  243  484 7829]]

2024-06-06 01:33:26,064 - ==> Best [Top1: 73.976   Top5: 95.594   Sparsity:0.00   Params: 169472 on epoch: 92]
2024-06-06 01:33:26,064 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:33:26,072 - 

2024-06-06 01:33:26,072 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:33:32,286 - Epoch: [100][  100/ 1218]    Overall Loss 0.623384    Objective Loss 0.623384                                        LR 0.000500    Time 0.062120    
2024-06-06 01:33:37,009 - Epoch: [100][  200/ 1218]    Overall Loss 0.618905    Objective Loss 0.618905                                        LR 0.000500    Time 0.054663    
2024-06-06 01:33:41,747 - Epoch: [100][  300/ 1218]    Overall Loss 0.615495    Objective Loss 0.615495                                        LR 0.000500    Time 0.052229    
2024-06-06 01:33:46,330 - Epoch: [100][  400/ 1218]    Overall Loss 0.609124    Objective Loss 0.609124                                        LR 0.000500    Time 0.050625    
2024-06-06 01:33:50,953 - Epoch: [100][  500/ 1218]    Overall Loss 0.607015    Objective Loss 0.607015                                        LR 0.000500    Time 0.049743    
2024-06-06 01:33:55,636 - Epoch: [100][  600/ 1218]    Overall Loss 0.603900    Objective Loss 0.603900                                        LR 0.000500    Time 0.049254    
2024-06-06 01:34:00,362 - Epoch: [100][  700/ 1218]    Overall Loss 0.602079    Objective Loss 0.602079                                        LR 0.000500    Time 0.048967    
2024-06-06 01:34:04,984 - Epoch: [100][  800/ 1218]    Overall Loss 0.600744    Objective Loss 0.600744                                        LR 0.000500    Time 0.048621    
2024-06-06 01:34:09,771 - Epoch: [100][  900/ 1218]    Overall Loss 0.599722    Objective Loss 0.599722                                        LR 0.000500    Time 0.048536    
2024-06-06 01:34:14,344 - Epoch: [100][ 1000/ 1218]    Overall Loss 0.598666    Objective Loss 0.598666                                        LR 0.000500    Time 0.048253    
2024-06-06 01:34:18,912 - Epoch: [100][ 1100/ 1218]    Overall Loss 0.597229    Objective Loss 0.597229                                        LR 0.000500    Time 0.048018    
2024-06-06 01:34:23,545 - Epoch: [100][ 1200/ 1218]    Overall Loss 0.596443    Objective Loss 0.596443                                        LR 0.000500    Time 0.047876    
2024-06-06 01:34:24,330 - Epoch: [100][ 1218/ 1218]    Overall Loss 0.596827    Objective Loss 0.596827    Top1 74.816626    Top5 95.110024    LR 0.000500    Time 0.047812    
2024-06-06 01:34:24,503 - --- validate (epoch=100)-----------
2024-06-06 01:34:24,503 - 34633 samples (256 per mini-batch)
2024-06-06 01:34:30,103 - Epoch: [100][  100/  136]    Loss 0.541549    Top1 74.429688    Top5 95.156250    
2024-06-06 01:34:31,781 - Epoch: [100][  136/  136]    Loss 0.541325    Top1 74.371264    Top5 95.336817    
2024-06-06 01:34:31,978 - ==> Top1: 74.371    Top5: 95.337    Loss: 0.541

2024-06-06 01:34:31,979 - ==> Confusion:
[[ 770    3    1    0   15    3    0    2   15   89    0    5    0    5    8    0    3    3    3    2    4]
 [   4  893    0    0   25   32    4   27    1    6    2   14    5    1    7    3    4    2   19    3   11]
 [   5    4  805   17    3    6   31   14    3   11    6   11    3    4    7    4   10    1    7    8   10]
 [   4    1   22  861    3   12    5    3    3    2   13    3    2    5   37    5    3   10   16    0    6]
 [  20   17    1    0  907   12    2    5    5   24    3    3    3    6   18    4    6    3    5    1    9]
 [   4   45    3    3   17  835    2   41    4    5    3   18    6   25    2    1    0    4   10    5   10]
 [   2    5   26    2    4    6  967   11    3    3    7    3    1    2    0   11    3    6    4   14    6]
 [   1   15    9    6    4   67    9  861    6    4    7   12    4    3    0    0    1    5   39   13   11]
 [   9    3    1    4    3    4    1    2  820   58   13    4    8   24   24    0    2    3   12    0    7]
 [  63    2    0    0   14    6    0    1   66  797    0    1    1   22    9    1    2    2    8    1    5]
 [   1    3   13   16    5    6    9    5   22    4  907    4    1   16   16    0    2    0   20    3   11]
 [   3    1    2    1    2   18    5    5    1    0    0  862   35   15    1   12   10   16    0   16    6]
 [   1    0    0    9    1    8    3    6    6    0    3  105  762    0    8    5    2   55   10    4    7]
 [   3    1    0    0    3   22    0    5   24   29    5   20    3  851    7    4    0    4    1   12    7]
 [  14    5    2   25   16    2    0    2   45   20    6    3    6    8  909    0    2    3   13    2   15]
 [   3    1    0    1    4    0    9    2    1    4    0   24    6    3    2  962    5   22    1    4   12]
 [   5   13    3    2    8   16    2    0    7    1    1   12    3    5    3   20  939    2    4    8   18]
 [   3    1    1    4    1    1    2    1    0    5    0   32   33    4    5   17    1  881    1    3    9]
 [   2   10    8   23    3    4    1   34   14    2    8    5    2    2   18    1    2    0  900    4   15]
 [   0    5    2    0    2   18   20   14    1    0    0   45   14    5    0    6    6    6    9  925   10]
 [ 267  314  287  163  332  317  135  254  194  182  207  387  467  401  337  207  335  181  255  367 8343]]

2024-06-06 01:34:31,981 - ==> Best [Top1: 74.371   Top5: 95.337   Sparsity:0.00   Params: 169472 on epoch: 100]
2024-06-06 01:34:31,981 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:34:31,996 - 

2024-06-06 01:34:31,996 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:34:38,207 - Epoch: [101][  100/ 1218]    Overall Loss 0.574182    Objective Loss 0.574182                                        LR 0.000500    Time 0.062085    
2024-06-06 01:34:42,881 - Epoch: [101][  200/ 1218]    Overall Loss 0.582866    Objective Loss 0.582866                                        LR 0.000500    Time 0.054404    
2024-06-06 01:34:47,467 - Epoch: [101][  300/ 1218]    Overall Loss 0.584765    Objective Loss 0.584765                                        LR 0.000500    Time 0.051550    
2024-06-06 01:34:52,281 - Epoch: [101][  400/ 1218]    Overall Loss 0.586331    Objective Loss 0.586331                                        LR 0.000500    Time 0.050694    
2024-06-06 01:34:56,870 - Epoch: [101][  500/ 1218]    Overall Loss 0.586904    Objective Loss 0.586904                                        LR 0.000500    Time 0.049728    
2024-06-06 01:35:01,702 - Epoch: [101][  600/ 1218]    Overall Loss 0.585797    Objective Loss 0.585797                                        LR 0.000500    Time 0.049492    
2024-06-06 01:35:06,271 - Epoch: [101][  700/ 1218]    Overall Loss 0.585123    Objective Loss 0.585123                                        LR 0.000500    Time 0.048946    
2024-06-06 01:35:11,153 - Epoch: [101][  800/ 1218]    Overall Loss 0.585442    Objective Loss 0.585442                                        LR 0.000500    Time 0.048928    
2024-06-06 01:35:15,817 - Epoch: [101][  900/ 1218]    Overall Loss 0.586482    Objective Loss 0.586482                                        LR 0.000500    Time 0.048672    
2024-06-06 01:35:20,381 - Epoch: [101][ 1000/ 1218]    Overall Loss 0.585728    Objective Loss 0.585728                                        LR 0.000500    Time 0.048366    
2024-06-06 01:35:25,190 - Epoch: [101][ 1100/ 1218]    Overall Loss 0.586431    Objective Loss 0.586431                                        LR 0.000500    Time 0.048340    
2024-06-06 01:35:29,820 - Epoch: [101][ 1200/ 1218]    Overall Loss 0.586986    Objective Loss 0.586986                                        LR 0.000500    Time 0.048168    
2024-06-06 01:35:30,608 - Epoch: [101][ 1218/ 1218]    Overall Loss 0.587173    Objective Loss 0.587173    Top1 72.371638    Top5 95.599022    LR 0.000500    Time 0.048103    
2024-06-06 01:35:30,808 - --- validate (epoch=101)-----------
2024-06-06 01:35:30,808 - 34633 samples (256 per mini-batch)
2024-06-06 01:35:36,330 - Epoch: [101][  100/  136]    Loss 0.539434    Top1 74.242188    Top5 95.554688    
2024-06-06 01:35:38,208 - Epoch: [101][  136/  136]    Loss 0.539632    Top1 74.365490    Top5 95.608235    
2024-06-06 01:35:38,388 - ==> Top1: 74.365    Top5: 95.608    Loss: 0.540

2024-06-06 01:35:38,390 - ==> Confusion:
[[ 749    0    2    3   25    0    0    2   14   91    1    4    5    3   11    2    2    1    4    2   10]
 [   3  902    3    2   27   43    4   11    3    3    5    8    2    2    7    1    5    3   14    3   12]
 [   8    4  794   28    5    5   29   14    3    6   13    6    5   10    4    7    7    2    8    3    9]
 [   0    3   13  853    7   10    4    3    2    0   28    1   14    4   34    4    2    5   21    3    5]
 [  17    9    3    2  936   10    0    2    1   14    2    5    3    4   17    6    8    3    1    1   10]
 [   4   32    2    7   20  860    6   25    2    4    1   21   10   18    3    2    6    1    6    4    9]
 [   4    4   20    2    4    7  967    7    2    1    9    8    1    3    0    9    5    2    3   15   13]
 [   3   18   14    3   11   64   10  827    0    1    7   19    9    3    0    1    2    1   53   17   14]
 [  15    5    1    2    0    6    0    2  815   45   18    3   10   16   43    1    2    2    9    1    6]
 [  60    3    2    1   12    9    0    3   75  779    5    3    0   26    9    0    1    4    2    0    7]
 [   0    3    9   19    1    5    7   10   21    3  929    2    4   11   11    0    2    1   21    1    4]
 [   0    0    3    1    1    9    3    5    4    1    2  874   29    9    1    9    8   17    5   19   11]
 [   1    2    0   10    1    3    1    4    2    0    2   92  818    1    2    5    4   26    4    8    9]
 [   3    1    0    1    6   17    1    8   16   18   12   20    9  846    9    4    3    5    3    7   12]
 [   7    2    1   16   10    6    2    3   43    7    6    5    2    5  961    1    0    3   12    0    6]
 [   4    0    3    2    3    3    9    0    0    0    1   29   16    2    1  947   12   15    2    7   10]
 [   2   14    2    2   12   12    3    1    5    0    2   20   10    1    2   11  932    6    0    6   29]
 [   5    1    2    3    1    0    1    1    2    2    0   35   47    5    8   27    1  851    5    5    3]
 [   0    6    3   31    6    4    2   16   16    2   11    3    4    3   20    1    0    1  915    7    7]
 [   1    4    2    0    3   16   22   12    2    1    1   37   13   10    1    9   14    5    5  913   17]
 [ 244  233  221  223  383  303  135  193  204  137  263  344  528  355  380  255  409  155  308  372 8287]]

2024-06-06 01:35:38,391 - ==> Best [Top1: 74.371   Top5: 95.337   Sparsity:0.00   Params: 169472 on epoch: 100]
2024-06-06 01:35:38,391 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:35:38,399 - 

2024-06-06 01:35:38,399 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:35:44,795 - Epoch: [102][  100/ 1218]    Overall Loss 0.576495    Objective Loss 0.576495                                        LR 0.000500    Time 0.063933    
2024-06-06 01:35:49,522 - Epoch: [102][  200/ 1218]    Overall Loss 0.580153    Objective Loss 0.580153                                        LR 0.000500    Time 0.055595    
2024-06-06 01:35:54,144 - Epoch: [102][  300/ 1218]    Overall Loss 0.582338    Objective Loss 0.582338                                        LR 0.000500    Time 0.052463    
2024-06-06 01:35:58,950 - Epoch: [102][  400/ 1218]    Overall Loss 0.582215    Objective Loss 0.582215                                        LR 0.000500    Time 0.051358    
2024-06-06 01:36:03,832 - Epoch: [102][  500/ 1218]    Overall Loss 0.580530    Objective Loss 0.580530                                        LR 0.000500    Time 0.050847    
2024-06-06 01:36:08,545 - Epoch: [102][  600/ 1218]    Overall Loss 0.578836    Objective Loss 0.578836                                        LR 0.000500    Time 0.050225    
2024-06-06 01:36:13,246 - Epoch: [102][  700/ 1218]    Overall Loss 0.582621    Objective Loss 0.582621                                        LR 0.000500    Time 0.049763    
2024-06-06 01:36:17,883 - Epoch: [102][  800/ 1218]    Overall Loss 0.582161    Objective Loss 0.582161                                        LR 0.000500    Time 0.049337    
2024-06-06 01:36:22,467 - Epoch: [102][  900/ 1218]    Overall Loss 0.583507    Objective Loss 0.583507                                        LR 0.000500    Time 0.048947    
2024-06-06 01:36:27,442 - Epoch: [102][ 1000/ 1218]    Overall Loss 0.583440    Objective Loss 0.583440                                        LR 0.000500    Time 0.049026    
2024-06-06 01:36:32,452 - Epoch: [102][ 1100/ 1218]    Overall Loss 0.583900    Objective Loss 0.583900                                        LR 0.000500    Time 0.049122    
2024-06-06 01:36:37,094 - Epoch: [102][ 1200/ 1218]    Overall Loss 0.583971    Objective Loss 0.583971                                        LR 0.000500    Time 0.048895    
2024-06-06 01:36:37,857 - Epoch: [102][ 1218/ 1218]    Overall Loss 0.584136    Objective Loss 0.584136    Top1 73.349633    Top5 95.354523    LR 0.000500    Time 0.048798    
2024-06-06 01:36:38,036 - --- validate (epoch=102)-----------
2024-06-06 01:36:38,036 - 34633 samples (256 per mini-batch)
2024-06-06 01:36:43,537 - Epoch: [102][  100/  136]    Loss 0.540262    Top1 74.316406    Top5 95.359375    
2024-06-06 01:36:45,229 - Epoch: [102][  136/  136]    Loss 0.536713    Top1 74.316403    Top5 95.339705    
2024-06-06 01:36:45,428 - ==> Top1: 74.316    Top5: 95.340    Loss: 0.537

2024-06-06 01:36:45,429 - ==> Confusion:
[[ 793    1    3    0   12    6    3    3    4   79    0    4    2    4    5    2    1    1    3    1    4]
 [   5  889    5    1   42   23    7   26    3    3    1    7    3    5    5    2    6    0   10   11    9]
 [   8    3  806   13    4    6   35   19    4    7    8    5    3    8    4    5    8    1    2   12    9]
 [   1    3   23  867    4   12    6    0    2    6   18    3    8    5   21    3    0    5   11    4   14]
 [  14   11    5    2  955    5    4    8    3    7    0    1    1    3    5    6    9    1    3    3    8]
 [   4   41    5    5   24  807    4   46    0    6    2   18    8   28    5    4    5    2    4   14   11]
 [   4    8   25    1    1    4  982    6    0    2    1    2    2    2    3   10    3    2    1   18    9]
 [   2   22   12    2    6   35    7  900    0    4    0   14    8    6    3    1    2    2   24   25    2]
 [  11    4    2    0    3    0    0    4  780   96   13    2    7   31   30    0    3    1    4    3    8]
 [  87    3    2    1   10    6    0    3   25  824    0    1    1   19    8    1    0    1    3    1    5]
 [   1    6   15   18    3    4    4    3   19    8  920    3    1   19    8    2    1    0   17    0   12]
 [   4    2    3    1    3    9    7    7    1    1    2  872   25    9    1   18    6   21    1   13    5]
 [   3    3    1   11    1    4    3    4    2    0    0   95  775    6    4   12    7   33    7   14   10]
 [   2    3    2    1   13    8    2    5   13   30    7   15    2  856    5    3    3    5    2   14   10]
 [  10    8    1   23   26    5    2    2   40   16    1    1    1   11  920    0    1    3   16    2    9]
 [   1    0    3    1    7    1    7    0    0    1    0   24    9    1    1  974    8   15    2    4    7]
 [   1   16    8    2   23   14    2    2    5    1    5    6    5    5    1   12  934    3    1   13   13]
 [   2    3    3    4    0    2    3    1    2    3    0   30   50    2    7   25    0  854    1    5    8]
 [   4   15   13   32    7    4    1   33   10    1    5    2    7    1   29    0    0    1  877    9    7]
 [   3   10    4    0    5    8   17   12    0    0    1   35   16    5    0   10    6    4    1  936   15]
 [ 328  316  335  201  458  222  164  269  125  221  205  266  392  444  290  264  390  143  207  476 8216]]

2024-06-06 01:36:45,431 - ==> Best [Top1: 74.371   Top5: 95.337   Sparsity:0.00   Params: 169472 on epoch: 100]
2024-06-06 01:36:45,431 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:36:45,440 - 

2024-06-06 01:36:45,441 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:36:51,579 - Epoch: [103][  100/ 1218]    Overall Loss 0.575649    Objective Loss 0.575649                                        LR 0.000500    Time 0.061366    
2024-06-06 01:36:56,232 - Epoch: [103][  200/ 1218]    Overall Loss 0.580949    Objective Loss 0.580949                                        LR 0.000500    Time 0.053939    
2024-06-06 01:37:01,045 - Epoch: [103][  300/ 1218]    Overall Loss 0.578641    Objective Loss 0.578641                                        LR 0.000500    Time 0.051995    
2024-06-06 01:37:06,125 - Epoch: [103][  400/ 1218]    Overall Loss 0.575698    Objective Loss 0.575698                                        LR 0.000500    Time 0.051692    
2024-06-06 01:37:11,168 - Epoch: [103][  500/ 1218]    Overall Loss 0.575259    Objective Loss 0.575259                                        LR 0.000500    Time 0.051436    
2024-06-06 01:37:15,981 - Epoch: [103][  600/ 1218]    Overall Loss 0.575757    Objective Loss 0.575757                                        LR 0.000500    Time 0.050882    
2024-06-06 01:37:20,618 - Epoch: [103][  700/ 1218]    Overall Loss 0.578111    Objective Loss 0.578111                                        LR 0.000500    Time 0.050235    
2024-06-06 01:37:25,208 - Epoch: [103][  800/ 1218]    Overall Loss 0.578251    Objective Loss 0.578251                                        LR 0.000500    Time 0.049691    
2024-06-06 01:37:30,120 - Epoch: [103][  900/ 1218]    Overall Loss 0.577753    Objective Loss 0.577753                                        LR 0.000500    Time 0.049625    
2024-06-06 01:37:34,831 - Epoch: [103][ 1000/ 1218]    Overall Loss 0.578911    Objective Loss 0.578911                                        LR 0.000500    Time 0.049372    
2024-06-06 01:37:39,489 - Epoch: [103][ 1100/ 1218]    Overall Loss 0.579377    Objective Loss 0.579377                                        LR 0.000500    Time 0.049117    
2024-06-06 01:37:44,016 - Epoch: [103][ 1200/ 1218]    Overall Loss 0.578694    Objective Loss 0.578694                                        LR 0.000500    Time 0.048794    
2024-06-06 01:37:44,790 - Epoch: [103][ 1218/ 1218]    Overall Loss 0.578493    Objective Loss 0.578493    Top1 77.995110    Top5 97.555012    LR 0.000500    Time 0.048708    
2024-06-06 01:37:44,995 - --- validate (epoch=103)-----------
2024-06-06 01:37:44,995 - 34633 samples (256 per mini-batch)
2024-06-06 01:37:50,541 - Epoch: [103][  100/  136]    Loss 0.535358    Top1 75.589844    Top5 95.726562    
2024-06-06 01:37:52,263 - Epoch: [103][  136/  136]    Loss 0.530128    Top1 75.676378    Top5 95.749718    
2024-06-06 01:37:52,457 - ==> Top1: 75.676    Top5: 95.750    Loss: 0.530

2024-06-06 01:37:52,458 - ==> Confusion:
[[ 788    1    0    0   11    2    2    1    9   67    0    6    3    3   10    3    4    4    5    0   12]
 [   5  916    5    0   21   24    4   11    4    3    1    5    4    0    6    1    9    6   24    3   11]
 [   3    2  818   16   10    3   29    6    0    5   11    5    5    4    7    6    6    1   14    6   13]
 [   2    5   24  848    1    7    6    1    0    2   19    6   17    4   39    1    1    7   18    1    7]
 [  24   16    0    2  918    8    2    1    1   15    0    3    2    7   18    4   10    3    8    1   11]
 [   3   42    2    8   19  798    3   40    1    6    2   15   11   24    4    4   10    6   10   22   13]
 [   1    4   25    1    5    4  974    4    2    0    0    6    3    6    0   15    3    5    6   11   11]
 [   3   21   30    2    5   37   11  839    1    2    4   12    6    5    4    1    4    7   59   18    6]
 [  15    9    2    5    0    3    0    2  788   57   22    3    7   21   34    0    2    6   10    2   14]
 [  83    0    2    1   15    6    0    1   45  788    1    0    2   17   18    1    2    4    5    1    9]
 [   0    8    9   19    1    7    7    7   14    4  933    2    2    9    9    0    3    1   18    4    7]
 [   3    5    3    1    2   17    2    2    2    3    0  840   48    6    0   20    7   14    3   18   15]
 [   1    0    3    6    0    2    4    3    0    0    1   70  804    5    5    8   10   39    9   11   14]
 [   4    1    4    3    4   11    2    4   13   22   10   16    8  852    9    3    6    8    1   10   10]
 [  11    5    5   24    9    1    0    1   21   10   12    2    6    5  933    1    6    4   24    4   14]
 [   0    1    4    1    5    2   12    0    1    0    0   12    9    4    0  977   11   16    3    2    6]
 [   1   11    5    3   11    6    1    0    3    1    2   11    5    2    2   18  963    2    1   10   14]
 [   3    0    1    4    0    3    1    2    0    2    0   20   34    0    7   13    2  893    6    7    7]
 [   5    5    8   18    4    2    1   20    3    2    6    0    5    2   23    2    2    2  932    4   12]
 [   1   10    5    0    1   14   21   10    1    0    0   30   12    3    1    7   13    4    8  936   11]
 [ 228  304  292  178  269  235  129  183  112  157  202  258  458  310  346  260  380  214  335  411 8671]]

2024-06-06 01:37:52,460 - ==> Best [Top1: 75.676   Top5: 95.750   Sparsity:0.00   Params: 169472 on epoch: 103]
2024-06-06 01:37:52,460 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:37:52,469 - 

2024-06-06 01:37:52,469 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:37:58,847 - Epoch: [104][  100/ 1218]    Overall Loss 0.571588    Objective Loss 0.571588                                        LR 0.000500    Time 0.063758    
2024-06-06 01:38:03,784 - Epoch: [104][  200/ 1218]    Overall Loss 0.578594    Objective Loss 0.578594                                        LR 0.000500    Time 0.056552    
2024-06-06 01:38:08,352 - Epoch: [104][  300/ 1218]    Overall Loss 0.571669    Objective Loss 0.571669                                        LR 0.000500    Time 0.052923    
2024-06-06 01:38:12,986 - Epoch: [104][  400/ 1218]    Overall Loss 0.574435    Objective Loss 0.574435                                        LR 0.000500    Time 0.051274    
2024-06-06 01:38:17,541 - Epoch: [104][  500/ 1218]    Overall Loss 0.575246    Objective Loss 0.575246                                        LR 0.000500    Time 0.050125    
2024-06-06 01:38:22,273 - Epoch: [104][  600/ 1218]    Overall Loss 0.574657    Objective Loss 0.574657                                        LR 0.000500    Time 0.049654    
2024-06-06 01:38:27,009 - Epoch: [104][  700/ 1218]    Overall Loss 0.573773    Objective Loss 0.573773                                        LR 0.000500    Time 0.049325    
2024-06-06 01:38:31,606 - Epoch: [104][  800/ 1218]    Overall Loss 0.575599    Objective Loss 0.575599                                        LR 0.000500    Time 0.048902    
2024-06-06 01:38:36,532 - Epoch: [104][  900/ 1218]    Overall Loss 0.574077    Objective Loss 0.574077                                        LR 0.000500    Time 0.048940    
2024-06-06 01:38:41,288 - Epoch: [104][ 1000/ 1218]    Overall Loss 0.573972    Objective Loss 0.573972                                        LR 0.000500    Time 0.048800    
2024-06-06 01:38:46,300 - Epoch: [104][ 1100/ 1218]    Overall Loss 0.573540    Objective Loss 0.573540                                        LR 0.000500    Time 0.048919    
2024-06-06 01:38:51,026 - Epoch: [104][ 1200/ 1218]    Overall Loss 0.573077    Objective Loss 0.573077                                        LR 0.000500    Time 0.048779    
2024-06-06 01:38:51,813 - Epoch: [104][ 1218/ 1218]    Overall Loss 0.572661    Objective Loss 0.572661    Top1 74.572127    Top5 96.088020    LR 0.000500    Time 0.048704    
2024-06-06 01:38:52,011 - --- validate (epoch=104)-----------
2024-06-06 01:38:52,012 - 34633 samples (256 per mini-batch)
2024-06-06 01:38:57,561 - Epoch: [104][  100/  136]    Loss 0.516866    Top1 75.906250    Top5 96.101562    
2024-06-06 01:38:59,257 - Epoch: [104][  136/  136]    Loss 0.515018    Top1 75.924696    Top5 96.012474    
2024-06-06 01:38:59,437 - ==> Top1: 75.925    Top5: 96.012    Loss: 0.515

2024-06-06 01:38:59,438 - ==> Confusion:
[[ 790    1    4    0   20    7    1    1    8   69    0    0    1    7    3    3    1    3    1    2    9]
 [   4  905    4    3   28   38    4   14    4    1    6    4    2    3    6    1    8    3   14    4    7]
 [   9    7  810    9    8    3   48    9    0    3    8    7    4    8    4    4    5    1    5    5   13]
 [   5    7   16  840    5    8    7    2    1    1   28    3   13    4   39    1    1    9   13    2   11]
 [  26   13    4    0  928   13    2    3    2   16    3    5    1    3   12    3    2    0    5    0   13]
 [   5   36    2    2   12  878    4   29    2    6    1   14    4   17    1    2    6    1    2    5   14]
 [   3    9   24    0    1    8  983    5    1    0    4   11    0    2    0    9    3    2    2    6   13]
 [   3   13   10    5    3   68   14  853    1    2    4   17    5    4    2    1    0    3   36   20   13]
 [  17    5    2    1    0    6    1    1  805   56   14    2    8   24   39    0    3    1    6    1   10]
 [  77    3    2    0    8    2    1    1   44  810    1    2    3   26    9    2    1    1    2    1    5]
 [   0    1    9   15    2    8    7   10   15    2  947    1    2   12   13    0    4    0    9    3    4]
 [   1    2    0    0    0   16    2    4    0    2    0  853   50   17    0   13    5   20    1   16    9]
 [   1    5    5    5    1    8    2    7    3    0    2   90  802    4    3    4    2   30    1    8   12]
 [   2    4    2    0    9   19    1    3   17   18   10   12    3  880    3    1    0    3    4    5    5]
 [  11    5    0   15   11    5    0    2   25   11    9    1    7   10  951    1    0    5   15    1   13]
 [   1    0    7    0    5    3   10    0    0    0    1   24   14    2    0  957   15   14    2    5    6]
 [   6   12    5    1    9   16    3    0    4    0    5   19    4    6    4   13  935    1    1   12   16]
 [   3    4    1    1    1    3    0    2    1    0    1   39   50    7    6   12    1  856    4    4    9]
 [   3    7    9   22    7    7    0   32   10    0   12    3    5    1   31    0    4    2  889    5    9]
 [   2    6    5    0    2   15   21   12    3    0    0   34   13    6    1    6    9    0    1  938   14]
 [ 273  291  263  119  330  358  163  234  137  146  216  289  481  396  325  178  370  135  198  345 8685]]

2024-06-06 01:38:59,440 - ==> Best [Top1: 75.925   Top5: 96.012   Sparsity:0.00   Params: 169472 on epoch: 104]
2024-06-06 01:38:59,440 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:38:59,455 - 

2024-06-06 01:38:59,455 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:39:05,753 - Epoch: [105][  100/ 1218]    Overall Loss 0.566358    Objective Loss 0.566358                                        LR 0.000500    Time 0.062950    
2024-06-06 01:39:10,400 - Epoch: [105][  200/ 1218]    Overall Loss 0.564286    Objective Loss 0.564286                                        LR 0.000500    Time 0.054705    
2024-06-06 01:39:15,021 - Epoch: [105][  300/ 1218]    Overall Loss 0.572015    Objective Loss 0.572015                                        LR 0.000500    Time 0.051867    
2024-06-06 01:39:19,612 - Epoch: [105][  400/ 1218]    Overall Loss 0.574201    Objective Loss 0.574201                                        LR 0.000500    Time 0.050373    
2024-06-06 01:39:24,581 - Epoch: [105][  500/ 1218]    Overall Loss 0.572150    Objective Loss 0.572150                                        LR 0.000500    Time 0.050232    
2024-06-06 01:39:29,216 - Epoch: [105][  600/ 1218]    Overall Loss 0.573759    Objective Loss 0.573759                                        LR 0.000500    Time 0.049582    
2024-06-06 01:39:33,998 - Epoch: [105][  700/ 1218]    Overall Loss 0.575218    Objective Loss 0.575218                                        LR 0.000500    Time 0.049328    
2024-06-06 01:39:38,584 - Epoch: [105][  800/ 1218]    Overall Loss 0.575218    Objective Loss 0.575218                                        LR 0.000500    Time 0.048893    
2024-06-06 01:39:43,131 - Epoch: [105][  900/ 1218]    Overall Loss 0.574789    Objective Loss 0.574789                                        LR 0.000500    Time 0.048511    
2024-06-06 01:39:47,760 - Epoch: [105][ 1000/ 1218]    Overall Loss 0.574446    Objective Loss 0.574446                                        LR 0.000500    Time 0.048286    
2024-06-06 01:39:52,381 - Epoch: [105][ 1100/ 1218]    Overall Loss 0.573952    Objective Loss 0.573952                                        LR 0.000500    Time 0.048096    
2024-06-06 01:39:57,006 - Epoch: [105][ 1200/ 1218]    Overall Loss 0.574720    Objective Loss 0.574720                                        LR 0.000500    Time 0.047941    
2024-06-06 01:39:57,782 - Epoch: [105][ 1218/ 1218]    Overall Loss 0.574660    Objective Loss 0.574660    Top1 77.261614    Top5 96.821516    LR 0.000500    Time 0.047869    
2024-06-06 01:39:57,968 - --- validate (epoch=105)-----------
2024-06-06 01:39:57,968 - 34633 samples (256 per mini-batch)
2024-06-06 01:40:03,471 - Epoch: [105][  100/  136]    Loss 0.539845    Top1 74.796875    Top5 95.628906    
2024-06-06 01:40:05,295 - Epoch: [105][  136/  136]    Loss 0.534255    Top1 74.888112    Top5 95.720844    
2024-06-06 01:40:05,468 - ==> Top1: 74.888    Top5: 95.721    Loss: 0.534

2024-06-06 01:40:05,469 - ==> Confusion:
[[ 822    0    6    1   12    4    0    0    4   49    0    5    2    3    4    3    3    0    5    1    7]
 [   4  912    3    3   18   35    2   25    7    1    3    3    1    0    5    2   11    0   18    5    5]
 [   3    4  839    7    4    4   22   19    3    6    5    4    5    5    5    7    3    0    8    5   12]
 [   6    2   25  824    6   10    3    3    5    1   16    2   15    4   46    1    3    6   29    3    6]
 [  38   16    5    2  892   14    1    4    2   17    1    1    2    5   14    4   12    3    8    3   10]
 [   5   41    5    4   15  821    2   50    8    2    3   19    8   21    5    4    7    3    3    9    8]
 [   3    8   41    2    4    6  953   11    0    1    4    6    3    2    1    4    3    5    4   12   13]
 [   3   16   14    0    2   42    4  898    1    2    4   12    5    2    3    1    1    1   37   21    8]
 [  21    2    1    0    0    5    0    0  833   43    9    2    6   13   28    0    5    4   18    0   12]
 [ 122    4    2    0    5    4    1    4   60  748    1    1    3   20   10    0    1    3    4    0    8]
 [   3   10   10   12    1    8    5   11   15    2  914    4    4   15   22    0    2    0   23    1    2]
 [   3    2    3    1    1   15    7   10    1    1    0  853   36    6    1   12    5   19    8   22    5]
 [   3    2    4    2    0    7    3    4    4    0    0   81  799    2    3    5    5   41   14    9    7]
 [   2    1    1    1    4   12    0    6   22   25    6   13   11  845    6    5    8    5    5   10   13]
 [  16    5    2   15   10    2    0    3   36    9    3    0    4    7  945    0    1    4   27    1    8]
 [   2    2    3    2    4    2   10    0    0    0    0   28   20    1    1  933   13   18    1   13   13]
 [   5   11    5    2   11   10    0    0    5    0    1   11    6    2    3   19  942    2    6    8   23]
 [   1    2    1    4    1    6    0    0    2    3    1   27   40    5    3   12    3  871    3    5   15]
 [   1    4    2   12    2    5    0   32   10    1    3    0    5    1   14    2    2    3  951    3    5]
 [   1    9    6    0    1   22   18   18    2    1    1   26    9    1    0    7    8    2    3  940   13]
 [ 331  365  300  130  287  275  131  325  192  136  181  242  488  306  366  178  387  174  299  438 8401]]

2024-06-06 01:40:05,471 - ==> Best [Top1: 75.925   Top5: 96.012   Sparsity:0.00   Params: 169472 on epoch: 104]
2024-06-06 01:40:05,471 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:40:05,479 - 

2024-06-06 01:40:05,479 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:40:11,525 - Epoch: [106][  100/ 1218]    Overall Loss 0.566279    Objective Loss 0.566279                                        LR 0.000500    Time 0.060439    
2024-06-06 01:40:16,183 - Epoch: [106][  200/ 1218]    Overall Loss 0.574006    Objective Loss 0.574006                                        LR 0.000500    Time 0.053501    
2024-06-06 01:40:20,827 - Epoch: [106][  300/ 1218]    Overall Loss 0.572636    Objective Loss 0.572636                                        LR 0.000500    Time 0.051140    
2024-06-06 01:40:25,361 - Epoch: [106][  400/ 1218]    Overall Loss 0.571945    Objective Loss 0.571945                                        LR 0.000500    Time 0.049688    
2024-06-06 01:40:29,924 - Epoch: [106][  500/ 1218]    Overall Loss 0.572374    Objective Loss 0.572374                                        LR 0.000500    Time 0.048873    
2024-06-06 01:40:34,592 - Epoch: [106][  600/ 1218]    Overall Loss 0.569803    Objective Loss 0.569803                                        LR 0.000500    Time 0.048505    
2024-06-06 01:40:39,422 - Epoch: [106][  700/ 1218]    Overall Loss 0.567947    Objective Loss 0.567947                                        LR 0.000500    Time 0.048472    
2024-06-06 01:40:44,092 - Epoch: [106][  800/ 1218]    Overall Loss 0.567795    Objective Loss 0.567795                                        LR 0.000500    Time 0.048247    
2024-06-06 01:40:48,845 - Epoch: [106][  900/ 1218]    Overall Loss 0.567656    Objective Loss 0.567656                                        LR 0.000500    Time 0.048166    
2024-06-06 01:40:53,628 - Epoch: [106][ 1000/ 1218]    Overall Loss 0.569189    Objective Loss 0.569189                                        LR 0.000500    Time 0.048130    
2024-06-06 01:40:58,455 - Epoch: [106][ 1100/ 1218]    Overall Loss 0.570812    Objective Loss 0.570812                                        LR 0.000500    Time 0.048141    
2024-06-06 01:41:03,041 - Epoch: [106][ 1200/ 1218]    Overall Loss 0.571941    Objective Loss 0.571941                                        LR 0.000500    Time 0.047950    
2024-06-06 01:41:03,854 - Epoch: [106][ 1218/ 1218]    Overall Loss 0.571934    Objective Loss 0.571934    Top1 74.327628    Top5 96.577017    LR 0.000500    Time 0.047909    
2024-06-06 01:41:04,040 - --- validate (epoch=106)-----------
2024-06-06 01:41:04,041 - 34633 samples (256 per mini-batch)
2024-06-06 01:41:09,995 - Epoch: [106][  100/  136]    Loss 0.518652    Top1 75.960938    Top5 95.617188    
2024-06-06 01:41:11,803 - Epoch: [106][  136/  136]    Loss 0.520173    Top1 75.809199    Top5 95.585136    
2024-06-06 01:41:11,982 - ==> Top1: 75.809    Top5: 95.585    Loss: 0.520

2024-06-06 01:41:11,983 - ==> Confusion:
[[ 811    0    3    7    7    3    0    0   11   58    0    1    4    4    6    2    1    1    3    0    9]
 [   3  890    3    3   20   36    2   19    8    2   10    3    4    5    6    3   10    3   14    4   15]
 [  10    2  840   12    5    2   16    7    1    7   11    4    4    2    2   10    6    3    7    4   15]
 [   4    1   23  874    2    3    4    3    5    3   22    1   13    2   26    1    4    5   11    2    7]
 [  38   11    1    0  899    9    1    2    6   20    3    4    1    5   15    4   14    1    5    0   15]
 [   4   23    4    9   18  844    3   35    3    9    1   19    7   23    6    1    4    2    4   12   12]
 [   1    4   28    2    5    7  985    6    1    1    2    5    3    3    0   11    2    1    4    6    9]
 [   2   14   21    5    4   44    3  870    6    1    5   12    6    5    1    2    5    2   43   14   12]
 [   9    3    1    2    2    3    0    0  849   52   13    1    5   21   20    0    4    3    6    1    7]
 [ 117    1    4    1    5    5    0    1   60  759    1    0    2   13   10    2    4    0    6    3    7]
 [   1    3    9   24    2    8    5    3   20    3  926    3    0   12   16    0    0    1   16    2   10]
 [   1    1    2    0    2   11    2    9    3    1    1  837   37   21    1   16    7   29    4   19    7]
 [   1    1    3    7    0    6    3    3    2    0    3   57  818    6    4    9    5   44    8    7    8]
 [   4    5    4    2    3   10    3    3   25   24    8   13    5  850   11    4    3    5    2    5   12]
 [  14    1    3   27    8    0    0    2   46   10    4    1    5    9  936    0    1    5   12    0   14]
 [   6    1    4    3    3    1    7    0    3    1    0   20   15    2    1  951   16   20    3    5    4]
 [   3    6   10    3    7    8    1    3   10    0    3   15    3    7    1   14  960    1    2    3   12]
 [   0    1    2    5    2    1    2    1    1    6    0   17   36    6    4   13    5  883    2    2   16]
 [   2    9    8   32    5    2    2   23   10    3    7    2    7    3   25    2    1    1  904    2    8]
 [   1    6    6    3    2   15   19   18    1    1    0   27   12    7    0    6    3    6    3  936   16]
 [ 361  219  343  180  269  254  120  176  223  177  230  253  481  403  285  192  334  160  252  387 8633]]

2024-06-06 01:41:11,985 - ==> Best [Top1: 75.925   Top5: 96.012   Sparsity:0.00   Params: 169472 on epoch: 104]
2024-06-06 01:41:11,985 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:41:11,992 - 

2024-06-06 01:41:11,993 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:41:17,986 - Epoch: [107][  100/ 1218]    Overall Loss 0.575912    Objective Loss 0.575912                                        LR 0.000500    Time 0.059916    
2024-06-06 01:41:22,765 - Epoch: [107][  200/ 1218]    Overall Loss 0.575404    Objective Loss 0.575404                                        LR 0.000500    Time 0.053842    
2024-06-06 01:41:27,360 - Epoch: [107][  300/ 1218]    Overall Loss 0.573499    Objective Loss 0.573499                                        LR 0.000500    Time 0.051206    
2024-06-06 01:41:31,976 - Epoch: [107][  400/ 1218]    Overall Loss 0.570254    Objective Loss 0.570254                                        LR 0.000500    Time 0.049940    
2024-06-06 01:41:36,535 - Epoch: [107][  500/ 1218]    Overall Loss 0.568461    Objective Loss 0.568461                                        LR 0.000500    Time 0.049067    
2024-06-06 01:41:41,080 - Epoch: [107][  600/ 1218]    Overall Loss 0.566676    Objective Loss 0.566676                                        LR 0.000500    Time 0.048462    
2024-06-06 01:41:45,837 - Epoch: [107][  700/ 1218]    Overall Loss 0.568540    Objective Loss 0.568540                                        LR 0.000500    Time 0.048332    
2024-06-06 01:41:50,495 - Epoch: [107][  800/ 1218]    Overall Loss 0.571032    Objective Loss 0.571032                                        LR 0.000500    Time 0.048110    
2024-06-06 01:41:55,424 - Epoch: [107][  900/ 1218]    Overall Loss 0.572031    Objective Loss 0.572031                                        LR 0.000500    Time 0.048239    
2024-06-06 01:42:00,210 - Epoch: [107][ 1000/ 1218]    Overall Loss 0.572264    Objective Loss 0.572264                                        LR 0.000500    Time 0.048199    
2024-06-06 01:42:04,939 - Epoch: [107][ 1100/ 1218]    Overall Loss 0.570864    Objective Loss 0.570864                                        LR 0.000500    Time 0.048116    
2024-06-06 01:42:09,545 - Epoch: [107][ 1200/ 1218]    Overall Loss 0.572210    Objective Loss 0.572210                                        LR 0.000500    Time 0.047942    
2024-06-06 01:42:10,327 - Epoch: [107][ 1218/ 1218]    Overall Loss 0.572243    Objective Loss 0.572243    Top1 76.772616    Top5 97.799511    LR 0.000500    Time 0.047875    
2024-06-06 01:42:10,518 - --- validate (epoch=107)-----------
2024-06-06 01:42:10,518 - 34633 samples (256 per mini-batch)
2024-06-06 01:42:16,127 - Epoch: [107][  100/  136]    Loss 0.514309    Top1 75.539062    Top5 95.957031    
2024-06-06 01:42:17,788 - Epoch: [107][  136/  136]    Loss 0.514465    Top1 75.557994    Top5 95.980712    
2024-06-06 01:42:17,954 - ==> Top1: 75.558    Top5: 95.981    Loss: 0.514

2024-06-06 01:42:17,955 - ==> Confusion:
[[ 766    2    4    1   18    4    0    1   13   79    1    3    1    7    9    4    3    3    3    1    8]
 [   3  924    4    2   28   21    3   15    4    0    6    7    1    2    8    3    6    1   16    0    9]
 [   6    2  833   11    6    2   32   12    0    3    5    5    6   10    4    8    7    0    3    3   12]
 [   3    5   26  848    5    5    3    0    2    1   24    5   10    2   41    3    2    5   18    1    7]
 [  29   18    3    1  937    8    3    2    0   13    0    0    1    2   11    5    7    4    4    2    4]
 [   6   48    5    2   21  829    6   33    5    5    1   15    4   19    3    2   11    1    5    9   13]
 [   0    9   20    3    3    5  984    7    2    0    3    6    2    1    1    7    3    4    2   10   14]
 [   1   17   17    3    6   64   11  852    3    2    5   12    4    2    0    3    0    7   46   12   10]
 [  11    0    3    1    2    3    1    1  821   56   12    3    4   15   32    1    7    5   14    1    9]
 [  80    1    3    2    8    3    0    3   51  791    1    2    1   12   14    6    2    6    4    0   11]
 [   1    5    4   14    2    5    8    6   18    1  945    3    0   11   12    1    2    1   17    2    6]
 [   2    2    2    0    1   19    3    8    3    1    2  844   37   12    1   19    3   15    5   21   11]
 [   0    3    4    9    1    6    1    7    1    0    2  100  767    5    5    6    2   50    6    8   12]
 [   1    3    2    1    6   15    0    4   19   27   10   14    6  846   15    6    4    4    2    5   11]
 [  15    6    2   12    9    0    0    2   39    8    5    2    7    4  954    2    3    3   13    0   12]
 [   1    1    7    2    6    1    4    1    0    0    0   22    9    1    1  969   13   13    3    4    8]
 [   2   14    5    5    6    7    2    2    4    0    3    9    5    7    4   25  954    2    0    4   12]
 [   1    1    0    1    0    1    1    0    1    2    1   26   23    1    8   23    4  899    3    2    7]
 [   1    9    9   13    6    2    3   20    5    2    7    4    4    0   27    0    0    1  924    6   15]
 [   0   10    4    0    5   13   23    9    0    1    1   28    5    8    0   10   12    6    6  939    8]
 [ 271  339  317  135  361  245  130  199  173  152  241  245  423  336  332  254  420  170  281  366 8542]]

2024-06-06 01:42:17,957 - ==> Best [Top1: 75.925   Top5: 96.012   Sparsity:0.00   Params: 169472 on epoch: 104]
2024-06-06 01:42:17,957 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:42:17,965 - 

2024-06-06 01:42:17,965 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:42:24,142 - Epoch: [108][  100/ 1218]    Overall Loss 0.562767    Objective Loss 0.562767                                        LR 0.000500    Time 0.061755    
2024-06-06 01:42:28,794 - Epoch: [108][  200/ 1218]    Overall Loss 0.567975    Objective Loss 0.567975                                        LR 0.000500    Time 0.054129    
2024-06-06 01:42:33,635 - Epoch: [108][  300/ 1218]    Overall Loss 0.565694    Objective Loss 0.565694                                        LR 0.000500    Time 0.052217    
2024-06-06 01:42:38,448 - Epoch: [108][  400/ 1218]    Overall Loss 0.568878    Objective Loss 0.568878                                        LR 0.000500    Time 0.051191    
2024-06-06 01:42:43,063 - Epoch: [108][  500/ 1218]    Overall Loss 0.567699    Objective Loss 0.567699                                        LR 0.000500    Time 0.050179    
2024-06-06 01:42:47,699 - Epoch: [108][  600/ 1218]    Overall Loss 0.568799    Objective Loss 0.568799                                        LR 0.000500    Time 0.049539    
2024-06-06 01:42:52,421 - Epoch: [108][  700/ 1218]    Overall Loss 0.569120    Objective Loss 0.569120                                        LR 0.000500    Time 0.049205    
2024-06-06 01:42:57,076 - Epoch: [108][  800/ 1218]    Overall Loss 0.568527    Objective Loss 0.568527                                        LR 0.000500    Time 0.048871    
2024-06-06 01:43:01,691 - Epoch: [108][  900/ 1218]    Overall Loss 0.569476    Objective Loss 0.569476                                        LR 0.000500    Time 0.048567    
2024-06-06 01:43:06,249 - Epoch: [108][ 1000/ 1218]    Overall Loss 0.570579    Objective Loss 0.570579                                        LR 0.000500    Time 0.048266    
2024-06-06 01:43:10,869 - Epoch: [108][ 1100/ 1218]    Overall Loss 0.570845    Objective Loss 0.570845                                        LR 0.000500    Time 0.048077    
2024-06-06 01:43:15,508 - Epoch: [108][ 1200/ 1218]    Overall Loss 0.569883    Objective Loss 0.569883                                        LR 0.000500    Time 0.047936    
2024-06-06 01:43:16,384 - Epoch: [108][ 1218/ 1218]    Overall Loss 0.569950    Objective Loss 0.569950    Top1 74.327628    Top5 96.332518    LR 0.000500    Time 0.047946    
2024-06-06 01:43:16,551 - --- validate (epoch=108)-----------
2024-06-06 01:43:16,551 - 34633 samples (256 per mini-batch)
2024-06-06 01:43:22,178 - Epoch: [108][  100/  136]    Loss 0.520364    Top1 75.703125    Top5 95.527344    
2024-06-06 01:43:23,713 - Epoch: [108][  136/  136]    Loss 0.517476    Top1 75.890047    Top5 95.654434    
2024-06-06 01:43:23,895 - ==> Top1: 75.890    Top5: 95.654    Loss: 0.517

2024-06-06 01:43:23,896 - ==> Confusion:
[[ 809    2    3    0   13    2    2    1   11   57    0    2    2    4    4    4    1    1    2    1   10]
 [   3  892    3    2   25   27   11   21    5    2    6    7    6    0   14    1    4    2   15    5   12]
 [  10    2  824   11    4    1   34   11    1    5    6    4    3    7    4    7    4    1   11    6   14]
 [   4    3   27  852    4    5    1    4    6    1   21    2   15    4   34    2    2    5   10    2   12]
 [  41   10    3    1  922    5    2    1    4   16    3    5    1    3   16    4    3    1    7    1    5]
 [  12   21    3    6   25  796    9   48    7    5    2   14   11   29    4    3    4    6    9    8   21]
 [   2    1   19    1    3    2  983    5    1    1    3   11    2    2    0   11    1    7    5   14   12]
 [   6   23   16    3    4   41   10  863    4    1    6   13    6    1    3    1    3    4   42   15   12]
 [  19    3    0    3    1    4    0    2  847   51   12    3    3    9   19    1    2    4    7    2   10]
 [ 108    2    2    0    9    3    2    1   60  774    4    1    2   10    8    2    0    3    2    0    8]
 [   2    6   14   17    1    5    4    5   31    1  923    2    1    8   11    1    2    2   23    1    4]
 [   3    2    1    0    3   10    4   10    0    0    3  845   45   17    0   14    4   27    3   14    6]
 [   1    1    2    5    1    4    2    3    5    0    3   67  819    3    1    8    3   41    5    7   14]
 [   5    0    0    0    6   13    3    3   34   26   10   16    6  827   12    1    3    4    4    9   19]
 [  14    3    1   12    7    3    0    1   42    9    4    1    6    8  955    0    2    3   18    1    8]
 [   3    2    3    2    6    0   13    1    1    0    0   22    5    1    0  967    7   19    4    3    7]
 [   7   13    7    1    8    8    2    1    5    2    5    9    4    6    4   24  930    2    2    7   25]
 [   4    1    0    4    0    1    3    2    3    2    0   19   43    5    3   12    1  890    5    3    4]
 [   3    5    7   22    3    4    0   26   10    0    5    3    7    1   20    2    0    0  926    5    9]
 [   3    4    2    0    1   12   21    8    0    1    1   32   14    8    1   12    6    2    5  946    9]
 [ 334  247  266  149  330  191  178  212  230  166  216  260  432  322  343  267  254  156  310  376 8693]]

2024-06-06 01:43:23,898 - ==> Best [Top1: 75.925   Top5: 96.012   Sparsity:0.00   Params: 169472 on epoch: 104]
2024-06-06 01:43:23,898 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:43:23,906 - 

2024-06-06 01:43:23,906 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:43:29,855 - Epoch: [109][  100/ 1218]    Overall Loss 0.556600    Objective Loss 0.556600                                        LR 0.000500    Time 0.059470    
2024-06-06 01:43:34,686 - Epoch: [109][  200/ 1218]    Overall Loss 0.568511    Objective Loss 0.568511                                        LR 0.000500    Time 0.053879    
2024-06-06 01:43:39,314 - Epoch: [109][  300/ 1218]    Overall Loss 0.566989    Objective Loss 0.566989                                        LR 0.000500    Time 0.051342    
2024-06-06 01:43:44,021 - Epoch: [109][  400/ 1218]    Overall Loss 0.564629    Objective Loss 0.564629                                        LR 0.000500    Time 0.050269    
2024-06-06 01:43:48,754 - Epoch: [109][  500/ 1218]    Overall Loss 0.565293    Objective Loss 0.565293                                        LR 0.000500    Time 0.049678    
2024-06-06 01:43:53,333 - Epoch: [109][  600/ 1218]    Overall Loss 0.565590    Objective Loss 0.565590                                        LR 0.000500    Time 0.049027    
2024-06-06 01:43:58,033 - Epoch: [109][  700/ 1218]    Overall Loss 0.564468    Objective Loss 0.564468                                        LR 0.000500    Time 0.048736    
2024-06-06 01:44:02,674 - Epoch: [109][  800/ 1218]    Overall Loss 0.565082    Objective Loss 0.565082                                        LR 0.000500    Time 0.048443    
2024-06-06 01:44:07,207 - Epoch: [109][  900/ 1218]    Overall Loss 0.564607    Objective Loss 0.564607                                        LR 0.000500    Time 0.048095    
2024-06-06 01:44:11,943 - Epoch: [109][ 1000/ 1218]    Overall Loss 0.564339    Objective Loss 0.564339                                        LR 0.000500    Time 0.048020    
2024-06-06 01:44:16,504 - Epoch: [109][ 1100/ 1218]    Overall Loss 0.564094    Objective Loss 0.564094                                        LR 0.000500    Time 0.047799    
2024-06-06 01:44:21,228 - Epoch: [109][ 1200/ 1218]    Overall Loss 0.564677    Objective Loss 0.564677                                        LR 0.000500    Time 0.047750    
2024-06-06 01:44:22,063 - Epoch: [109][ 1218/ 1218]    Overall Loss 0.564280    Objective Loss 0.564280    Top1 75.305623    Top5 95.843521    LR 0.000500    Time 0.047730    
2024-06-06 01:44:22,262 - --- validate (epoch=109)-----------
2024-06-06 01:44:22,262 - 34633 samples (256 per mini-batch)
2024-06-06 01:44:27,985 - Epoch: [109][  100/  136]    Loss 0.507153    Top1 75.968750    Top5 95.890625    
2024-06-06 01:44:29,646 - Epoch: [109][  136/  136]    Loss 0.511570    Top1 75.930471    Top5 95.934513    
2024-06-06 01:44:29,837 - ==> Top1: 75.930    Top5: 95.935    Loss: 0.512

2024-06-06 01:44:29,838 - ==> Confusion:
[[ 768    1    5    1   18    4    0    4   11   82    1    2    1    7    5    4    1    3    5    0    8]
 [   5  907    0    1   25   36    4   14    5    1    5    3    3    2    7    5   10    3    8    8   11]
 [   3    4  815   13    6    1   43   12    2    8   10    3    2    6    2    9    3    2    5    8   13]
 [   5    7   21  841    4    7    3    3    5    5   22    1   14    3   28    5    2    8   15    3   14]
 [  23    7    4    1  941    6    1    3    3   15    1    2    0    4    9   11    6    0    5    4    8]
 [   4   29    3    4   18  845    8   21    5    4    1   18    8   27    5    4    8    5    3    8   15]
 [   3    4   21    2    1    4  986    5    2    2    4    4    3    2    0   12    5    2    2   12   10]
 [   3   14   17    1    7   57    7  865    2    1    9   13    8    4    1    2    2    5   33   18    8]
 [  13    4    3    0    4    6    0    1  808   69   20    5    4   18   22    1    3    5    8    1    7]
 [  73    3    1    0    5    4    0    2   47  817    2    1    1   14   13    3    1    3    0    1   10]
 [   1    0   17   11    4    8    5    4   15    0  945    0    3   14    4    2    3    0   16    5    7]
 [   2    3    3    0    0   27    5   10    0    2    1  803   52   11    1   20    5   32    2   22   10]
 [   0    2    2    6    0    9    4    3    2    0    3   66  803    5    3   11    4   49    5    3   15]
 [   3    0    0    1    3   22    3    2   17   19   10    9    5  864    5    3    5    4    0   13   13]
 [   7    6    5   19   22    3    3    0   37    8   11    3    4    6  928    1    1    1   19    1   13]
 [   4    1    1    2    3    2    5    1    1    1    1   13    6    2    0  979   11   15    2    7    9]
 [   3   10    4    2   11   10    2    2    9    1    2    8    5    2    1   15  948    3    2    6   26]
 [   0    0    1    3    1    4    2    0    0    2    0   16   36    7    5   16    3  894    2    6    7]
 [   1    5    8   23    7    5    2   21   10    0    8    3    8    1   13    1    1    2  918    9   12]
 [   0    5    3    3    1   14   22   10    1    0    2   27   12    7    1   15    6    4    4  946    5]
 [ 245  266  269  156  373  282  165  199  160  177  230  229  459  377  248  322  309  189  216  385 8676]]

2024-06-06 01:44:29,840 - ==> Best [Top1: 75.930   Top5: 95.935   Sparsity:0.00   Params: 169472 on epoch: 109]
2024-06-06 01:44:29,840 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:44:29,849 - 

2024-06-06 01:44:29,849 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:44:36,398 - Epoch: [110][  100/ 1218]    Overall Loss 0.557441    Objective Loss 0.557441                                        LR 0.000500    Time 0.065469    
2024-06-06 01:44:41,394 - Epoch: [110][  200/ 1218]    Overall Loss 0.558555    Objective Loss 0.558555                                        LR 0.000500    Time 0.057707    
2024-06-06 01:44:46,020 - Epoch: [110][  300/ 1218]    Overall Loss 0.556918    Objective Loss 0.556918                                        LR 0.000500    Time 0.053886    
2024-06-06 01:44:50,658 - Epoch: [110][  400/ 1218]    Overall Loss 0.559968    Objective Loss 0.559968                                        LR 0.000500    Time 0.052005    
2024-06-06 01:44:55,393 - Epoch: [110][  500/ 1218]    Overall Loss 0.558897    Objective Loss 0.558897                                        LR 0.000500    Time 0.051071    
2024-06-06 01:45:00,022 - Epoch: [110][  600/ 1218]    Overall Loss 0.560038    Objective Loss 0.560038                                        LR 0.000500    Time 0.050272    
2024-06-06 01:45:04,630 - Epoch: [110][  700/ 1218]    Overall Loss 0.560003    Objective Loss 0.560003                                        LR 0.000500    Time 0.049670    
2024-06-06 01:45:09,270 - Epoch: [110][  800/ 1218]    Overall Loss 0.561299    Objective Loss 0.561299                                        LR 0.000500    Time 0.049259    
2024-06-06 01:45:13,900 - Epoch: [110][  900/ 1218]    Overall Loss 0.562620    Objective Loss 0.562620                                        LR 0.000500    Time 0.048928    
2024-06-06 01:45:18,812 - Epoch: [110][ 1000/ 1218]    Overall Loss 0.563593    Objective Loss 0.563593                                        LR 0.000500    Time 0.048945    
2024-06-06 01:45:23,811 - Epoch: [110][ 1100/ 1218]    Overall Loss 0.563622    Objective Loss 0.563622                                        LR 0.000500    Time 0.049038    
2024-06-06 01:45:28,717 - Epoch: [110][ 1200/ 1218]    Overall Loss 0.563959    Objective Loss 0.563959                                        LR 0.000500    Time 0.049039    
2024-06-06 01:45:29,516 - Epoch: [110][ 1218/ 1218]    Overall Loss 0.563990    Objective Loss 0.563990    Top1 73.594132    Top5 95.599022    LR 0.000500    Time 0.048969    
2024-06-06 01:45:29,684 - --- validate (epoch=110)-----------
2024-06-06 01:45:29,684 - 34633 samples (256 per mini-batch)
2024-06-06 01:45:35,168 - Epoch: [110][  100/  136]    Loss 0.531042    Top1 75.542969    Top5 95.832031    
2024-06-06 01:45:36,844 - Epoch: [110][  136/  136]    Loss 0.532921    Top1 75.598418    Top5 95.833454    
2024-06-06 01:45:37,041 - ==> Top1: 75.598    Top5: 95.833    Loss: 0.533

2024-06-06 01:45:37,042 - ==> Confusion:
[[ 787    2    3    4    7    3    0    1    2   89    1    1    2    4    1    5    3    1    4    2    9]
 [   3  894    1    1   25   43    8   26    6    4    5    6    2    0    6    5    7    2    6    3   10]
 [   8    5  809   12    0    3   53   16    1   11    8    3    4    2    4    6    2    1    2    8   12]
 [   2    2   19  856    2    6   10    2    3    3   21    3    9    2   36    1    1    7   14    5   12]
 [  29   16    8    1  907   10    5    5    2   26    1    7    2    4    8    4    5    1    5    2    6]
 [   6   32    1    1   19  817   11   44    9    9    6   13    7   17    2    3    2    6    2   23   13]
 [   2    7   15    2    3    7  981    4    1    0    9    8    0    0    1   13    5    1    1   15   11]
 [   4   16   14    0    3   35    8  894    1    1    8   19    9    5    2    0    0    1   24   19   14]
 [  15    3    2    1    0    2    0    2  805   79   15    3    4   17   30    1    2    2    8    1   10]
 [  82    0    5    0    4    0    2    2   35  827    3    0    2   16   13    2    0    2    1    1    4]
 [   1    6    8   11    2    5    9    9   18    3  932    0    5   13   18    0    1    0   11    0   12]
 [   2    0    2    0    2    7    2   10    2    5    1  855   41    7    1   10    3   24    1   25   11]
 [   0    0    2    6    4    2    1    7    3    0    1   93  775    4    5    8    5   56    8    5   10]
 [   6    2    1    1    4   14    4    2   26   31   13   16    7  827    3    3    2    1    3   14   21]
 [  12    4    2   14   12   10    0    3   25   23    2    0    3    2  956    0    3    2   12    3   10]
 [   1    1    2    2    7    2   10    1    0    1    0   33   13    2    0  950    8   15    2    4   12]
 [   7   12    8    3    7   11    1    0   10    1    1   13    8    6    3   16  935    4    2    6   18]
 [   0    2    0    4    1    2    4    2    3    3    0   34   18    3    2   13    0  893    1    6   14]
 [   3   13   14   30    5    4    4   37   13    0    9    4    4    2   37    1    1    3  853    3   18]
 [   2    5    4    0    2    7   17   12    0    3    3   38    7    3    1   12    5    8    3  944   12]
 [ 274  241  284  148  315  222  204  255  131  212  252  282  424  312  344  256  305  185  198  404 8684]]

2024-06-06 01:45:37,044 - ==> Best [Top1: 75.930   Top5: 95.935   Sparsity:0.00   Params: 169472 on epoch: 109]
2024-06-06 01:45:37,044 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:45:37,052 - 

2024-06-06 01:45:37,052 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:45:43,211 - Epoch: [111][  100/ 1218]    Overall Loss 0.577201    Objective Loss 0.577201                                        LR 0.000500    Time 0.061567    
2024-06-06 01:45:47,905 - Epoch: [111][  200/ 1218]    Overall Loss 0.570944    Objective Loss 0.570944                                        LR 0.000500    Time 0.054247    
2024-06-06 01:45:52,766 - Epoch: [111][  300/ 1218]    Overall Loss 0.569059    Objective Loss 0.569059                                        LR 0.000500    Time 0.052363    
2024-06-06 01:45:57,359 - Epoch: [111][  400/ 1218]    Overall Loss 0.569507    Objective Loss 0.569507                                        LR 0.000500    Time 0.050748    
2024-06-06 01:46:01,923 - Epoch: [111][  500/ 1218]    Overall Loss 0.569739    Objective Loss 0.569739                                        LR 0.000500    Time 0.049722    
2024-06-06 01:46:06,498 - Epoch: [111][  600/ 1218]    Overall Loss 0.567124    Objective Loss 0.567124                                        LR 0.000500    Time 0.049058    
2024-06-06 01:46:11,188 - Epoch: [111][  700/ 1218]    Overall Loss 0.568399    Objective Loss 0.568399                                        LR 0.000500    Time 0.048747    
2024-06-06 01:46:15,804 - Epoch: [111][  800/ 1218]    Overall Loss 0.566968    Objective Loss 0.566968                                        LR 0.000500    Time 0.048421    
2024-06-06 01:46:20,407 - Epoch: [111][  900/ 1218]    Overall Loss 0.567195    Objective Loss 0.567195                                        LR 0.000500    Time 0.048154    
2024-06-06 01:46:25,030 - Epoch: [111][ 1000/ 1218]    Overall Loss 0.567355    Objective Loss 0.567355                                        LR 0.000500    Time 0.047960    
2024-06-06 01:46:29,635 - Epoch: [111][ 1100/ 1218]    Overall Loss 0.566177    Objective Loss 0.566177                                        LR 0.000500    Time 0.047784    
2024-06-06 01:46:34,418 - Epoch: [111][ 1200/ 1218]    Overall Loss 0.565808    Objective Loss 0.565808                                        LR 0.000500    Time 0.047787    
2024-06-06 01:46:35,326 - Epoch: [111][ 1218/ 1218]    Overall Loss 0.566044    Objective Loss 0.566044    Top1 75.061125    Top5 95.110024    LR 0.000500    Time 0.047826    
2024-06-06 01:46:35,532 - --- validate (epoch=111)-----------
2024-06-06 01:46:35,532 - 34633 samples (256 per mini-batch)
2024-06-06 01:46:40,995 - Epoch: [111][  100/  136]    Loss 0.532323    Top1 74.585938    Top5 95.175781    
2024-06-06 01:46:42,663 - Epoch: [111][  136/  136]    Loss 0.525740    Top1 74.671556    Top5 95.276182    
2024-06-06 01:46:42,853 - ==> Top1: 74.672    Top5: 95.276    Loss: 0.526

2024-06-06 01:46:42,854 - ==> Confusion:
[[ 797    5    3    2   15    2    1    1    9   58    0    2    1    4    6    0    4    2    4    4   11]
 [   5  944    1    1   16   28    8    6    4    1    3    2    3    3    4    3   10    4    7    4    6]
 [   6    2  816   18    8    6   46   10    0    4    5    3    4    7    0    3    7    5    7    3   10]
 [   6    2   17  865    6    8    7    0    3    2   19    4   14    1   29    1    2    4   16    1    9]
 [  24   16    4    1  927    6    3    0    3   11    2    4    3    2   14   10    6    3    4    1   10]
 [   2   39    4    4   28  811    6   42    6    9    0   11    8   26    4    6    9    4    5   13    6]
 [   2    5   20    2    2    2  989    3    1    1    3    4    4    0    1   17    1    5    3   10   11]
 [   4   28   10    2    5   41   10  874    2    3   11   13    4    1    0    1    4    8   30   17    9]
 [  15    7    0    1    3    4    0    2  820   43   23    3    7   19   24    0    1    5    9    3   13]
 [ 102    1    2    3   14    4    0    2   72  763    5    1    0   11   10    2    0    2    0    1    6]
 [   1    9   16    8    4    4    7    8   17    4  940    0    3   10   11    0    2    2   11    1    6]
 [   1    3    3    1    2   13    5    5    1    2    2  817   34    7    1   23    4   46    1   29   11]
 [   0    2    7    8    0    3    2    3    2    0    2   75  799    1    3   11    9   44    4    8   12]
 [   4    2    3    3    6   15    3    4   23   30   20   12    8  824    8    6    2    3    2   17    6]
 [  15    6    5   18   22    2    2    1   34    7    9    3    3    4  917    3    6    2   25    1   13]
 [   3    4    1    2    5    1    7    0    0    1    0   18    7    2    0  970   17   16    2    2    8]
 [   1   11    5    3   12   11    2    3    5    0    0    9    6    2    1   16  956    2    2    6   19]
 [   2    1    1    5    1    0    3    1    2    1    0   20   25    0    2   15    3  908    1    8    6]
 [   3   19    9   22    4    4    1   23   16    0   13    2    9    1   15    0    4    3  899    3    8]
 [   2    9    1    1    2    7   18    4    2    0    2   21   13    3    0    8   10    5    5  968    7]
 [ 275  373  313  234  392  212  211  188  184  171  265  236  413  295  298  283  455  222  252  403 8257]]

2024-06-06 01:46:42,856 - ==> Best [Top1: 75.930   Top5: 95.935   Sparsity:0.00   Params: 169472 on epoch: 109]
2024-06-06 01:46:42,856 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:46:42,863 - 

2024-06-06 01:46:42,864 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:46:48,941 - Epoch: [112][  100/ 1218]    Overall Loss 0.552626    Objective Loss 0.552626                                        LR 0.000500    Time 0.060757    
2024-06-06 01:46:53,550 - Epoch: [112][  200/ 1218]    Overall Loss 0.556739    Objective Loss 0.556739                                        LR 0.000500    Time 0.053414    
2024-06-06 01:46:58,195 - Epoch: [112][  300/ 1218]    Overall Loss 0.557965    Objective Loss 0.557965                                        LR 0.000500    Time 0.051087    
2024-06-06 01:47:02,818 - Epoch: [112][  400/ 1218]    Overall Loss 0.561061    Objective Loss 0.561061                                        LR 0.000500    Time 0.049867    
2024-06-06 01:47:07,478 - Epoch: [112][  500/ 1218]    Overall Loss 0.559492    Objective Loss 0.559492                                        LR 0.000500    Time 0.049211    
2024-06-06 01:47:12,133 - Epoch: [112][  600/ 1218]    Overall Loss 0.560899    Objective Loss 0.560899                                        LR 0.000500    Time 0.048764    
2024-06-06 01:47:16,868 - Epoch: [112][  700/ 1218]    Overall Loss 0.561342    Objective Loss 0.561342                                        LR 0.000500    Time 0.048560    
2024-06-06 01:47:21,557 - Epoch: [112][  800/ 1218]    Overall Loss 0.562333    Objective Loss 0.562333                                        LR 0.000500    Time 0.048349    
2024-06-06 01:47:26,199 - Epoch: [112][  900/ 1218]    Overall Loss 0.562700    Objective Loss 0.562700                                        LR 0.000500    Time 0.048133    
2024-06-06 01:47:30,810 - Epoch: [112][ 1000/ 1218]    Overall Loss 0.563412    Objective Loss 0.563412                                        LR 0.000500    Time 0.047929    
2024-06-06 01:47:35,502 - Epoch: [112][ 1100/ 1218]    Overall Loss 0.563416    Objective Loss 0.563416                                        LR 0.000500    Time 0.047835    
2024-06-06 01:47:40,237 - Epoch: [112][ 1200/ 1218]    Overall Loss 0.563894    Objective Loss 0.563894                                        LR 0.000500    Time 0.047793    
2024-06-06 01:47:41,040 - Epoch: [112][ 1218/ 1218]    Overall Loss 0.563730    Objective Loss 0.563730    Top1 79.706601    Top5 97.066015    LR 0.000500    Time 0.047746    
2024-06-06 01:47:41,219 - --- validate (epoch=112)-----------
2024-06-06 01:47:41,219 - 34633 samples (256 per mini-batch)
2024-06-06 01:47:46,832 - Epoch: [112][  100/  136]    Loss 0.522990    Top1 74.703125    Top5 95.132812    
2024-06-06 01:47:48,548 - Epoch: [112][  136/  136]    Loss 0.529931    Top1 74.605145    Top5 95.062513    
2024-06-06 01:47:48,760 - ==> Top1: 74.605    Top5: 95.063    Loss: 0.530

2024-06-06 01:47:48,761 - ==> Confusion:
[[ 829    2    8    1    8    3    0    2    6   41    0    2    3    3    6    3    5    1    5    1    2]
 [   2  896    5    1   25   29    7   18    6    2    3    3    4    0    6    4   12    1   22    4   13]
 [   6    5  840   17    3    1   26   12    0    3    6    2    3    3    3    2   10    0    9    4   15]
 [   5    3   27  868    4    8    6    2    2    2   11    1    5    2   25    3    7    6   22    1    6]
 [  46    9    8    0  911   13    0    4    4    8    3    5    1    1   12    2   11    1    4    2    9]
 [   6   44    9    4    9  813    7   50    7    5    3   19    5   13    5    1    7    2    5   18   11]
 [   1    9   25    2    2    1  998    4    2    0    1    3    0    0    1    7    7    1    1   10   11]
 [   2   15   13    2    2   40   14  875    3    3    6   11    1    2    3    1    3    3   52   18    8]
 [  21    8    3    2    3    3    1    2  810   48   19    3    8   12   32    0    4    2   14    0    7]
 [ 162    1    3    0    8    4    0    2   64  697    1    2    1   22   18    0    0    2    3    3    8]
 [   2    5   16   22    2    5    4    3   10    1  921    4    2   10   15    0    4    0   30    3    5]
 [   3    3    1    1    0   16    1    7    0    0    1  881   18    5    0   12    8   19    2   27    6]
 [   5    0    3    8    1    5    5    5    1    1    1  122  742    5    5   13    7   35   12    7   12]
 [   6    1    2    3    6   26    2    3   11   14   11   18    3  847    8    5    6    1    3   12   13]
 [  19    9    4   26   11    3    0    0   36    7    6    2    5    6  930    0    5    4   18    1    6]
 [   3    1    7    2    7    2   14    1    0    1    0   27    7    3    0  946   21    5    4    2   13]
 [   7    7   14    5    4   10    3    1    6    0    2    7    5    2    1   10  963    1    2    8   14]
 [   6    1    2    7    1    3    6    0    1    2    0   41   25    4    8   29    3  841    6    7   12]
 [   2    5   14   17    4    2    2   23    8    0    3    4    5    0   25    0    3    0  933    2    6]
 [   3    5    3    5    1    8   22   14    3    0    0   23    7    2    3    7   11    4    5  949   13]
 [ 346  284  392  195  302  268  208  250  145  121  204  291  331  341  330  203  513  118  327  415 8348]]

2024-06-06 01:47:48,762 - ==> Best [Top1: 75.930   Top5: 95.935   Sparsity:0.00   Params: 169472 on epoch: 109]
2024-06-06 01:47:48,762 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:47:48,770 - 

2024-06-06 01:47:48,770 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:47:55,227 - Epoch: [113][  100/ 1218]    Overall Loss 0.568305    Objective Loss 0.568305                                        LR 0.000500    Time 0.064546    
2024-06-06 01:47:59,928 - Epoch: [113][  200/ 1218]    Overall Loss 0.564030    Objective Loss 0.564030                                        LR 0.000500    Time 0.055769    
2024-06-06 01:48:04,721 - Epoch: [113][  300/ 1218]    Overall Loss 0.565107    Objective Loss 0.565107                                        LR 0.000500    Time 0.053149    
2024-06-06 01:48:09,417 - Epoch: [113][  400/ 1218]    Overall Loss 0.562044    Objective Loss 0.562044                                        LR 0.000500    Time 0.051598    
2024-06-06 01:48:14,120 - Epoch: [113][  500/ 1218]    Overall Loss 0.562001    Objective Loss 0.562001                                        LR 0.000500    Time 0.050682    
2024-06-06 01:48:18,706 - Epoch: [113][  600/ 1218]    Overall Loss 0.562237    Objective Loss 0.562237                                        LR 0.000500    Time 0.049876    
2024-06-06 01:48:23,420 - Epoch: [113][  700/ 1218]    Overall Loss 0.561186    Objective Loss 0.561186                                        LR 0.000500    Time 0.049481    
2024-06-06 01:48:28,195 - Epoch: [113][  800/ 1218]    Overall Loss 0.561471    Objective Loss 0.561471                                        LR 0.000500    Time 0.049263    
2024-06-06 01:48:32,763 - Epoch: [113][  900/ 1218]    Overall Loss 0.562135    Objective Loss 0.562135                                        LR 0.000500    Time 0.048861    
2024-06-06 01:48:37,399 - Epoch: [113][ 1000/ 1218]    Overall Loss 0.562513    Objective Loss 0.562513                                        LR 0.000500    Time 0.048609    
2024-06-06 01:48:42,086 - Epoch: [113][ 1100/ 1218]    Overall Loss 0.563354    Objective Loss 0.563354                                        LR 0.000500    Time 0.048449    
2024-06-06 01:48:46,682 - Epoch: [113][ 1200/ 1218]    Overall Loss 0.562555    Objective Loss 0.562555                                        LR 0.000500    Time 0.048240    
2024-06-06 01:48:47,507 - Epoch: [113][ 1218/ 1218]    Overall Loss 0.562557    Objective Loss 0.562557    Top1 72.127139    Top5 95.354523    LR 0.000500    Time 0.048204    
2024-06-06 01:48:47,667 - --- validate (epoch=113)-----------
2024-06-06 01:48:47,668 - 34633 samples (256 per mini-batch)
2024-06-06 01:48:53,149 - Epoch: [113][  100/  136]    Loss 0.515961    Top1 75.964844    Top5 95.429688    
2024-06-06 01:48:54,814 - Epoch: [113][  136/  136]    Loss 0.513340    Top1 76.025756    Top5 95.394566    
2024-06-06 01:48:55,015 - ==> Top1: 76.026    Top5: 95.395    Loss: 0.513

2024-06-06 01:48:55,016 - ==> Confusion:
[[ 761    1    6    0   30    2    2    2    8   89    1    4    2    4    3    3    0    3    2    0    8]
 [   5  896    2    2   29   37    6   21    6    2    1    3    2    1    7    2    9    2   10    8   12]
 [   8    4  820    7    7    2   30   12    3    6    4    5    1    8    1   10    8    2    7    8   17]
 [   6    3   19  864    3    6    7    1    4    1   14    5    7    2   27    2    6    7   15    2   15]
 [  20    6    3    0  957    8    0    1    3   10    0    4    1    5    6    8    6    2    6    1    7]
 [   1   24    5    6   23  846    3   27    1    5    0   27    5   25    5    4    8    3    5   13    7]
 [   4    4   22    2    4    7  978    9    5    1    2   10    1    1    0   11    2    7    1    9    6]
 [   4   13   19    2    4   42    5  878    3    2    8   20    8    8    0    5    1    4   29   14    8]
 [  13    4    0    1    2    4    1    2  806   69   12    2    4   26   27    0    7    1   11    3    7]
 [  76    0    2    1    5    3    0    2   45  811    2    1    2   24   14    2    1    5    2    0    3]
 [   2    5   11   11    1    6    4    7   11    5  936    4    3   28    6    0    3    0   14    2    5]
 [   3    2    0    0    2    9    3    2    1    1    0  891   25   12    0   17    7   19    0   10    7]
 [   4    4    2    6    0    5    2    0    0    1    1   88  812    1    2   11    8   24    7    6   11]
 [   1    0    1    1    7   14    1    1   12   25    5   24    8  858    2    6    6    4    3    7   15]
 [  13    2    5   10   23    3    1    0   29   13    4    4    6   11  925    4    1    6   21    2   15]
 [   6    2    4    1    2    2    4    0    1    1    0   28   10    1    0  973    5   14    0    2   10]
 [   3    6    3    1   12   11    4    0    7    1    2   12    4    7    1   19  952    3    3    7   14]
 [   1    2    0    6    1    2    0    1    2    4    0   31   39    2    4   20    0  879    2    1    8]
 [   5    7   11   21    8    7    3   15   10    1    5    8    9    1   21    2    2    1  914    2    5]
 [   0    7    5    3    6    8   16   14    4    0    3   49   13    6    1   16   11    6    5  906    9]
 [ 279  223  259  116  390  266  136  187  147  187  204  359  461  476  264  246  382  140  225  318 8667]]

2024-06-06 01:48:55,018 - ==> Best [Top1: 76.026   Top5: 95.395   Sparsity:0.00   Params: 169472 on epoch: 113]
2024-06-06 01:48:55,018 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:48:55,033 - 

2024-06-06 01:48:55,033 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:49:01,240 - Epoch: [114][  100/ 1218]    Overall Loss 0.560934    Objective Loss 0.560934                                        LR 0.000500    Time 0.062048    
2024-06-06 01:49:05,802 - Epoch: [114][  200/ 1218]    Overall Loss 0.556732    Objective Loss 0.556732                                        LR 0.000500    Time 0.053824    
2024-06-06 01:49:10,416 - Epoch: [114][  300/ 1218]    Overall Loss 0.558022    Objective Loss 0.558022                                        LR 0.000500    Time 0.051257    
2024-06-06 01:49:14,995 - Epoch: [114][  400/ 1218]    Overall Loss 0.558327    Objective Loss 0.558327                                        LR 0.000500    Time 0.049887    
2024-06-06 01:49:19,744 - Epoch: [114][  500/ 1218]    Overall Loss 0.556776    Objective Loss 0.556776                                        LR 0.000500    Time 0.049404    
2024-06-06 01:49:24,338 - Epoch: [114][  600/ 1218]    Overall Loss 0.557154    Objective Loss 0.557154                                        LR 0.000500    Time 0.048823    
2024-06-06 01:49:29,009 - Epoch: [114][  700/ 1218]    Overall Loss 0.559894    Objective Loss 0.559894                                        LR 0.000500    Time 0.048519    
2024-06-06 01:49:33,889 - Epoch: [114][  800/ 1218]    Overall Loss 0.560688    Objective Loss 0.560688                                        LR 0.000500    Time 0.048552    
2024-06-06 01:49:38,469 - Epoch: [114][  900/ 1218]    Overall Loss 0.561161    Objective Loss 0.561161                                        LR 0.000500    Time 0.048244    
2024-06-06 01:49:43,341 - Epoch: [114][ 1000/ 1218]    Overall Loss 0.562380    Objective Loss 0.562380                                        LR 0.000500    Time 0.048290    
2024-06-06 01:49:48,191 - Epoch: [114][ 1100/ 1218]    Overall Loss 0.562675    Objective Loss 0.562675                                        LR 0.000500    Time 0.048308    
2024-06-06 01:49:52,897 - Epoch: [114][ 1200/ 1218]    Overall Loss 0.562865    Objective Loss 0.562865                                        LR 0.000500    Time 0.048202    
2024-06-06 01:49:53,676 - Epoch: [114][ 1218/ 1218]    Overall Loss 0.562516    Objective Loss 0.562516    Top1 77.017115    Top5 96.088020    LR 0.000500    Time 0.048129    
2024-06-06 01:49:53,844 - --- validate (epoch=114)-----------
2024-06-06 01:49:53,845 - 34633 samples (256 per mini-batch)
2024-06-06 01:49:59,422 - Epoch: [114][  100/  136]    Loss 0.519857    Top1 75.519531    Top5 95.757812    
2024-06-06 01:50:01,089 - Epoch: [114][  136/  136]    Loss 0.516069    Top1 75.806312    Top5 95.755493    
2024-06-06 01:50:01,253 - ==> Top1: 75.806    Top5: 95.755    Loss: 0.516

2024-06-06 01:50:01,254 - ==> Confusion:
[[ 800    3    4    1   21    3    0    0    6   54    1    5    0    4    4    2    5    3    4    0   11]
 [   2  934    1    2   19   23    6   16    6    1    4    3    2    1    7    3   10    3   11    4    5]
 [   8    4  835   10    8    5   20   10    0    6    4    5    2    5    5    6    4    3   11    7   12]
 [   5    2   18  847    4   10    3    2    3    1   12    3   10    4   42    3    2    6   23    0   16]
 [  27   16    3    1  941   10    1    3    2   10    0    5    1    5    8    2    1    2    7    3    6]
 [   3   41    3    1   26  812    5   37    3    3    2   28    3   35    3    2    4    7    8    5   12]
 [   4   10   35    4    5    3  951    8    1    0    3    8    2    4    0    9    5    3    3   15   13]
 [   4   18   10    4    6   29    2  910    4    3    1   20    5    7    1    0    0    2   25   17    9]
 [  15    5    3    2    3    1    0    1  822   56   12    3    5   15   33    1    2    4   10    0    9]
 [ 100    2    1    0   10    3    1    3   45  782    1    1    1   23   10    0    4    4    0    0   10]
 [   1    3   15   18    1    3    6    7   24    4  894    0    3   18   21    1    4    3   21    3   14]
 [   1    3    2    1    1    6    4    8    1    4    0  876   29   11    2   12    8   14    4   11   13]
 [   0    1    2    5    1    3    2    5    0    0    1  106  775    8    7    5    3   36   12    8   15]
 [   3    3    3    0    7   15    0    3   24   21    5    7    4  866    4    1    3    8    4    9   11]
 [   9    8    3   20   17    4    0    1   26    9    0    1    2    9  959    0    1    4   12    2   11]
 [   4    0    4    3    5    1    7    2    0    0    0   31   13    5    3  936   18   17    2    4   11]
 [   4   11    3    3    9    7    1    3   10    3    1   16    3    6    2    8  951    1    5    8   17]
 [   1    1    1    2    1    2    3    1    3    1    1   27   24    5    6    5    9  897    1    3   11]
 [   4   16    5   12    3    5    0   34    8    1    2    2    5    3   28    0    2    1  922    1    4]
 [   1   12    4    0    1    8   16   16    1    0    2   41    8    1    1    5   10    4    6  939   12]
 [ 315  352  265  157  366  269  100  274  147  144  143  280  394  423  387  170  359  161  276  345 8605]]

2024-06-06 01:50:01,256 - ==> Best [Top1: 76.026   Top5: 95.395   Sparsity:0.00   Params: 169472 on epoch: 113]
2024-06-06 01:50:01,256 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:50:01,263 - 

2024-06-06 01:50:01,263 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:50:07,427 - Epoch: [115][  100/ 1218]    Overall Loss 0.558631    Objective Loss 0.558631                                        LR 0.000500    Time 0.061616    
2024-06-06 01:50:12,173 - Epoch: [115][  200/ 1218]    Overall Loss 0.556065    Objective Loss 0.556065                                        LR 0.000500    Time 0.054532    
2024-06-06 01:50:16,848 - Epoch: [115][  300/ 1218]    Overall Loss 0.559688    Objective Loss 0.559688                                        LR 0.000500    Time 0.051932    
2024-06-06 01:50:21,424 - Epoch: [115][  400/ 1218]    Overall Loss 0.560530    Objective Loss 0.560530                                        LR 0.000500    Time 0.050386    
2024-06-06 01:50:25,962 - Epoch: [115][  500/ 1218]    Overall Loss 0.558730    Objective Loss 0.558730                                        LR 0.000500    Time 0.049380    
2024-06-06 01:50:30,543 - Epoch: [115][  600/ 1218]    Overall Loss 0.559725    Objective Loss 0.559725                                        LR 0.000500    Time 0.048782    
2024-06-06 01:50:35,212 - Epoch: [115][  700/ 1218]    Overall Loss 0.560793    Objective Loss 0.560793                                        LR 0.000500    Time 0.048482    
2024-06-06 01:50:39,858 - Epoch: [115][  800/ 1218]    Overall Loss 0.561984    Objective Loss 0.561984                                        LR 0.000500    Time 0.048226    
2024-06-06 01:50:44,504 - Epoch: [115][  900/ 1218]    Overall Loss 0.562110    Objective Loss 0.562110                                        LR 0.000500    Time 0.048028    
2024-06-06 01:50:49,197 - Epoch: [115][ 1000/ 1218]    Overall Loss 0.561203    Objective Loss 0.561203                                        LR 0.000500    Time 0.047917    
2024-06-06 01:50:53,973 - Epoch: [115][ 1100/ 1218]    Overall Loss 0.561326    Objective Loss 0.561326                                        LR 0.000500    Time 0.047901    
2024-06-06 01:50:58,588 - Epoch: [115][ 1200/ 1218]    Overall Loss 0.561438    Objective Loss 0.561438                                        LR 0.000500    Time 0.047754    
2024-06-06 01:50:59,418 - Epoch: [115][ 1218/ 1218]    Overall Loss 0.561778    Objective Loss 0.561778    Top1 72.616137    Top5 94.376528    LR 0.000500    Time 0.047728    
2024-06-06 01:50:59,603 - --- validate (epoch=115)-----------
2024-06-06 01:50:59,603 - 34633 samples (256 per mini-batch)
2024-06-06 01:51:05,223 - Epoch: [115][  100/  136]    Loss 0.507478    Top1 75.792969    Top5 95.875000    
2024-06-06 01:51:06,890 - Epoch: [115][  136/  136]    Loss 0.509264    Top1 75.783213    Top5 95.873878    
2024-06-06 01:51:07,086 - ==> Top1: 75.783    Top5: 95.874    Loss: 0.509

2024-06-06 01:51:07,087 - ==> Confusion:
[[ 817    1    2    1   14    2    0    1    6   53    0    0    3    5    7    2    5    3    3    0    6]
 [   2  898    1    2   24   35    5   28    6    3    4    5    1    1    7    3   13    0    6    3   16]
 [   6    4  827   15    5    2   29    8    2    7    6    3    5    9    3   13    5    1    4    3   13]
 [   3    1   20  854    5    7    4    6    3    2   17    0   15    3   34    4    4    6   16    4    8]
 [  37   11    1    3  926    9    0    4    2   18    0    5    1    1   11    5   11    1    1    2    5]
 [   5   23    1    3   22  844    6   46    3    7    2   12    8   19    3    0    3    2    6   19    9]
 [   4    3   26    4    4    7  967    8    1    0    5    5    1    1    2   14    5    1    5   14    9]
 [   8   11   12    2    1   41    4  909    2    1    3   11    8    2    1    0    2    2   31   21    5]
 [  18    4    1    0    1    2    0    2  797   87   12    0    6   17   30    0    3    2    8    2   10]
 [  87    0    2    1   13    3    0    1   44  809    0    0    0   15   10    0    1    3    3    1    8]
 [   1    2   13   16    3    8    4    8   19    4  913    3    0   22   16    0    3    1   13    5   10]
 [   4    0    0    0    2   17    2    7    3    1    1  856   37   13    0   18    5   16    2   22    5]
 [   4    0    1    6    0    6    2    4    4    0    2   87  787    3    5   12    6   38    7    8   13]
 [   7    1    0    0    6   16    2    7   14   28    7   12    3  863    2    4    3    5    0   11   10]
 [  16    4    1   10   15    5    0    3   36   17    1    0    3    8  954    1    6    0   11    0    7]
 [   3    1    8    1    1    1    5    0    0    0    0   20   11    1    0  977   12    8    2    5   10]
 [   5    7    4    3   11   10    0    2    6    4    2   11    2    7    4    9  956    1    4    7   17]
 [   7    2    2    2    2    1    2    0    2    1    0   27   49    5    3   27    2  853    1    1   16]
 [   1    8   12   17    5    6    1   22    8    3    9    2    7    1   26    2    4    1  913    2    8]
 [   1    5    0    0    3   20   11   18    1    0    0   23   10    9    1   11   10    4    4  952    5]
 [ 347  218  311  156  357  241  150  254  182  203  203  238  391  362  339  226  402  124  231  423 8574]]

2024-06-06 01:51:07,088 - ==> Best [Top1: 76.026   Top5: 95.395   Sparsity:0.00   Params: 169472 on epoch: 113]
2024-06-06 01:51:07,089 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:51:07,096 - 

2024-06-06 01:51:07,097 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:51:13,370 - Epoch: [116][  100/ 1218]    Overall Loss 0.556009    Objective Loss 0.556009                                        LR 0.000500    Time 0.062713    
2024-06-06 01:51:17,970 - Epoch: [116][  200/ 1218]    Overall Loss 0.559809    Objective Loss 0.559809                                        LR 0.000500    Time 0.054348    
2024-06-06 01:51:22,652 - Epoch: [116][  300/ 1218]    Overall Loss 0.560768    Objective Loss 0.560768                                        LR 0.000500    Time 0.051831    
2024-06-06 01:51:27,332 - Epoch: [116][  400/ 1218]    Overall Loss 0.559184    Objective Loss 0.559184                                        LR 0.000500    Time 0.050569    
2024-06-06 01:51:32,183 - Epoch: [116][  500/ 1218]    Overall Loss 0.560561    Objective Loss 0.560561                                        LR 0.000500    Time 0.050150    
2024-06-06 01:51:36,804 - Epoch: [116][  600/ 1218]    Overall Loss 0.558908    Objective Loss 0.558908                                        LR 0.000500    Time 0.049490    
2024-06-06 01:51:41,480 - Epoch: [116][  700/ 1218]    Overall Loss 0.560438    Objective Loss 0.560438                                        LR 0.000500    Time 0.049098    
2024-06-06 01:51:46,136 - Epoch: [116][  800/ 1218]    Overall Loss 0.559611    Objective Loss 0.559611                                        LR 0.000500    Time 0.048777    
2024-06-06 01:51:50,759 - Epoch: [116][  900/ 1218]    Overall Loss 0.558259    Objective Loss 0.558259                                        LR 0.000500    Time 0.048493    
2024-06-06 01:51:55,486 - Epoch: [116][ 1000/ 1218]    Overall Loss 0.558710    Objective Loss 0.558710                                        LR 0.000500    Time 0.048369    
2024-06-06 01:52:00,085 - Epoch: [116][ 1100/ 1218]    Overall Loss 0.560393    Objective Loss 0.560393                                        LR 0.000500    Time 0.048150    
2024-06-06 01:52:04,647 - Epoch: [116][ 1200/ 1218]    Overall Loss 0.560472    Objective Loss 0.560472                                        LR 0.000500    Time 0.047939    
2024-06-06 01:52:05,431 - Epoch: [116][ 1218/ 1218]    Overall Loss 0.560210    Objective Loss 0.560210    Top1 73.594132    Top5 95.599022    LR 0.000500    Time 0.047873    
2024-06-06 01:52:05,623 - --- validate (epoch=116)-----------
2024-06-06 01:52:05,623 - 34633 samples (256 per mini-batch)
2024-06-06 01:52:11,141 - Epoch: [116][  100/  136]    Loss 0.517464    Top1 75.273438    Top5 95.410156    
2024-06-06 01:52:12,794 - Epoch: [116][  136/  136]    Loss 0.516474    Top1 75.295239    Top5 95.380129    
2024-06-06 01:52:12,968 - ==> Top1: 75.295    Top5: 95.380    Loss: 0.516

2024-06-06 01:52:12,970 - ==> Confusion:
[[ 825    0    3    1    9    2    0    3    4   47    1    1    3    3    8    5    0    0    4    3    9]
 [   3  885    2    2   33   36    6   22    2    3    2    7    4    1    9    3   10    1   19    4    9]
 [  13    4  823   10    5    4   28   10    4    6    6    6    4    7    6    6    3    4    6    4   11]
 [   1    0   16  883    4   10    1    3    4    1   20    1   10    3   33    2    0    5   10    3    6]
 [  40    8    0    2  930   11    1    3    3   10    2    3    0    6    7    6    9    2    8    0    3]
 [   7   24    3    6   18  854    5   38    1    5    1   14   10   20    7    3    4    4    2   10    7]
 [   1    1   22    0    4    7  991   12    2    2    6    2    4    2    1    9    2    3    1    9    5]
 [   8   14   16    2    1   35    9  901    1    0    4    6    5   10    1    1    3    3   32   14   11]
 [  13    3    2    0    3    5    0    4  780   72   16    4   10   29   36    1    7    0   13    1    3]
 [ 121    2    4    1    6    2    1    7   42  751    0    1    1   34   10    2    2    3    1    1    9]
 [   2    5   13   21    2    4    8   10   17    1  916    4    1   17   19    0    2    2   16    1    3]
 [   1    2    2    1    2   14    2   13    1    0    0  850   21    5    3   17    9   26    3   33    6]
 [   1    1    1    9    2    5    3    5    3    0    1   96  798    2    4    9    4   24    7    8   12]
 [   1    4    2    0   11   17    1    3   12   14    8    9    8  872    7    4    3    6    1    8   10]
 [   9    5    5   15   12    2    0    0   30   10    3    1    3    9  969    0    1    2   10    0   12]
 [   1    2    6    0    5    0   10    2    2    0    1   19    9    3    0  976   11    9    2    4    4]
 [   8    6    4    3    5    8    2    1    3    0    7    9    3    6    5   20  958    2    3   10    9]
 [   3    0    2    6    4    4    4    2    4    1    0   32   40    7    3   19    4  856    2    6    6]
 [   6    7   13   18    3    2    3   25   13    1    5    4    3    1   32    0    1    6  907    1    7]
 [   2    2    3    0    1    8   17   14    0    1    2   29    6    4    1    8   11    8    8  953   10]
 [ 350  212  291  187  328  279  152  261  176  135  190  257  367  443  367  253  449  172  265  399 8399]]

2024-06-06 01:52:12,971 - ==> Best [Top1: 76.026   Top5: 95.395   Sparsity:0.00   Params: 169472 on epoch: 113]
2024-06-06 01:52:12,971 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:52:12,979 - 

2024-06-06 01:52:12,979 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:52:19,311 - Epoch: [117][  100/ 1218]    Overall Loss 0.567352    Objective Loss 0.567352                                        LR 0.000500    Time 0.063297    
2024-06-06 01:52:24,015 - Epoch: [117][  200/ 1218]    Overall Loss 0.568164    Objective Loss 0.568164                                        LR 0.000500    Time 0.055162    
2024-06-06 01:52:28,549 - Epoch: [117][  300/ 1218]    Overall Loss 0.566479    Objective Loss 0.566479                                        LR 0.000500    Time 0.051880    
2024-06-06 01:52:33,158 - Epoch: [117][  400/ 1218]    Overall Loss 0.566723    Objective Loss 0.566723                                        LR 0.000500    Time 0.050428    
2024-06-06 01:52:37,798 - Epoch: [117][  500/ 1218]    Overall Loss 0.565979    Objective Loss 0.565979                                        LR 0.000500    Time 0.049619    
2024-06-06 01:52:42,412 - Epoch: [117][  600/ 1218]    Overall Loss 0.564443    Objective Loss 0.564443                                        LR 0.000500    Time 0.049036    
2024-06-06 01:52:47,200 - Epoch: [117][  700/ 1218]    Overall Loss 0.564826    Objective Loss 0.564826                                        LR 0.000500    Time 0.048869    
2024-06-06 01:52:51,881 - Epoch: [117][  800/ 1218]    Overall Loss 0.565922    Objective Loss 0.565922                                        LR 0.000500    Time 0.048609    
2024-06-06 01:52:56,539 - Epoch: [117][  900/ 1218]    Overall Loss 0.564621    Objective Loss 0.564621                                        LR 0.000500    Time 0.048381    
2024-06-06 01:53:01,365 - Epoch: [117][ 1000/ 1218]    Overall Loss 0.565218    Objective Loss 0.565218                                        LR 0.000500    Time 0.048367    
2024-06-06 01:53:06,086 - Epoch: [117][ 1100/ 1218]    Overall Loss 0.564280    Objective Loss 0.564280                                        LR 0.000500    Time 0.048260    
2024-06-06 01:53:10,717 - Epoch: [117][ 1200/ 1218]    Overall Loss 0.562867    Objective Loss 0.562867                                        LR 0.000500    Time 0.048096    
2024-06-06 01:53:11,512 - Epoch: [117][ 1218/ 1218]    Overall Loss 0.562982    Objective Loss 0.562982    Top1 78.239609    Top5 96.332518    LR 0.000500    Time 0.048038    
2024-06-06 01:53:11,695 - --- validate (epoch=117)-----------
2024-06-06 01:53:11,695 - 34633 samples (256 per mini-batch)
2024-06-06 01:53:17,281 - Epoch: [117][  100/  136]    Loss 0.515302    Top1 75.640625    Top5 96.128906    
2024-06-06 01:53:19,096 - Epoch: [117][  136/  136]    Loss 0.516084    Top1 75.676378    Top5 96.162619    
2024-06-06 01:53:19,295 - ==> Top1: 75.676    Top5: 96.163    Loss: 0.516

2024-06-06 01:53:19,297 - ==> Confusion:
[[ 811    1    2    0    9    4    1    3    7   45    2    0    4    6   12    3    4    4    2    0   11]
 [   2  902    1    0   26   35    8   21    2    1    2    4    6    0    2    3   13    4   15    2   14]
 [   6    4  823    7    6    4   32    9    1    6    6    3    8    5    3    7    9    2   12    7   10]
 [   5    3   18  849    4    5    6    5    0    1   18    1   18    6   35    1    4    5   22    0   10]
 [  19   13    1    1  929   11    3    2    1   16    0    5    3    6   11    7   15    1    3    1    6]
 [   1   31    2    8   23  830    5   36    3    1    0   14   14   32    3    2    6    1    3   11   17]
 [   2    1   27    2    2   10  977    8    4    2    4    3    1    1    1   12    5    4    6   11    3]
 [   3   19   11    3    1   27    9  894    1    2    3   14   13    2    2    1    0    4   43   17    8]
 [  12    2    0    1    2    4    0    3  819   48   19    5   10   18   26    1    5    1   15    2    9]
 [ 116    2    1    0    9    3    2    3   60  758    1    2    4   27    6    1    1    2    0    0    3]
 [   7    6    9   14    2   10    1    8   17    2  907    0    5   19   17    0    1    0   27    2   10]
 [   1    0    2    0    0   14    1    5    3    1    1  845   52   11    0   15    9   17    1   23   10]
 [   3    2    1    2    0    4    1    7    1    1    3   58  810   10    7    8    7   41   10    7   12]
 [   5    2    3    0    6   12    0    4   10   13    9   11   14  869   11    2    6    2    2    6   14]
 [  20    4    4   10   12    1    0    2   37   13    6    4    8    8  931    0    3    6   18    2    9]
 [   1    1    3    0    4    0   12    0    1    0    0   17   17    5    2  957   18   16    3    4    5]
 [   3    9    5    2   14    7    1    1    5    2    1   10    2    6    2   17  961    1    2    5   16]
 [   3    0    1    2    0    0    2    2    2    2    0   20   40    5    5   19    3  887    4    2    6]
 [   6    4    6   13    7    2    1   27    5    1    6    1    2    2   19    2    1    1  944    0    8]
 [   1    5    3    4    5    7   12   11    0    1    2   22   16    6    0    5   12    4    9  945   18]
 [ 332  261  254  160  346  225  156  244  144  130  169  220  491  371  314  239  486  158  287  384 8561]]

2024-06-06 01:53:19,298 - ==> Best [Top1: 76.026   Top5: 95.395   Sparsity:0.00   Params: 169472 on epoch: 113]
2024-06-06 01:53:19,298 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:53:19,306 - 

2024-06-06 01:53:19,306 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:53:25,449 - Epoch: [118][  100/ 1218]    Overall Loss 0.540119    Objective Loss 0.540119                                        LR 0.000500    Time 0.061412    
2024-06-06 01:53:30,135 - Epoch: [118][  200/ 1218]    Overall Loss 0.550734    Objective Loss 0.550734                                        LR 0.000500    Time 0.054123    
2024-06-06 01:53:34,666 - Epoch: [118][  300/ 1218]    Overall Loss 0.552263    Objective Loss 0.552263                                        LR 0.000500    Time 0.051181    
2024-06-06 01:53:39,319 - Epoch: [118][  400/ 1218]    Overall Loss 0.552399    Objective Loss 0.552399                                        LR 0.000500    Time 0.050013    
2024-06-06 01:53:43,869 - Epoch: [118][  500/ 1218]    Overall Loss 0.551353    Objective Loss 0.551353                                        LR 0.000500    Time 0.049107    
2024-06-06 01:53:48,459 - Epoch: [118][  600/ 1218]    Overall Loss 0.551999    Objective Loss 0.551999                                        LR 0.000500    Time 0.048569    
2024-06-06 01:53:53,256 - Epoch: [118][  700/ 1218]    Overall Loss 0.551532    Objective Loss 0.551532                                        LR 0.000500    Time 0.048482    
2024-06-06 01:53:57,996 - Epoch: [118][  800/ 1218]    Overall Loss 0.551877    Objective Loss 0.551877                                        LR 0.000500    Time 0.048344    
2024-06-06 01:54:02,679 - Epoch: [118][  900/ 1218]    Overall Loss 0.553907    Objective Loss 0.553907                                        LR 0.000500    Time 0.048173    
2024-06-06 01:54:07,275 - Epoch: [118][ 1000/ 1218]    Overall Loss 0.554589    Objective Loss 0.554589                                        LR 0.000500    Time 0.047950    
2024-06-06 01:54:12,045 - Epoch: [118][ 1100/ 1218]    Overall Loss 0.556315    Objective Loss 0.556315                                        LR 0.000500    Time 0.047926    
2024-06-06 01:54:16,601 - Epoch: [118][ 1200/ 1218]    Overall Loss 0.556823    Objective Loss 0.556823                                        LR 0.000500    Time 0.047727    
2024-06-06 01:54:17,411 - Epoch: [118][ 1218/ 1218]    Overall Loss 0.556945    Objective Loss 0.556945    Top1 80.440098    Top5 97.310513    LR 0.000500    Time 0.047686    
2024-06-06 01:54:17,586 - --- validate (epoch=118)-----------
2024-06-06 01:54:17,586 - 34633 samples (256 per mini-batch)
2024-06-06 01:54:23,213 - Epoch: [118][  100/  136]    Loss 0.504596    Top1 76.500000    Top5 96.164062    
2024-06-06 01:54:24,769 - Epoch: [118][  136/  136]    Loss 0.500971    Top1 76.418445    Top5 96.139520    
2024-06-06 01:54:24,965 - ==> Top1: 76.418    Top5: 96.140    Loss: 0.501

2024-06-06 01:54:24,967 - ==> Confusion:
[[ 819    2    4    0   10    0    1    2    7   53    1    4    0    4    7    3    2    2    1    1    8]
 [   1  938    4    3   23   17    4   23    5    0    3    2    3    0    1    1    8    1   10    5   11]
 [  10    6  827   12    6    5   25   15    1    4    9    6    5    5    3    3    5    1    7    3   12]
 [   5    1   29  836    6   10    6    4    1    3   23    4   10    4   28    1    1    6   27    1   10]
 [  26   12    1    1  940   13    3    2    3   14    0    3    2    4    7    3    5    0    5    2    8]
 [   1   38    2    3   19  826    7   35    3    3    2   24   11   21    2    1    5    6    8    9   17]
 [   1    6   24    3    4    3  984    6    1    1    3    6    0    1    1    6    2    4    5   13   12]
 [   1   13   12    5    4   37   14  902    0    1    6   13    3    1    1    0    1    4   40   11    8]
 [  11    4    2    3    2    4    0    2  800   66   20    2    4   20   27    1    3    4   13    2   12]
 [  98    2    3    1    8    2    0    2   55  786    2    0    1   18    9    1    2    2    3    1    5]
 [   2    8   14   11    0    4    5    8   15    1  934    1    1   14   12    1    2    1   14    4   12]
 [   0    1    4    0    2   14    8    8    1    2    0  869   32   10    1    7    6   21    2   21    2]
 [   0    2    3    6    1    5    3    6    3    0    2   79  803    6    2    5    3   31   11    8   16]
 [   2    4    1    0    7   11    0    9   19   30    9   19    9  834    5    1    7    4    2   14   14]
 [  11    5    1   22   21    3    0    1   23    7    1    1    5    9  936    0    0    4   30    1   17]
 [   2    3    9    4    6    2   11    0    2    1    0   30    8    1    0  943   12   10    4    5   13]
 [   9   15    1    3   12   12    2    1    1    0    1   13    4    3    3    6  957    4    7    6   12]
 [   2    0    0    3    3    3    1    4    3    1    1   38   31    0    3   12    0  882    6    5    7]
 [   1   12    8   12    4    4    1   25    7    2    3    4    6    0   14    0    1    2  939    3   10]
 [   0    7    8    1    1   11   23   13    2    0    3   27    7    2    0    7    6    3    4  958    5]
 [ 295  332  303  159  371  261  161  285  121  137  223  263  422  328  260  135  354  155  263  352 8752]]

2024-06-06 01:54:24,968 - ==> Best [Top1: 76.418   Top5: 96.140   Sparsity:0.00   Params: 169472 on epoch: 118]
2024-06-06 01:54:24,968 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:54:24,984 - 

2024-06-06 01:54:24,985 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:54:30,958 - Epoch: [119][  100/ 1218]    Overall Loss 0.549141    Objective Loss 0.549141                                        LR 0.000500    Time 0.059711    
2024-06-06 01:54:35,641 - Epoch: [119][  200/ 1218]    Overall Loss 0.549414    Objective Loss 0.549414                                        LR 0.000500    Time 0.053263    
2024-06-06 01:54:40,298 - Epoch: [119][  300/ 1218]    Overall Loss 0.550923    Objective Loss 0.550923                                        LR 0.000500    Time 0.051027    
2024-06-06 01:54:45,045 - Epoch: [119][  400/ 1218]    Overall Loss 0.553078    Objective Loss 0.553078                                        LR 0.000500    Time 0.050134    
2024-06-06 01:54:49,740 - Epoch: [119][  500/ 1218]    Overall Loss 0.554060    Objective Loss 0.554060                                        LR 0.000500    Time 0.049493    
2024-06-06 01:54:54,452 - Epoch: [119][  600/ 1218]    Overall Loss 0.553409    Objective Loss 0.553409                                        LR 0.000500    Time 0.049095    
2024-06-06 01:54:59,063 - Epoch: [119][  700/ 1218]    Overall Loss 0.553580    Objective Loss 0.553580                                        LR 0.000500    Time 0.048666    
2024-06-06 01:55:03,789 - Epoch: [119][  800/ 1218]    Overall Loss 0.553835    Objective Loss 0.553835                                        LR 0.000500    Time 0.048488    
2024-06-06 01:55:08,511 - Epoch: [119][  900/ 1218]    Overall Loss 0.554229    Objective Loss 0.554229                                        LR 0.000500    Time 0.048344    
2024-06-06 01:55:13,124 - Epoch: [119][ 1000/ 1218]    Overall Loss 0.554717    Objective Loss 0.554717                                        LR 0.000500    Time 0.048122    
2024-06-06 01:55:17,731 - Epoch: [119][ 1100/ 1218]    Overall Loss 0.555374    Objective Loss 0.555374                                        LR 0.000500    Time 0.047933    
2024-06-06 01:55:22,398 - Epoch: [119][ 1200/ 1218]    Overall Loss 0.555214    Objective Loss 0.555214                                        LR 0.000500    Time 0.047826    
2024-06-06 01:55:23,218 - Epoch: [119][ 1218/ 1218]    Overall Loss 0.554891    Objective Loss 0.554891    Top1 76.528117    Top5 95.843521    LR 0.000500    Time 0.047793    
2024-06-06 01:55:23,382 - --- validate (epoch=119)-----------
2024-06-06 01:55:23,382 - 34633 samples (256 per mini-batch)
2024-06-06 01:55:29,055 - Epoch: [119][  100/  136]    Loss 0.512108    Top1 75.449219    Top5 95.605469    
2024-06-06 01:55:30,620 - Epoch: [119][  136/  136]    Loss 0.507624    Top1 75.537782    Top5 95.645771    
2024-06-06 01:55:30,816 - ==> Top1: 75.538    Top5: 95.646    Loss: 0.508

2024-06-06 01:55:30,817 - ==> Confusion:
[[ 813    0    2    2   21    2    1    3    6   51    0    2    0    4    5    4    1    3    6    2    3]
 [   1  943    5    2   24   14    3   23    4    0    2    5    2    2    9    3    3    2    9    5    2]
 [  10    5  843   12    5    4   21   16    0    7    3    5    4    4    4    7    5    0    3    8    4]
 [   6    4   21  863    4    8    1    3    3    0   21    3    5    6   22    6    5    7   18    2    8]
 [  34   15    1    0  923    9    1    6    1   10    2    4    3    6   12    7    6    4    6    1    3]
 [   6   38    5    6   17  801    5   63    1    4    6   18    7   24    4    4    7    1    3   15    8]
 [   3    5   33    2    1   11  966    7    1    3    0    3    2    0    2    9    2    4    5   17   10]
 [   3   14   13    2    1   29    6  929    4    2    4    9    3    1    2    1    1    2   27   17    7]
 [  14    4    1    1    8    1    0    5  798   67   17    4    5   17   32    0    4    5   13    2    4]
 [ 106    0    2    1   15    4    0    1   52  771    1    1    1   17   13    2    0    0    2    2   10]
 [   5    5   15    9    4    7    3    8   13    2  933    0    3   13   12    0    1    2   19    5    5]
 [   1    1    4    1    4   10    3   12    0    2    1  869   23   13    1   14    4   16    5   22    5]
 [   3    2    4    7    2    2    3    4    1    1    1   94  790    6    5    5    6   30   11   11    7]
 [   2    3    0    0    9   11    1    3   22   27    9   16    7  852   11    3    2    1    2   10   10]
 [  13    5    1   16   10    3    1    1   29    8    5    5    5    8  962    0    3    4   11    1    7]
 [   3    0    5    1    4    3   10    2    0    1    0   25    8    1    1  963    6   12    2   10    9]
 [   6   10    7    1   14   12    5    2    6    0    1   12    1    4    3   12  936    6    0   16   18]
 [   3    0    1    6    1    3    4    7    4    2    0   31   26    2    2   17    2  880    1    4    9]
 [   5    6    6   10    5    4    1   35    6    1    8    1    5    0   24    2    3    3  914    8   11]
 [   0    7    4    1    5    7   20   20    1    0    3   22    5    5    0    7    4    3    3  959   12]
 [ 362  349  307  157  366  221  128  279  138  173  229  267  364  319  305  243  376  170  281  445 8453]]

2024-06-06 01:55:30,819 - ==> Best [Top1: 76.418   Top5: 96.140   Sparsity:0.00   Params: 169472 on epoch: 118]
2024-06-06 01:55:30,819 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:55:30,827 - 

2024-06-06 01:55:30,827 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:55:37,046 - Epoch: [120][  100/ 1218]    Overall Loss 0.568148    Objective Loss 0.568148                                        LR 0.000500    Time 0.062174    
2024-06-06 01:55:41,689 - Epoch: [120][  200/ 1218]    Overall Loss 0.564315    Objective Loss 0.564315                                        LR 0.000500    Time 0.054292    
2024-06-06 01:55:46,270 - Epoch: [120][  300/ 1218]    Overall Loss 0.566063    Objective Loss 0.566063                                        LR 0.000500    Time 0.051460    
2024-06-06 01:55:50,981 - Epoch: [120][  400/ 1218]    Overall Loss 0.561486    Objective Loss 0.561486                                        LR 0.000500    Time 0.050367    
2024-06-06 01:55:55,603 - Epoch: [120][  500/ 1218]    Overall Loss 0.560851    Objective Loss 0.560851                                        LR 0.000500    Time 0.049533    
2024-06-06 01:56:00,118 - Epoch: [120][  600/ 1218]    Overall Loss 0.560003    Objective Loss 0.560003                                        LR 0.000500    Time 0.048799    
2024-06-06 01:56:04,756 - Epoch: [120][  700/ 1218]    Overall Loss 0.559850    Objective Loss 0.559850                                        LR 0.000500    Time 0.048451    
2024-06-06 01:56:09,478 - Epoch: [120][  800/ 1218]    Overall Loss 0.559547    Objective Loss 0.559547                                        LR 0.000500    Time 0.048296    
2024-06-06 01:56:14,288 - Epoch: [120][  900/ 1218]    Overall Loss 0.558860    Objective Loss 0.558860                                        LR 0.000500    Time 0.048271    
2024-06-06 01:56:18,797 - Epoch: [120][ 1000/ 1218]    Overall Loss 0.557731    Objective Loss 0.557731                                        LR 0.000500    Time 0.047952    
2024-06-06 01:56:23,429 - Epoch: [120][ 1100/ 1218]    Overall Loss 0.557502    Objective Loss 0.557502                                        LR 0.000500    Time 0.047801    
2024-06-06 01:56:28,610 - Epoch: [120][ 1200/ 1218]    Overall Loss 0.556858    Objective Loss 0.556858                                        LR 0.000500    Time 0.048134    
2024-06-06 01:56:29,441 - Epoch: [120][ 1218/ 1218]    Overall Loss 0.556928    Objective Loss 0.556928    Top1 75.550122    Top5 97.555012    LR 0.000500    Time 0.048105    
2024-06-06 01:56:29,622 - --- validate (epoch=120)-----------
2024-06-06 01:56:29,622 - 34633 samples (256 per mini-batch)
2024-06-06 01:56:35,297 - Epoch: [120][  100/  136]    Loss 0.505668    Top1 74.773438    Top5 95.546875    
2024-06-06 01:56:36,972 - Epoch: [120][  136/  136]    Loss 0.504718    Top1 74.833252    Top5 95.515837    
2024-06-06 01:56:37,148 - ==> Top1: 74.833    Top5: 95.516    Loss: 0.505

2024-06-06 01:56:37,149 - ==> Confusion:
[[ 804    1    3    1    6    2    0    1    9   72    0    3    2    5    7    1    4    1    4    2    3]
 [   5  898    2    1   27   33    7   14    7    3    4    8    2    0    8    2   11    3   14    6    8]
 [  11    2  831   17    3    5   22   12    2    6    5    5    7    8    3    5    4    2    7    6    7]
 [   3    3   22  856    4    6    4    4    3    2   16    4    7    5   34    3    2    9   22    0    7]
 [  27    9    2    0  932   10    0    3    4   13    1    3    1    4   14    4    8    3    7    0    9]
 [   5   32    5    3   17  831    6   37    5    6    2   20    8   20    3    4    6    3    7   11   12]
 [   3    8   32    5    4    6  971    6    0    1    4    4    2    0    0   12    1    3    2   16    6]
 [   9   16   14    2    5   38    8  883    2    5    6   13    2    2    3    1    3    3   35   22    5]
 [  17    7    0    1    3    2    0    2  827   57   16    0    4   17   26    0    4    1    8    1    9]
 [  80    1    3    0    4    2    2    2   50  820    0    1    2   16    6    2    0    3    0    2    5]
 [   0    7    5   22    0    7    2    4   28    2  927    3    0   10   20    0    4    1   16    2    4]
 [   5    1    2    2    1   14    4    3    1    0    1  868   26    9    2   10    7   32    3   17    3]
 [   3    3    0    7    1    4    2    5    3    0    2   74  804    2    5    5    2   50    8    9    6]
 [   3    1    4    1    7   15    1    1   20   23    6   20    4  862    7    3    5    3    1    8    6]
 [  17    0    0   14    6    1    0    2   36    8    2    4    4    7  968    0    1    6   17    0    5]
 [   2    1    7    2    3    1   10    2    1    3    2   20   10    4    0  948   14   18    2    7    9]
 [   5   10    6    0    9    8    4    3    5    4    2   11    3    3    6    6  964    2    3    9    9]
 [   5    2    1    2    1    0    1    4    0    3    0   16   34    3    8    8    3  903    4    3    4]
 [   1    8   11   21    5    3    2   23    5    2    4    2    4    1   25    1    3    3  923    5    6]
 [   3    5    1    0    3    9   16   11    2    0    0   38   15    6    1   11    5    6    6  943    7]
 [ 369  285  289  178  323  233  133  248  203  197  220  308  430  371  355  242  462  204  317  411 8154]]

2024-06-06 01:56:37,151 - ==> Best [Top1: 76.418   Top5: 96.140   Sparsity:0.00   Params: 169472 on epoch: 118]
2024-06-06 01:56:37,151 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:56:37,158 - 

2024-06-06 01:56:37,158 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:56:43,543 - Epoch: [121][  100/ 1218]    Overall Loss 0.559404    Objective Loss 0.559404                                        LR 0.000500    Time 0.063827    
2024-06-06 01:56:48,527 - Epoch: [121][  200/ 1218]    Overall Loss 0.553589    Objective Loss 0.553589                                        LR 0.000500    Time 0.056822    
2024-06-06 01:56:53,222 - Epoch: [121][  300/ 1218]    Overall Loss 0.552334    Objective Loss 0.552334                                        LR 0.000500    Time 0.053526    
2024-06-06 01:56:58,067 - Epoch: [121][  400/ 1218]    Overall Loss 0.551020    Objective Loss 0.551020                                        LR 0.000500    Time 0.052252    
2024-06-06 01:57:02,666 - Epoch: [121][  500/ 1218]    Overall Loss 0.550123    Objective Loss 0.550123                                        LR 0.000500    Time 0.050996    
2024-06-06 01:57:07,322 - Epoch: [121][  600/ 1218]    Overall Loss 0.550357    Objective Loss 0.550357                                        LR 0.000500    Time 0.050254    
2024-06-06 01:57:12,044 - Epoch: [121][  700/ 1218]    Overall Loss 0.552194    Objective Loss 0.552194                                        LR 0.000500    Time 0.049819    
2024-06-06 01:57:16,714 - Epoch: [121][  800/ 1218]    Overall Loss 0.554835    Objective Loss 0.554835                                        LR 0.000500    Time 0.049426    
2024-06-06 01:57:21,338 - Epoch: [121][  900/ 1218]    Overall Loss 0.556650    Objective Loss 0.556650                                        LR 0.000500    Time 0.049070    
2024-06-06 01:57:25,881 - Epoch: [121][ 1000/ 1218]    Overall Loss 0.556641    Objective Loss 0.556641                                        LR 0.000500    Time 0.048704    
2024-06-06 01:57:30,520 - Epoch: [121][ 1100/ 1218]    Overall Loss 0.556608    Objective Loss 0.556608                                        LR 0.000500    Time 0.048493    
2024-06-06 01:57:35,078 - Epoch: [121][ 1200/ 1218]    Overall Loss 0.556588    Objective Loss 0.556588                                        LR 0.000500    Time 0.048248    
2024-06-06 01:57:35,862 - Epoch: [121][ 1218/ 1218]    Overall Loss 0.556730    Objective Loss 0.556730    Top1 77.017115    Top5 95.843521    LR 0.000500    Time 0.048178    
2024-06-06 01:57:36,038 - --- validate (epoch=121)-----------
2024-06-06 01:57:36,038 - 34633 samples (256 per mini-batch)
2024-06-06 01:57:41,588 - Epoch: [121][  100/  136]    Loss 0.521557    Top1 76.023438    Top5 95.726562    
2024-06-06 01:57:43,292 - Epoch: [121][  136/  136]    Loss 0.510645    Top1 76.161464    Top5 95.847891    
2024-06-06 01:57:43,483 - ==> Top1: 76.161    Top5: 95.848    Loss: 0.511

2024-06-06 01:57:43,484 - ==> Confusion:
[[ 783    0    7    0   21    1    0    5    6   65    1    2    1    5   11    2    9    1    3    0    8]
 [   0  915    6    2   27   32    5   18    3    1    2    5    2    1    7    3    9    4    9    3    9]
 [   3    1  862    5    6    3   18   13    1    5    2    1    3    6    4    4    7    3    3    7   13]
 [   1    1   27  859    6    4    4    3    2    3   13    3    9    5   28    3    3   11   12    2   17]
 [  23   10    2    1  938   11    2    0    1   10    0    1    2    7   11    3   13    3    3    3   10]
 [   4   49    4    1   19  802    6   37    5    9    4   17    6   28    6    4    8    4    2   14   14]
 [   0    2   34    1    1    2  987    0    1    1    5    4    1    1    1    9    4    3    2   14   13]
 [   2   17   17    5    9   39   10  867    0    1    5   14    6    4    5    2    1    1   38   20   14]
 [  13   11    0    1    3    0    0    0  784   65   17    2    5   31   35    0    6    3   12    4   10]
 [  88    0    5    0   21    7    0    0   46  774    2    1    3   19    9    1    2    7    1    0   15]
 [   1    8   16   14    3    6    3    4   21    3  917    1    1   10   23    0    3    1   13    2   14]
 [   5    1    3    1    4   15    4   10    1    2    0  851   28   13    2   10    7   19    5   19   11]
 [   0    1    5    8    1    7    2    2    4    1    1   89  787    2    5    5    9   35   12    6   13]
 [   2    0    4    2    4    4    1    3   15   23   10   14    6  869   12    4    3    4    1   13    7]
 [  11    4    3   10   15    4    0    2   30    5    5    2    4    9  969    0    1    3   11    1    9]
 [   1    2    5    0    9    0    7    2    1    1    0   15   10    0    0  977   11   19    0    2    4]
 [   5    5    5    3   11    8    1    1    5    0    3    7    5    5    4   12  954    3    7   12   16]
 [   5    2    2    4    4    2    4    1    0    3    0   19   27    7    6   21    5  876    2    7    8]
 [   2   10   13   19    5    4    1   23    3    1    5    3    4    1   23    0    2    3  929    1    6]
 [   1    9    6    1    3    9   19   13    1    0    1   24    7    2    6   14   13    3    1  946    9]
 [ 232  293  359  138  341  231  139  221  119  146  174  212  382  415  345  199  453  148  269  385 8731]]

2024-06-06 01:57:43,486 - ==> Best [Top1: 76.418   Top5: 96.140   Sparsity:0.00   Params: 169472 on epoch: 118]
2024-06-06 01:57:43,486 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:57:43,493 - 

2024-06-06 01:57:43,494 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:57:49,709 - Epoch: [122][  100/ 1218]    Overall Loss 0.549658    Objective Loss 0.549658                                        LR 0.000500    Time 0.062130    
2024-06-06 01:57:54,350 - Epoch: [122][  200/ 1218]    Overall Loss 0.550441    Objective Loss 0.550441                                        LR 0.000500    Time 0.054260    
2024-06-06 01:57:59,057 - Epoch: [122][  300/ 1218]    Overall Loss 0.556578    Objective Loss 0.556578                                        LR 0.000500    Time 0.051857    
2024-06-06 01:58:03,625 - Epoch: [122][  400/ 1218]    Overall Loss 0.559030    Objective Loss 0.559030                                        LR 0.000500    Time 0.050309    
2024-06-06 01:58:08,327 - Epoch: [122][  500/ 1218]    Overall Loss 0.558585    Objective Loss 0.558585                                        LR 0.000500    Time 0.049646    
2024-06-06 01:58:12,933 - Epoch: [122][  600/ 1218]    Overall Loss 0.556019    Objective Loss 0.556019                                        LR 0.000500    Time 0.049046    
2024-06-06 01:58:17,489 - Epoch: [122][  700/ 1218]    Overall Loss 0.554278    Objective Loss 0.554278                                        LR 0.000500    Time 0.048546    
2024-06-06 01:58:22,112 - Epoch: [122][  800/ 1218]    Overall Loss 0.555312    Objective Loss 0.555312                                        LR 0.000500    Time 0.048253    
2024-06-06 01:58:26,857 - Epoch: [122][  900/ 1218]    Overall Loss 0.556046    Objective Loss 0.556046                                        LR 0.000500    Time 0.048163    
2024-06-06 01:58:31,577 - Epoch: [122][ 1000/ 1218]    Overall Loss 0.557487    Objective Loss 0.557487                                        LR 0.000500    Time 0.048064    
2024-06-06 01:58:36,333 - Epoch: [122][ 1100/ 1218]    Overall Loss 0.557299    Objective Loss 0.557299                                        LR 0.000500    Time 0.048016    
2024-06-06 01:58:40,954 - Epoch: [122][ 1200/ 1218]    Overall Loss 0.556693    Objective Loss 0.556693                                        LR 0.000500    Time 0.047864    
2024-06-06 01:58:41,855 - Epoch: [122][ 1218/ 1218]    Overall Loss 0.557224    Objective Loss 0.557224    Top1 74.083130    Top5 96.088020    LR 0.000500    Time 0.047896    
2024-06-06 01:58:42,048 - --- validate (epoch=122)-----------
2024-06-06 01:58:42,048 - 34633 samples (256 per mini-batch)
2024-06-06 01:58:47,713 - Epoch: [122][  100/  136]    Loss 0.515175    Top1 74.617188    Top5 95.375000    
2024-06-06 01:58:49,512 - Epoch: [122][  136/  136]    Loss 0.508753    Top1 74.879450    Top5 95.443652    
2024-06-06 01:58:49,700 - ==> Top1: 74.879    Top5: 95.444    Loss: 0.509

2024-06-06 01:58:49,701 - ==> Confusion:
[[ 812    0    4    0    7    1    2    3    9   54    1    3    1    5   13    3    3    0    2    4    4]
 [   3  929    4    1   22   17    1   14    2    4    6    4    3    0   10    2   14    4   10    4    9]
 [  10    5  817    6    3    2   39   11    2    5    5    4    7    6    8    6   13    2    2    6   11]
 [   4    2   23  864    2    4    6    4    3    3   19    1   10    6   38    1    1    7    5    2   11]
 [  46    8    5    0  920    5    0    1    2    8    1    2    2    4   14    5    9    2    3    4   13]
 [   3   40    1    5   20  824    8   54    4    3    3   20    3   19    3    1    8    2    4   10    8]
 [   5    3   32    2    5    2  969    8    0    2    8    4    1    3    0   11    2    7    2   10   10]
 [   1   18   11    3    6   33   10  881   10    0   11   10    5    5    3    2    2    1   30   26    9]
 [  10    7    3    2    0    2    0    2  832   48   13    1    5   25   35    0    2    4    7    0    4]
 [  89    2    3    0    8    3    1    2   58  782    2    1    0   21   13    1    1    5    1    2    6]
 [   2    5   17   14    1    6    4    9   17    1  927    0    2   22   13    1    2    1   11    1    8]
 [   6    2    3    1    0   18    4    9    0    1    2  831   41   15    0   16    6   23    1   27    5]
 [   2    5    3    8    0    7    1    2    5    0    2   66  816    2    1   10    2   36    8    9   10]
 [   6    1    0    0   10   15    2    1   18   17    7   11    7  870    7    2    3    5    2    6   11]
 [   8    6    2   11    8    1    1    0   35   15    5    6    4   12  949    1    1    7   18    1    7]
 [   2    3    3    1    4    0    5    1    3    0    0   30   13    7    0  953   12   15    1    6    7]
 [   2   11    1    2   12   10    2    1    8    0    1    7    8    7    3    8  966    4    2    7   10]
 [   2    1    3    4    1    2    3    2    4    2    1   18   34    8    3   21    5  873    4    6    8]
 [   2   11   15   17    5    4    2   27   11    1    6    2    6    2   23    0    0    3  909    5    7]
 [   1    8    6    0    0   11   16   12    4    0    1   20   13   10    0    7   14    1    3  954    7]
 [ 300  297  371  115  360  243  166  248  182  150  246  254  423  459  329  256  462  177  223  416 8255]]

2024-06-06 01:58:49,703 - ==> Best [Top1: 76.418   Top5: 96.140   Sparsity:0.00   Params: 169472 on epoch: 118]
2024-06-06 01:58:49,703 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:58:49,711 - 

2024-06-06 01:58:49,711 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 01:58:55,899 - Epoch: [123][  100/ 1218]    Overall Loss 0.548880    Objective Loss 0.548880                                        LR 0.000500    Time 0.061863    
2024-06-06 01:59:00,658 - Epoch: [123][  200/ 1218]    Overall Loss 0.550495    Objective Loss 0.550495                                        LR 0.000500    Time 0.054716    
2024-06-06 01:59:05,458 - Epoch: [123][  300/ 1218]    Overall Loss 0.558451    Objective Loss 0.558451                                        LR 0.000500    Time 0.052471    
2024-06-06 01:59:10,055 - Epoch: [123][  400/ 1218]    Overall Loss 0.557032    Objective Loss 0.557032                                        LR 0.000500    Time 0.050842    
2024-06-06 01:59:14,749 - Epoch: [123][  500/ 1218]    Overall Loss 0.555567    Objective Loss 0.555567                                        LR 0.000500    Time 0.050059    
2024-06-06 01:59:19,398 - Epoch: [123][  600/ 1218]    Overall Loss 0.552894    Objective Loss 0.552894                                        LR 0.000500    Time 0.049460    
2024-06-06 01:59:24,087 - Epoch: [123][  700/ 1218]    Overall Loss 0.554233    Objective Loss 0.554233                                        LR 0.000500    Time 0.049091    
2024-06-06 01:59:28,620 - Epoch: [123][  800/ 1218]    Overall Loss 0.553954    Objective Loss 0.553954                                        LR 0.000500    Time 0.048619    
2024-06-06 01:59:33,471 - Epoch: [123][  900/ 1218]    Overall Loss 0.554176    Objective Loss 0.554176                                        LR 0.000500    Time 0.048605    
2024-06-06 01:59:38,140 - Epoch: [123][ 1000/ 1218]    Overall Loss 0.554042    Objective Loss 0.554042                                        LR 0.000500    Time 0.048412    
2024-06-06 01:59:42,765 - Epoch: [123][ 1100/ 1218]    Overall Loss 0.553887    Objective Loss 0.553887                                        LR 0.000500    Time 0.048214    
2024-06-06 01:59:47,728 - Epoch: [123][ 1200/ 1218]    Overall Loss 0.555067    Objective Loss 0.555067                                        LR 0.000500    Time 0.048330    
2024-06-06 01:59:48,509 - Epoch: [123][ 1218/ 1218]    Overall Loss 0.555182    Objective Loss 0.555182    Top1 72.616137    Top5 95.110024    LR 0.000500    Time 0.048256    
2024-06-06 01:59:48,698 - --- validate (epoch=123)-----------
2024-06-06 01:59:48,699 - 34633 samples (256 per mini-batch)
2024-06-06 01:59:54,375 - Epoch: [123][  100/  136]    Loss 0.507446    Top1 76.523438    Top5 96.003906    
2024-06-06 01:59:56,044 - Epoch: [123][  136/  136]    Loss 0.508919    Top1 76.484855    Top5 96.012474    
2024-06-06 01:59:56,211 - ==> Top1: 76.485    Top5: 96.012    Loss: 0.509

2024-06-06 01:59:56,213 - ==> Confusion:
[[ 771    3    4    1   11    2    0    1   10   87    1    4    0    5   12    1    3    1    5    0    9]
 [   3  942    0    3   21   11    3   11    6    1    6    2    4    0    5    1    9    4   21    3    7]
 [   5    9  825   24    5    1   23    8    0    7    3    5    4    9    1    4    5    2    6    8   16]
 [   4    4   23  886    2    5    3    2    4    1   15    2    6    3   22    1    3    7   16    2    5]
 [  35   13    3    5  893    9    0    4    4   23    3    3    2    7   15    4   13    2    5    1   10]
 [   3   66    3    7   12  798    6   26    3    6    7   13   12   31    6    0    7    3    7   17   10]
 [   0    8   29    3    1    3  968    9    2    2    3    5    8    1    0    5    5    0    6   16   12]
 [   3   23   20    1    3   29    8  876    0    3    6   10    7    5    1    0    1    1   48   26    6]
 [  12    6    1    1    3    2    0    0  849   39   15    1    3   21   20    1    4    2   11    1   10]
 [  66    1    1    0    6    0    0    3   75  801    3    1    2   23    8    0    1    1    2    2    5]
 [   1   12   12   23    2    0    2    5   19    3  933    1    2   12   13    0    1    1   11    2    9]
 [   2    4    5    1    2   12    0   11    2    3    1  845   50    8    1   11    7   20    4   16    6]
 [   0    1    1    7    0    4    3    8    4    0    3   63  830    2    3    2    6   29   11    5   13]
 [   2    2    2    3    5   13    0    6   20   20   14    7    8  856   10    2    3    4    5    6   13]
 [   8    6    1   23    5    1    0    1   38    9    3    1    4    3  951    1    1    2   29    0   11]
 [   2    4    6    2    3    1    5    1    1    2    2   21   13    4    0  952   10   16    3    5   13]
 [   1   20    3    3    7    5    3    0    9    2    3    7    5    5    1   17  939    1    3   11   27]
 [   4    1    0    3    1    1    2    1    4    1    0   24   42    7    1    8    1  888    2    6    8]
 [   5    9    8   23    1    4    1   20    5    3    7    1    5    2   17    1    2    2  932    2    8]
 [   4    9    1    1    2   10   15    8    1    0    0   18   14    3    0    7    8    5   11  949   22]
 [ 263  351  322  228  219  166  107  220  195  169  235  158  468  351  309  168  339  148  349  362 8805]]

2024-06-06 01:59:56,214 - ==> Best [Top1: 76.485   Top5: 96.012   Sparsity:0.00   Params: 169472 on epoch: 123]
2024-06-06 01:59:56,215 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 01:59:56,230 - 

2024-06-06 01:59:56,230 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:00:02,648 - Epoch: [124][  100/ 1218]    Overall Loss 0.545261    Objective Loss 0.545261                                        LR 0.000500    Time 0.064158    
2024-06-06 02:00:07,590 - Epoch: [124][  200/ 1218]    Overall Loss 0.551052    Objective Loss 0.551052                                        LR 0.000500    Time 0.056781    
2024-06-06 02:00:12,375 - Epoch: [124][  300/ 1218]    Overall Loss 0.550416    Objective Loss 0.550416                                        LR 0.000500    Time 0.053798    
2024-06-06 02:00:17,035 - Epoch: [124][  400/ 1218]    Overall Loss 0.551023    Objective Loss 0.551023                                        LR 0.000500    Time 0.051995    
2024-06-06 02:00:21,724 - Epoch: [124][  500/ 1218]    Overall Loss 0.546645    Objective Loss 0.546645                                        LR 0.000500    Time 0.050962    
2024-06-06 02:00:26,363 - Epoch: [124][  600/ 1218]    Overall Loss 0.549496    Objective Loss 0.549496                                        LR 0.000500    Time 0.050197    
2024-06-06 02:00:30,995 - Epoch: [124][  700/ 1218]    Overall Loss 0.550921    Objective Loss 0.550921                                        LR 0.000500    Time 0.049641    
2024-06-06 02:00:35,658 - Epoch: [124][  800/ 1218]    Overall Loss 0.550691    Objective Loss 0.550691                                        LR 0.000500    Time 0.049262    
2024-06-06 02:00:40,496 - Epoch: [124][  900/ 1218]    Overall Loss 0.550577    Objective Loss 0.550577                                        LR 0.000500    Time 0.049162    
2024-06-06 02:00:45,138 - Epoch: [124][ 1000/ 1218]    Overall Loss 0.549774    Objective Loss 0.549774                                        LR 0.000500    Time 0.048885    
2024-06-06 02:00:49,882 - Epoch: [124][ 1100/ 1218]    Overall Loss 0.549433    Objective Loss 0.549433                                        LR 0.000500    Time 0.048753    
2024-06-06 02:00:54,831 - Epoch: [124][ 1200/ 1218]    Overall Loss 0.548442    Objective Loss 0.548442                                        LR 0.000500    Time 0.048813    
2024-06-06 02:00:55,676 - Epoch: [124][ 1218/ 1218]    Overall Loss 0.548496    Objective Loss 0.548496    Top1 71.882641    Top5 95.110024    LR 0.000500    Time 0.048785    
2024-06-06 02:00:55,848 - --- validate (epoch=124)-----------
2024-06-06 02:00:55,849 - 34633 samples (256 per mini-batch)
2024-06-06 02:01:01,477 - Epoch: [124][  100/  136]    Loss 0.499443    Top1 75.480469    Top5 95.851562    
2024-06-06 02:01:03,148 - Epoch: [124][  136/  136]    Loss 0.505004    Top1 75.624404    Top5 95.845003    
2024-06-06 02:01:03,339 - ==> Top1: 75.624    Top5: 95.845    Loss: 0.505

2024-06-06 02:01:03,340 - ==> Confusion:
[[ 776    2    8    1   22    0    0    2    8   72    2    2    0    6   13    0    2    0    7    1    7]
 [   2  906    2    8   26   23    8   23    9    1    1    6    2    0    7    1   10    1   18    4    5]
 [   5    1  841   11    3    4   30   12    0    7    9    6    2    5    2    3    9    1    7    3    9]
 [   6    3   20  862    6    9    5    6    1    3    9    0   13    1   32    2    0    5   22    2    9]
 [  20   13    6    1  933   13    2    5    2   17    2    5    1    4   12    3    6    1    4    3    1]
 [   2   30    4    6   19  851    6   43    2    3    2   11    7   23    5    1    4    2    8    9    5]
 [   1    4   26    3    1    5  975    9    1    2    5   11    2    1    0    5    4    4    7   14    6]
 [   2   17   15    2    3   35    3  907    3    3    7   11    4    1    0    1    1    0   39   16    7]
 [  10    8    1    0    3    6    0    4  810   66   20    3    4   18   18    2    5    0   17    0    7]
 [  64    3    3    1   12    4    1    4   62  792    2    1    1   25    9    2    3    4    3    0    5]
 [   2    5   18   18    4    2    3   10   21    3  917    1    3    6   12    1    1    0   26    2    9]
 [   3    2    7    1    0   14    3    7    0    2    1  877   35    7    1    5    3   11    6   16   10]
 [   2    0    3    7    0    8    3    4    2    1    4   94  792    0    3    8    6   24   14    2   18]
 [   4    2    4    3   12   13    0    7   16   20    7   16    5  851    5    7    5    2    1   10   11]
 [   9    9    3   17   11    0    1    6   25   12    5    3    4    9  950    1    3    3   17    2    8]
 [   3    2    3    0    3    4    7    0    0    0    0   31   17    3    0  949   19   14    3    3    5]
 [   4    6    9    4   10   11    1    2    3    2    2   11    5    7    6   10  940    2    7    7   23]
 [   0    0    2    5    2    2    2    3    0    3    1   24   43    3    4   21    2  869    5    2   12]
 [   3    7   11   14    7    2    4   19    4    1    4    1    3    1   14    0    3    0  952    3    5]
 [   2   10    5    0    0   11   16   14    1    0    1   31    5    4    2    4    7    2    7  958    8]
 [ 296  315  363  187  326  283  164  265  159  151  198  287  413  328  314  174  377  129  351  369 8483]]

2024-06-06 02:01:03,342 - ==> Best [Top1: 76.485   Top5: 96.012   Sparsity:0.00   Params: 169472 on epoch: 123]
2024-06-06 02:01:03,342 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:01:03,349 - 

2024-06-06 02:01:03,350 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:01:09,783 - Epoch: [125][  100/ 1218]    Overall Loss 0.550903    Objective Loss 0.550903                                        LR 0.000500    Time 0.064308    
2024-06-06 02:01:14,710 - Epoch: [125][  200/ 1218]    Overall Loss 0.549671    Objective Loss 0.549671                                        LR 0.000500    Time 0.056784    
2024-06-06 02:01:19,615 - Epoch: [125][  300/ 1218]    Overall Loss 0.555045    Objective Loss 0.555045                                        LR 0.000500    Time 0.054198    
2024-06-06 02:01:24,388 - Epoch: [125][  400/ 1218]    Overall Loss 0.556596    Objective Loss 0.556596                                        LR 0.000500    Time 0.052577    
2024-06-06 02:01:29,179 - Epoch: [125][  500/ 1218]    Overall Loss 0.557804    Objective Loss 0.557804                                        LR 0.000500    Time 0.051639    
2024-06-06 02:01:33,980 - Epoch: [125][  600/ 1218]    Overall Loss 0.557573    Objective Loss 0.557573                                        LR 0.000500    Time 0.051031    
2024-06-06 02:01:38,788 - Epoch: [125][  700/ 1218]    Overall Loss 0.557075    Objective Loss 0.557075                                        LR 0.000500    Time 0.050608    
2024-06-06 02:01:43,371 - Epoch: [125][  800/ 1218]    Overall Loss 0.557687    Objective Loss 0.557687                                        LR 0.000500    Time 0.050008    
2024-06-06 02:01:48,072 - Epoch: [125][  900/ 1218]    Overall Loss 0.557876    Objective Loss 0.557876                                        LR 0.000500    Time 0.049673    
2024-06-06 02:01:52,594 - Epoch: [125][ 1000/ 1218]    Overall Loss 0.558698    Objective Loss 0.558698                                        LR 0.000500    Time 0.049226    
2024-06-06 02:01:57,418 - Epoch: [125][ 1100/ 1218]    Overall Loss 0.557946    Objective Loss 0.557946                                        LR 0.000500    Time 0.049135    
2024-06-06 02:02:02,059 - Epoch: [125][ 1200/ 1218]    Overall Loss 0.556909    Objective Loss 0.556909                                        LR 0.000500    Time 0.048907    
2024-06-06 02:02:02,831 - Epoch: [125][ 1218/ 1218]    Overall Loss 0.556809    Objective Loss 0.556809    Top1 74.572127    Top5 96.088020    LR 0.000500    Time 0.048818    
2024-06-06 02:02:03,004 - --- validate (epoch=125)-----------
2024-06-06 02:02:03,004 - 34633 samples (256 per mini-batch)
2024-06-06 02:02:08,566 - Epoch: [125][  100/  136]    Loss 0.496385    Top1 76.324219    Top5 95.878906    
2024-06-06 02:02:10,251 - Epoch: [125][  136/  136]    Loss 0.501602    Top1 75.993994    Top5 95.845003    
2024-06-06 02:02:10,449 - ==> Top1: 75.994    Top5: 95.845    Loss: 0.502

2024-06-06 02:02:10,451 - ==> Confusion:
[[ 805    0    2    1   13    1    0    1   10   62    1    2    3    4    8    2    3    4    1    0    8]
 [   2  922    2    4   32   11    5   16    6    2    5    4    3    1    2    3   13    1   13    5   11]
 [  10    2  838    7    6    2   20   11    0    5    7    7    3    5    2    9    6    1    4   10   15]
 [   4    1   34  840    4    7    5    3    3    1   26    2    9    2   32    2    5    7    9    4   16]
 [  25   14    1    0  939    6    1    3    4   16    2    4    2    5    9    6    4    1    2    1    9]
 [   5   61    5    4   29  788   10   41    6    6    1   18    0   24    1    0    8    6    4   17    9]
 [   4    7   28    1    3    3  984    3    0    2    5    7    2    2    0    7    2    5    2    9   10]
 [   5   38   19    3    6   27    6  879    3    3    6   12    5    1    1    2    0    5   31   14   11]
 [  12    3    1    0    3    5    0    0  860   43   12    0    9   12   29    0    1    0    5    3    4]
 [  89    3    1    0    9    1    2    0   85  766    0    4    1   15    9    4    1    3    0    2    6]
 [   2    6   10   10    3    1    3    8   22    3  936    1    0   16   15    0    6    0   12    5    5]
 [   3    4    2    0    3   12    3    8    1    3    1  843   44    4    0   19    6   24    1   26    4]
 [   0    2    3    6    0    6    0    3    3    0    1   79  816    2    3    9    7   34    5    6   10]
 [   2    4    5    0    4   15    2    3   26   21    8    9    5  843    6    8    5    5    0   15   15]
 [  15    3    3   12   11    1    1    0   47   12   11    1    6    7  932    1    3    6   13    0   13]
 [   2    2    4    2    8    1    3    2    0    3    0   16   12    2    1  971    8   13    2    6    8]
 [   7   10    9    2   12    6    4    1    7    0    3    7    4    8    1   10  951    2    1    9   18]
 [   7    1    0    1    1    2    1    1    3    1    1   21   35    6    4    9    3  898    2    4    4]
 [   1   10   11   19    5    0    0   28   12    1   14    0    5    3   18    0    4    4  911    3    9]
 [   0    8    1    1    1    5   17   13    0    0    1   27    7    4    0    7   10    8    2  970    6]
 [ 291  334  358  161  343  151  132  196  225  155  239  224  417  327  299  224  412  192  224  401 8627]]

2024-06-06 02:02:10,453 - ==> Best [Top1: 76.485   Top5: 96.012   Sparsity:0.00   Params: 169472 on epoch: 123]
2024-06-06 02:02:10,453 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:02:10,460 - 

2024-06-06 02:02:10,460 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:02:16,669 - Epoch: [126][  100/ 1218]    Overall Loss 0.547793    Objective Loss 0.547793                                        LR 0.000500    Time 0.062067    
2024-06-06 02:02:21,195 - Epoch: [126][  200/ 1218]    Overall Loss 0.553259    Objective Loss 0.553259                                        LR 0.000500    Time 0.053655    
2024-06-06 02:02:25,859 - Epoch: [126][  300/ 1218]    Overall Loss 0.553929    Objective Loss 0.553929                                        LR 0.000500    Time 0.051310    
2024-06-06 02:02:30,399 - Epoch: [126][  400/ 1218]    Overall Loss 0.552082    Objective Loss 0.552082                                        LR 0.000500    Time 0.049830    
2024-06-06 02:02:35,016 - Epoch: [126][  500/ 1218]    Overall Loss 0.552494    Objective Loss 0.552494                                        LR 0.000500    Time 0.049094    
2024-06-06 02:02:39,779 - Epoch: [126][  600/ 1218]    Overall Loss 0.554565    Objective Loss 0.554565                                        LR 0.000500    Time 0.048847    
2024-06-06 02:02:44,353 - Epoch: [126][  700/ 1218]    Overall Loss 0.552259    Objective Loss 0.552259                                        LR 0.000500    Time 0.048401    
2024-06-06 02:02:49,019 - Epoch: [126][  800/ 1218]    Overall Loss 0.550788    Objective Loss 0.550788                                        LR 0.000500    Time 0.048181    
2024-06-06 02:02:53,847 - Epoch: [126][  900/ 1218]    Overall Loss 0.551356    Objective Loss 0.551356                                        LR 0.000500    Time 0.048189    
2024-06-06 02:02:58,370 - Epoch: [126][ 1000/ 1218]    Overall Loss 0.550142    Objective Loss 0.550142                                        LR 0.000500    Time 0.047892    
2024-06-06 02:03:03,047 - Epoch: [126][ 1100/ 1218]    Overall Loss 0.550949    Objective Loss 0.550949                                        LR 0.000500    Time 0.047788    
2024-06-06 02:03:07,615 - Epoch: [126][ 1200/ 1218]    Overall Loss 0.551157    Objective Loss 0.551157                                        LR 0.000500    Time 0.047612    
2024-06-06 02:03:08,390 - Epoch: [126][ 1218/ 1218]    Overall Loss 0.551206    Objective Loss 0.551206    Top1 74.572127    Top5 97.066015    LR 0.000500    Time 0.047544    
2024-06-06 02:03:08,587 - --- validate (epoch=126)-----------
2024-06-06 02:03:08,587 - 34633 samples (256 per mini-batch)
2024-06-06 02:03:14,057 - Epoch: [126][  100/  136]    Loss 0.518985    Top1 75.218750    Top5 95.453125    
2024-06-06 02:03:15,746 - Epoch: [126][  136/  136]    Loss 0.522026    Top1 75.263477    Top5 95.498513    
2024-06-06 02:03:15,932 - ==> Top1: 75.263    Top5: 95.499    Loss: 0.522

2024-06-06 02:03:15,934 - ==> Confusion:
[[ 791    0    4    0   12    0    1    6    6   73    3    2    3    4    7    3    4    0    6    1    5]
 [   6  887    8    3   30   29    5   29    8    1    4    7    3    1    4    4    8    2   12    4    8]
 [   6    2  853   11    6    2   15   11    2    6    6    5    5    8    2    4    7    3    3    5    8]
 [   4    2   30  853    7    4    2    4    1    2   17    3    6    5   37    2    4    9   22    0    2]
 [  30    8    2    1  944    5    0    2    3   12    1    3    1    9    8    4    8    1    7    0    5]
 [  11   24    4    4   20  808    2   57    4    4    3   23    5   36    6    4    5    5    5    6    7]
 [   1    2   36    6    7    1  953    7    1    2    5    9    4    2    0   12    4    5    6    6   17]
 [  11   11   17    0    4   31    5  893    3    2    8   14    4    6    1    3    3    5   34   15    7]
 [  15    7    3    0    4    3    0    5  815   66   11    4    7   15   30    1    2    0   10    0    4]
 [  76    0    1    3    8    3    1    2   53  795    3    3    1   17    9    4    2    6    1    0   13]
 [   2    5   13   14    2    3    5    6   17    4  931    2    5   12   16    1    3    1   15    2    5]
 [   4    3    2    1    2    9    3    8    1    0    0  836   41   14    3   17   10   33    4   10   10]
 [   5    1    4    9    3    6    1    3    2    0    1   71  798    5    5    9    4   46    8    5    9]
 [   5    2    2    2    7    7    1    4   26   21    9   10    6  863    5    5    4    9    1    6    6]
 [  10    5    4   17   13    5    0    3   37   12    7    3    3    5  944    2    3    6   10    1    8]
 [   3    0    7    0    3    2    5    1    0    2    0   17    9    7    0  974   14   11    2    1    8]
 [   4   13    2    1   13    4    1    1    6    2    4   11    5    3    0   19  960    2    2    5   14]
 [   0    0    4    3    1    3    1    1    0    3    3   24   36    4    4   12    0  891    5    3    7]
 [   4    9   12   11    7    3    2   22    7    1    7    3    4    1   22    0    1    0  927    5   10]
 [   3   10    5    3    4   10   12   20    3    1    2   30   11    9    0   11    6    4    4  929   11]
 [ 329  257  331  180  397  220   98  266  167  170  225  249  442  426  278  291  445  188  265  287 8421]]

2024-06-06 02:03:15,935 - ==> Best [Top1: 76.485   Top5: 96.012   Sparsity:0.00   Params: 169472 on epoch: 123]
2024-06-06 02:03:15,935 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:03:15,944 - 

2024-06-06 02:03:15,944 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:03:22,335 - Epoch: [127][  100/ 1218]    Overall Loss 0.545984    Objective Loss 0.545984                                        LR 0.000500    Time 0.063889    
2024-06-06 02:03:27,094 - Epoch: [127][  200/ 1218]    Overall Loss 0.547226    Objective Loss 0.547226                                        LR 0.000500    Time 0.055731    
2024-06-06 02:03:31,640 - Epoch: [127][  300/ 1218]    Overall Loss 0.547553    Objective Loss 0.547553                                        LR 0.000500    Time 0.052302    
2024-06-06 02:03:36,216 - Epoch: [127][  400/ 1218]    Overall Loss 0.549085    Objective Loss 0.549085                                        LR 0.000500    Time 0.050662    
2024-06-06 02:03:41,083 - Epoch: [127][  500/ 1218]    Overall Loss 0.551345    Objective Loss 0.551345                                        LR 0.000500    Time 0.050260    
2024-06-06 02:03:45,759 - Epoch: [127][  600/ 1218]    Overall Loss 0.551342    Objective Loss 0.551342                                        LR 0.000500    Time 0.049674    
2024-06-06 02:03:50,327 - Epoch: [127][  700/ 1218]    Overall Loss 0.551014    Objective Loss 0.551014                                        LR 0.000500    Time 0.049100    
2024-06-06 02:03:54,938 - Epoch: [127][  800/ 1218]    Overall Loss 0.551321    Objective Loss 0.551321                                        LR 0.000500    Time 0.048724    
2024-06-06 02:03:59,639 - Epoch: [127][  900/ 1218]    Overall Loss 0.551120    Objective Loss 0.551120                                        LR 0.000500    Time 0.048532    
2024-06-06 02:04:04,342 - Epoch: [127][ 1000/ 1218]    Overall Loss 0.550510    Objective Loss 0.550510                                        LR 0.000500    Time 0.048381    
2024-06-06 02:04:09,107 - Epoch: [127][ 1100/ 1218]    Overall Loss 0.549544    Objective Loss 0.549544                                        LR 0.000500    Time 0.048312    
2024-06-06 02:04:13,875 - Epoch: [127][ 1200/ 1218]    Overall Loss 0.549989    Objective Loss 0.549989                                        LR 0.000500    Time 0.048258    
2024-06-06 02:04:14,726 - Epoch: [127][ 1218/ 1218]    Overall Loss 0.549991    Objective Loss 0.549991    Top1 73.838631    Top5 93.887531    LR 0.000500    Time 0.048243    
2024-06-06 02:04:14,905 - --- validate (epoch=127)-----------
2024-06-06 02:04:14,905 - 34633 samples (256 per mini-batch)
2024-06-06 02:04:20,688 - Epoch: [127][  100/  136]    Loss 0.519063    Top1 77.250000    Top5 95.851562    
2024-06-06 02:04:22,518 - Epoch: [127][  136/  136]    Loss 0.517355    Top1 77.131637    Top5 95.896977    
2024-06-06 02:04:22,703 - ==> Top1: 77.132    Top5: 95.897    Loss: 0.517

2024-06-06 02:04:22,704 - ==> Confusion:
[[ 798    1    5    2    9    2    0    5    6   72    3    2    0    3    5    1    3    3    3    0    8]
 [   3  908    0    2   20   34    5   28    5    6    2    3    1    2    7    0    4    2   24    1    6]
 [  12    0  826   25    2    3   33   11    2    7    8    3    5    5    2    5    4    0    4    5    8]
 [   6    4   11  872    3    8    7    3    4    1   13    1    7    4   28    1    3    9   21    1    9]
 [  52   11    5    1  897   16    1    5    0   18    4    2    1    3   11    2    5    0    9    0   11]
 [   6   24    4    4   15  854    4   45    0    7    3   11    7   16    3    2    5    5    7    7   14]
 [   1    5   20    4    2    9  973   11    2    2    3    6    2    1    1    8    3    4    3   11   15]
 [   5   11   17    2    3   48    6  896    2    4    3    6    4    2    3    0    4    3   27   14   17]
 [  16    4    2    3    0    5    1    1  813   85   12    4    5   16   14    1    1    2   11    1    5]
 [  90    0    0    0    3    4    0    4   45  819    1    1    0   14    7    1    1    1    3    1    6]
 [   1    3   12   19    3    6    2    4   23    1  916    2    1   18   15    1    2    0   18    0   17]
 [   3    1    1    0    3   26    7   11    2    1    0  833   26   17    1   19    5   23    3   20    9]
 [   0    2    3   10    0    7    6    5    9    0    4   66  772    5    3   15    5   50   13   10   10]
 [   6    0    4    1    7   27    2    5   23   37   12    5    3  837    6    5    4    1    4    5    7]
 [  20    4    6   13   11    3    0    1   48   15    6    2    3    7  930    1    1    3   19    0    5]
 [   5    0   10    2    7    1    9    1    0    2    0   24   10    3    0  939   16   15    2    5   15]
 [   7    4    7    3    5    9    2    2    8    2    1    5    1    8    1   10  961    4    3    7   22]
 [   3    0    1    3    2    3    2    5    3    2    0   19   27    4    6   14    1  891    4    3   12]
 [   5    6   10   16    3    4    3   16    9    2    9    4    7    1   17    0    2    1  928    5   10]
 [   2    4    4    1    4   26   23   18    2    2    0   24    9    5    0    9    8    4    4  926   13]
 [ 349  250  265  163  281  308  147  271  141  183  186  189  338  333  272  139  293  146  261  293 9124]]

2024-06-06 02:04:22,705 - ==> Best [Top1: 77.132   Top5: 95.897   Sparsity:0.00   Params: 169472 on epoch: 127]
2024-06-06 02:04:22,706 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:04:22,721 - 

2024-06-06 02:04:22,721 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:04:28,982 - Epoch: [128][  100/ 1218]    Overall Loss 0.545177    Objective Loss 0.545177                                        LR 0.000500    Time 0.062581    
2024-06-06 02:04:33,696 - Epoch: [128][  200/ 1218]    Overall Loss 0.546016    Objective Loss 0.546016                                        LR 0.000500    Time 0.054852    
2024-06-06 02:04:38,427 - Epoch: [128][  300/ 1218]    Overall Loss 0.549546    Objective Loss 0.549546                                        LR 0.000500    Time 0.052333    
2024-06-06 02:04:43,202 - Epoch: [128][  400/ 1218]    Overall Loss 0.550013    Objective Loss 0.550013                                        LR 0.000500    Time 0.051184    
2024-06-06 02:04:47,813 - Epoch: [128][  500/ 1218]    Overall Loss 0.548180    Objective Loss 0.548180                                        LR 0.000500    Time 0.050164    
2024-06-06 02:04:52,739 - Epoch: [128][  600/ 1218]    Overall Loss 0.548289    Objective Loss 0.548289                                        LR 0.000500    Time 0.050010    
2024-06-06 02:04:57,518 - Epoch: [128][  700/ 1218]    Overall Loss 0.549819    Objective Loss 0.549819                                        LR 0.000500    Time 0.049691    
2024-06-06 02:05:02,075 - Epoch: [128][  800/ 1218]    Overall Loss 0.552094    Objective Loss 0.552094                                        LR 0.000500    Time 0.049174    
2024-06-06 02:05:06,751 - Epoch: [128][  900/ 1218]    Overall Loss 0.552441    Objective Loss 0.552441                                        LR 0.000500    Time 0.048903    
2024-06-06 02:05:11,343 - Epoch: [128][ 1000/ 1218]    Overall Loss 0.552974    Objective Loss 0.552974                                        LR 0.000500    Time 0.048604    
2024-06-06 02:05:15,921 - Epoch: [128][ 1100/ 1218]    Overall Loss 0.553668    Objective Loss 0.553668                                        LR 0.000500    Time 0.048345    
2024-06-06 02:05:20,652 - Epoch: [128][ 1200/ 1218]    Overall Loss 0.553048    Objective Loss 0.553048                                        LR 0.000500    Time 0.048257    
2024-06-06 02:05:21,434 - Epoch: [128][ 1218/ 1218]    Overall Loss 0.552688    Objective Loss 0.552688    Top1 76.772616    Top5 96.577017    LR 0.000500    Time 0.048186    
2024-06-06 02:05:21,620 - --- validate (epoch=128)-----------
2024-06-06 02:05:21,621 - 34633 samples (256 per mini-batch)
2024-06-06 02:05:27,114 - Epoch: [128][  100/  136]    Loss 0.508932    Top1 76.160156    Top5 95.824219    
2024-06-06 02:05:28,794 - Epoch: [128][  136/  136]    Loss 0.512468    Top1 75.959345    Top5 95.798805    
2024-06-06 02:05:28,976 - ==> Top1: 75.959    Top5: 95.799    Loss: 0.512

2024-06-06 02:05:28,977 - ==> Confusion:
[[ 796    1    5    0   21    3    1    2    5   58    3    3    2    5    4    1    3    2    4    0   12]
 [   2  909    8    4   37   22    1   14    8    1    2    8    5    2    4    5    9    1    8    2   11]
 [  11    2  854    9    9    1   16    8    0    8    7    2    4    0    3    9    7    2    7    6    5]
 [   4    2   22  859    6    4    2    0    2    3   20    2    8    1   39    4    3    3   18    3   11]
 [  15    7    2    0  959    5    1    0    1   16    0    2    1    3    9    9   10    2    5    0    7]
 [   7   44    8    7   30  808    3   37    4    5    3   18    6   23    4    0    7    4    3    9   13]
 [   1    9   54    0    3    3  966    3    1    1    5    2    2    3    0    6    4    2    3    4   14]
 [   4   21   18    3    3   29    7  875    4    2   11   13   11    4    2    0    1    1   37   22    9]
 [  11    5    2    1    1    1    0    1  802   65   24    0    6   19   35    1    5    4   10    2    7]
 [  80    2    4    1   11    1    0    1   43  809    4    0    1   19   11    3    1    2    0    0    8]
 [   0    7   18   11    2    1    3    7   14    5  929    0    4   11   18    0    5    0   21    3    5]
 [   1    1    4    1    1   11    5    7    1    3    1  830   43   10    2   15    5   35    1   24   10]
 [   0    1    0    9    0    4    2    4    6    0    4   56  803    2    4   10    4   56   10    8   12]
 [   1    1    5    2   10   15    2    4   19   17   15   16    8  850    8    1    6    3    2    2   14]
 [  12    8    2   22   15    0    0    3   22    8    1    1    1    8  966    0    4    2   15    0    8]
 [   3    2    8    1    7    1    4    0    0    2    1   20   13    3    1  961   11   15    1    3    9]
 [   2    8   12    1   14    8    2    3    4    0    3    7    5    6    6   17  946    2    4    6   16]
 [   1    0    1    4    3    2    2    2    0    2    1   23   26    2    3   21    4  896    2    2    8]
 [   0   10   14   22    7    1    1   23    8    0    3    5    5    1   27    2    4    2  908    4   11]
 [   3    9    6    3    3    9   21   13    1    0    1   24   14    5    0   12    6    3    1  946    8]
 [ 266  323  398  163  414  170  142  191  140  132  218  213  429  344  364  255  341  167  251  376 8635]]

2024-06-06 02:05:28,979 - ==> Best [Top1: 77.132   Top5: 95.897   Sparsity:0.00   Params: 169472 on epoch: 127]
2024-06-06 02:05:28,979 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:05:28,987 - 

2024-06-06 02:05:28,987 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:05:35,663 - Epoch: [129][  100/ 1218]    Overall Loss 0.539138    Objective Loss 0.539138                                        LR 0.000500    Time 0.066740    
2024-06-06 02:05:40,248 - Epoch: [129][  200/ 1218]    Overall Loss 0.546730    Objective Loss 0.546730                                        LR 0.000500    Time 0.056286    
2024-06-06 02:05:44,861 - Epoch: [129][  300/ 1218]    Overall Loss 0.546355    Objective Loss 0.546355                                        LR 0.000500    Time 0.052894    
2024-06-06 02:05:49,469 - Epoch: [129][  400/ 1218]    Overall Loss 0.545051    Objective Loss 0.545051                                        LR 0.000500    Time 0.051187    
2024-06-06 02:05:54,066 - Epoch: [129][  500/ 1218]    Overall Loss 0.546416    Objective Loss 0.546416                                        LR 0.000500    Time 0.050139    
2024-06-06 02:05:58,825 - Epoch: [129][  600/ 1218]    Overall Loss 0.549384    Objective Loss 0.549384                                        LR 0.000500    Time 0.049711    
2024-06-06 02:06:03,755 - Epoch: [129][  700/ 1218]    Overall Loss 0.550428    Objective Loss 0.550428                                        LR 0.000500    Time 0.049650    
2024-06-06 02:06:08,395 - Epoch: [129][  800/ 1218]    Overall Loss 0.551446    Objective Loss 0.551446                                        LR 0.000500    Time 0.049241    
2024-06-06 02:06:13,155 - Epoch: [129][  900/ 1218]    Overall Loss 0.552350    Objective Loss 0.552350                                        LR 0.000500    Time 0.049058    
2024-06-06 02:06:17,761 - Epoch: [129][ 1000/ 1218]    Overall Loss 0.552313    Objective Loss 0.552313                                        LR 0.000500    Time 0.048756    
2024-06-06 02:06:22,597 - Epoch: [129][ 1100/ 1218]    Overall Loss 0.550694    Objective Loss 0.550694                                        LR 0.000500    Time 0.048719    
2024-06-06 02:06:27,258 - Epoch: [129][ 1200/ 1218]    Overall Loss 0.549915    Objective Loss 0.549915                                        LR 0.000500    Time 0.048542    
2024-06-06 02:06:28,064 - Epoch: [129][ 1218/ 1218]    Overall Loss 0.549670    Objective Loss 0.549670    Top1 72.127139    Top5 96.577017    LR 0.000500    Time 0.048486    
2024-06-06 02:06:28,244 - --- validate (epoch=129)-----------
2024-06-06 02:06:28,244 - 34633 samples (256 per mini-batch)
2024-06-06 02:06:33,903 - Epoch: [129][  100/  136]    Loss 0.497853    Top1 76.675781    Top5 96.242188    
2024-06-06 02:06:35,533 - Epoch: [129][  136/  136]    Loss 0.493909    Top1 76.603240    Top5 96.298328    
2024-06-06 02:06:35,726 - ==> Top1: 76.603    Top5: 96.298    Loss: 0.494

2024-06-06 02:06:35,728 - ==> Confusion:
[[ 797    3    3    0   17    4    0    1    6   69    0    2    1    3    7    1    4    2    0    1   10]
 [   0  928    4    2   26   27    2   11    3    3    2    7    2    1    3    1   14    1   14    1   11]
 [   6    5  816   16    9    3   34   21    2    5    8    4    3    4    3    7    4    3    3    4   10]
 [   2    7   18  882    4    5    4    3    3    1    9    2    9    1   24    2    6    5   18    1   10]
 [  26   13    1    1  956    8    0    0    2    8    1    3    0    1    9    3    5    2    1    1   13]
 [   5   29    1    7   22  841    7   28    5    5    2   24    2   16    3    3    1    3    4   19   16]
 [   2    4   19    1    3    1  997    6    1    0    6    7    0    3    0    7    2    1    3   11   12]
 [   7   25   11    1    5   40    6  890    3    1    7   13    7    0    2    1    0    5   28   15   10]
 [  12    4    2    1    3    4    0    5  831   52   11    1    4   15   26    1    5    2    4    1   18]
 [  96    1    2    1   12    4    1    7   49  787    1    1    0   16   14    1    0    2    1    1    4]
 [   1    6    7   14    0    3    4    8   24    1  933    0    2    7   18    1    3    0   14    4   14]
 [   1    1    3    1    2   17    6    6    1    2    0  841   39    7    1   11    7   28    3   27    7]
 [   1    0    2    8    0    5    1    5    2    2    2   77  799    1    2    5    5   50    4    8   16]
 [   5    0    0    0   15   23    2    6   25   20   11   15    4  820    7    3    9    9    4   14    9]
 [  11    6    2    8   17    2    0    1   37   13    2    2    0    5  964    0    1    5   12    2    8]
 [   5    0    2    3    7    1    4    1    1    1    0   17    9    3    0  960   14   21    0    4   13]
 [   5   14    4    0   10    6    4    2    7    1    3   10    2    0    2   13  964    0    1    6   18]
 [   2    1    0    6    1    4    5    0    2    5    0   16   28    1    5   11    0  903    5    4    6]
 [   1   11    7   22    2    3    2   22   10    2    5    7    2    2   22    1    4    1  918    5    9]
 [   4    5    4    0    2   11   21   13    1    1    0   41    5    4    1    6    8    6    5  934   16]
 [ 312  312  183  168  325  258  141  225  167  160  196  284  389  311  354  176  416  166  216  405 8768]]

2024-06-06 02:06:35,729 - ==> Best [Top1: 77.132   Top5: 95.897   Sparsity:0.00   Params: 169472 on epoch: 127]
2024-06-06 02:06:35,729 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:06:35,737 - 

2024-06-06 02:06:35,737 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:06:41,774 - Epoch: [130][  100/ 1218]    Overall Loss 0.538313    Objective Loss 0.538313                                        LR 0.000500    Time 0.060347    
2024-06-06 02:06:46,335 - Epoch: [130][  200/ 1218]    Overall Loss 0.540062    Objective Loss 0.540062                                        LR 0.000500    Time 0.052969    
2024-06-06 02:06:50,906 - Epoch: [130][  300/ 1218]    Overall Loss 0.539944    Objective Loss 0.539944                                        LR 0.000500    Time 0.050545    
2024-06-06 02:06:55,473 - Epoch: [130][  400/ 1218]    Overall Loss 0.539037    Objective Loss 0.539037                                        LR 0.000500    Time 0.049321    
2024-06-06 02:07:00,122 - Epoch: [130][  500/ 1218]    Overall Loss 0.542252    Objective Loss 0.542252                                        LR 0.000500    Time 0.048752    
2024-06-06 02:07:04,840 - Epoch: [130][  600/ 1218]    Overall Loss 0.545666    Objective Loss 0.545666                                        LR 0.000500    Time 0.048487    
2024-06-06 02:07:09,575 - Epoch: [130][  700/ 1218]    Overall Loss 0.546468    Objective Loss 0.546468                                        LR 0.000500    Time 0.048321    
2024-06-06 02:07:14,338 - Epoch: [130][  800/ 1218]    Overall Loss 0.546755    Objective Loss 0.546755                                        LR 0.000500    Time 0.048232    
2024-06-06 02:07:18,992 - Epoch: [130][  900/ 1218]    Overall Loss 0.547014    Objective Loss 0.547014                                        LR 0.000500    Time 0.048042    
2024-06-06 02:07:23,705 - Epoch: [130][ 1000/ 1218]    Overall Loss 0.546990    Objective Loss 0.546990                                        LR 0.000500    Time 0.047950    
2024-06-06 02:07:28,525 - Epoch: [130][ 1100/ 1218]    Overall Loss 0.548173    Objective Loss 0.548173                                        LR 0.000500    Time 0.047970    
2024-06-06 02:07:33,138 - Epoch: [130][ 1200/ 1218]    Overall Loss 0.549688    Objective Loss 0.549688                                        LR 0.000500    Time 0.047816    
2024-06-06 02:07:33,903 - Epoch: [130][ 1218/ 1218]    Overall Loss 0.549737    Objective Loss 0.549737    Top1 74.816626    Top5 97.066015    LR 0.000500    Time 0.047738    
2024-06-06 02:07:34,086 - --- validate (epoch=130)-----------
2024-06-06 02:07:34,086 - 34633 samples (256 per mini-batch)
2024-06-06 02:07:39,824 - Epoch: [130][  100/  136]    Loss 0.503553    Top1 76.421875    Top5 96.214844    
2024-06-06 02:07:41,561 - Epoch: [130][  136/  136]    Loss 0.499693    Top1 76.227875    Top5 96.156845    
2024-06-06 02:07:41,760 - ==> Top1: 76.228    Top5: 96.157    Loss: 0.500

2024-06-06 02:07:41,761 - ==> Confusion:
[[ 775    0    7    1   14    8    0    6   12   67    0    2    4    3    6    0    3    4    5    0   14]
 [   1  924    3    0   17   37    3   23    5    1    4    4    1    2    5    0   13    2    9    5    4]
 [   5    2  859    7    3    4   22   14    2    4    6    4    2    8    4    2    8    1    3    1    9]
 [   2    1   31  853    5    9    3    9    4    1   16    2   11    3   22    0    3    5   20    2   14]
 [  23   10    4    2  918   15    2    9    7   13    0    2    1    4    9    6   10    0    4    4   11]
 [   4   24    6    2   13  869    5   45    1    3    0   16    6   14    1    2    6    0    3   10   13]
 [   0    7   42    2    1    7  967    8    0    1    5    4    1    3    0    4    4    3    3   18    6]
 [   0   11   20    3    2   38    6  921    2    2    4   12    3    3    0    0    2    0   17   25    6]
 [  11    4    2    3    1    6    1    5  811   53   18    1    9   26   23    0    3    3   13    2    7]
 [  69    2    4    1   14    7    1    1   62  793    1    1    0   28    0    2    3    3    0    0    9]
 [   3    8   11   12    2    8    3   16   18    4  912    0    1   16    9    0    1    0   25    4   11]
 [   4    2    3    0    1   21    6    8    1    0    0  851   42    4    0    9    6   21    1   20   11]
 [   1    1    6    6    0    6    1    4    2    0    3   73  817    3    3    2    3   27    6   14   17]
 [   2    2    0    0    6   28    1    6   15   13    7   13   10  860    3    2    3    4    3   14    9]
 [  12    5    1   14   17    3    0    2   29   10    6    2    5    7  940    0    3    4   23    4   11]
 [   3    4    4    0    7    2   11    0    1    1    0   20   16    1    0  944   18   15    4    5   10]
 [   2    8    7    2    9   12    1    2    9    1    3    9    4    1    1    9  966    0    2    7   17]
 [   2    1    1    2    2    8    2    2    0    1    0   22   39    4    5   13    1  886    2    2   10]
 [   2   11   11   10    4    1    2   34   10    0    5    1    3    2   19    0    1    0  927    7    8]
 [   1    5    2    2    1   16   21   17    2    1    0   20   14    4    0   11   13    2    2  939   15]
 [ 271  274  363  140  285  328  147  302  145  142  186  228  460  350  250  156  424  153  240  420 8668]]

2024-06-06 02:07:41,763 - ==> Best [Top1: 77.132   Top5: 95.897   Sparsity:0.00   Params: 169472 on epoch: 127]
2024-06-06 02:07:41,763 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:07:41,770 - 

2024-06-06 02:07:41,771 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:07:48,250 - Epoch: [131][  100/ 1218]    Overall Loss 0.549841    Objective Loss 0.549841                                        LR 0.000500    Time 0.064770    
2024-06-06 02:07:52,914 - Epoch: [131][  200/ 1218]    Overall Loss 0.548856    Objective Loss 0.548856                                        LR 0.000500    Time 0.055701    
2024-06-06 02:07:57,939 - Epoch: [131][  300/ 1218]    Overall Loss 0.545680    Objective Loss 0.545680                                        LR 0.000500    Time 0.053875    
2024-06-06 02:08:02,536 - Epoch: [131][  400/ 1218]    Overall Loss 0.550796    Objective Loss 0.550796                                        LR 0.000500    Time 0.051896    
2024-06-06 02:08:07,276 - Epoch: [131][  500/ 1218]    Overall Loss 0.550266    Objective Loss 0.550266                                        LR 0.000500    Time 0.050993    
2024-06-06 02:08:11,865 - Epoch: [131][  600/ 1218]    Overall Loss 0.550158    Objective Loss 0.550158                                        LR 0.000500    Time 0.050139    
2024-06-06 02:08:16,432 - Epoch: [131][  700/ 1218]    Overall Loss 0.549217    Objective Loss 0.549217                                        LR 0.000500    Time 0.049499    
2024-06-06 02:08:21,020 - Epoch: [131][  800/ 1218]    Overall Loss 0.549857    Objective Loss 0.549857                                        LR 0.000500    Time 0.049044    
2024-06-06 02:08:25,683 - Epoch: [131][  900/ 1218]    Overall Loss 0.549321    Objective Loss 0.549321                                        LR 0.000500    Time 0.048774    
2024-06-06 02:08:30,491 - Epoch: [131][ 1000/ 1218]    Overall Loss 0.550641    Objective Loss 0.550641                                        LR 0.000500    Time 0.048703    
2024-06-06 02:08:35,071 - Epoch: [131][ 1100/ 1218]    Overall Loss 0.550710    Objective Loss 0.550710                                        LR 0.000500    Time 0.048437    
2024-06-06 02:08:39,699 - Epoch: [131][ 1200/ 1218]    Overall Loss 0.550267    Objective Loss 0.550267                                        LR 0.000500    Time 0.048256    
2024-06-06 02:08:40,482 - Epoch: [131][ 1218/ 1218]    Overall Loss 0.550150    Objective Loss 0.550150    Top1 74.083130    Top5 95.110024    LR 0.000500    Time 0.048186    
2024-06-06 02:08:40,668 - --- validate (epoch=131)-----------
2024-06-06 02:08:40,668 - 34633 samples (256 per mini-batch)
2024-06-06 02:08:46,156 - Epoch: [131][  100/  136]    Loss 0.529813    Top1 75.070312    Top5 95.324219    
2024-06-06 02:08:47,946 - Epoch: [131][  136/  136]    Loss 0.522657    Top1 75.272139    Top5 95.391678    
2024-06-06 02:08:48,132 - ==> Top1: 75.272    Top5: 95.392    Loss: 0.523

2024-06-06 02:08:48,133 - ==> Confusion:
[[ 761    1    8    0   11    3    0    0   11   97    2    3    2    3    8    2    4    4    4    1    6]
 [   2  922    1    2   32   29    6    7    4    4    3    4    2    3    4    0    6    4   11    5   12]
 [   9    1  831   20    9    5   20    3    5    4    9    8    2    8    6    9    7    3    4    3    4]
 [   5    0   25  862    7    9    0    1    4    2   22    3    9    1   38    1    2    3    9    0   13]
 [  19    7    6    3  939   13    0    2    1   11    2    2    2    1   19    3    9    0    5    1    9]
 [   3   26    6    4   21  843    2   26    2    4    2   17    9   25   16    3    7    1    6   10   10]
 [   1    4   34    3    7    6  968    7    1    1    2    8    2    2    0   11    2    4    3   11    9]
 [   7   23   22    2    7   52    7  848    5    2   13   10    5    4    1    2    2    3   32   23    7]
 [  12    6    3    3    2    5    0    1  831   50   18    4    4   17   20    0    2    3   10    0   11]
 [  70    1    4    1   11    4    1    0   63  797    4    3    1   25    7    1    2    1    0    2    3]
 [   1    6   11   21    3    4    5    1   17    4  945    4    1   10   16    0    0    0   10    0    5]
 [   1    5    2    0    2   18    4    3    2    3    0  853   36   13    0   12    7   16    2   27    5]
 [   2    1    5    4    3    7    2    4    5    0    4   87  789    5    3    9    3   36    4   10   12]
 [   3    3    9    1    9   15    1    3   31   20   14   11    9  832    9    4    3    5    1    9    9]
 [   8    9    3   14   10    2    0    0   32    7    6    0    6    4  969    1    5    2   14    0    6]
 [   2    2    8    2    8    2   11    0    0    2    0   21   14    2    1  952    9   15    2    5    8]
 [   3   12   12    5   17    8    3    2    8    0    0    8    6    5    3    9  943    3    4    6   15]
 [   0    5    3    5    0    3    3    1    3    1    0   34   36    6    7   22    0  866    1    3    6]
 [   3    4    9   27    5    5    0   16   12    1   13    7    2    2   39    1    1    1  892    4   14]
 [   1    9   13    2    2   12   23    7    1    0    3   26    7    7    1    8    4    1    2  951    8]
 [ 226  289  411  185  429  275  157  175  150  180  247  246  443  368  374  230  354  136  210  372 8475]]

2024-06-06 02:08:48,135 - ==> Best [Top1: 77.132   Top5: 95.897   Sparsity:0.00   Params: 169472 on epoch: 127]
2024-06-06 02:08:48,135 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:08:48,143 - 

2024-06-06 02:08:48,143 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:08:54,349 - Epoch: [132][  100/ 1218]    Overall Loss 0.552711    Objective Loss 0.552711                                        LR 0.000500    Time 0.062036    
2024-06-06 02:08:59,039 - Epoch: [132][  200/ 1218]    Overall Loss 0.546841    Objective Loss 0.546841                                        LR 0.000500    Time 0.054459    
2024-06-06 02:09:04,016 - Epoch: [132][  300/ 1218]    Overall Loss 0.550950    Objective Loss 0.550950                                        LR 0.000500    Time 0.052893    
2024-06-06 02:09:09,155 - Epoch: [132][  400/ 1218]    Overall Loss 0.552121    Objective Loss 0.552121                                        LR 0.000500    Time 0.052511    
2024-06-06 02:09:13,981 - Epoch: [132][  500/ 1218]    Overall Loss 0.550944    Objective Loss 0.550944                                        LR 0.000500    Time 0.051658    
2024-06-06 02:09:18,754 - Epoch: [132][  600/ 1218]    Overall Loss 0.552529    Objective Loss 0.552529                                        LR 0.000500    Time 0.051000    
2024-06-06 02:09:23,722 - Epoch: [132][  700/ 1218]    Overall Loss 0.549879    Objective Loss 0.549879                                        LR 0.000500    Time 0.050808    
2024-06-06 02:09:28,500 - Epoch: [132][  800/ 1218]    Overall Loss 0.549553    Objective Loss 0.549553                                        LR 0.000500    Time 0.050428    
2024-06-06 02:09:33,264 - Epoch: [132][  900/ 1218]    Overall Loss 0.549662    Objective Loss 0.549662                                        LR 0.000500    Time 0.050116    
2024-06-06 02:09:37,889 - Epoch: [132][ 1000/ 1218]    Overall Loss 0.550059    Objective Loss 0.550059                                        LR 0.000500    Time 0.049728    
2024-06-06 02:09:42,708 - Epoch: [132][ 1100/ 1218]    Overall Loss 0.549841    Objective Loss 0.549841                                        LR 0.000500    Time 0.049586    
2024-06-06 02:09:47,567 - Epoch: [132][ 1200/ 1218]    Overall Loss 0.549287    Objective Loss 0.549287                                        LR 0.000500    Time 0.049502    
2024-06-06 02:09:48,343 - Epoch: [132][ 1218/ 1218]    Overall Loss 0.549570    Objective Loss 0.549570    Top1 78.239609    Top5 95.599022    LR 0.000500    Time 0.049407    
2024-06-06 02:09:48,531 - --- validate (epoch=132)-----------
2024-06-06 02:09:48,531 - 34633 samples (256 per mini-batch)
2024-06-06 02:09:54,148 - Epoch: [132][  100/  136]    Loss 0.510572    Top1 75.937500    Top5 95.808594    
2024-06-06 02:09:55,833 - Epoch: [132][  136/  136]    Loss 0.506589    Top1 76.106604    Top5 95.813242    
2024-06-06 02:09:56,005 - ==> Top1: 76.107    Top5: 95.813    Loss: 0.507

2024-06-06 02:09:56,007 - ==> Confusion:
[[ 805    1    3    2    9    0    1    0    9   70    0    4    3    4    5    1    3    2    4    1    4]
 [   1  902    0    3   30   39    5   17    4    3    4    1    3    4    7    1   12    1   12    5    9]
 [  15    2  826   12    6    3   17   17    5    7    3    2    4    6    5    5    5    4    8    5   13]
 [   5    3   19  867    3   10    3    2    3    3   18    0   11    2   38    1    4    4   14    0    6]
 [  37   12    2    2  917   14    1    3    3   22    0    3    0    4   13    7    3    1    3    1    6]
 [   7   25    3    7   16  834    2   44    6    4    2   19    4   29    3    1    5    3    1   10   18]
 [   3    9   34    2    3    8  964   10    4    0    2    3    1    2    0    9    1    3    5    7   16]
 [   9   13   10    2    2   53    5  888    0    3    4   14    6    6    3    1    0    2   35   10   11]
 [  19    6    0    0    0    3    0    3  829   60    6    2    7   21   24    1    1    3    6    1   10]
 [  90    1    0    0    5    3    1    0   60  784    2    1    3   26   14    0    0    3    4    0    4]
 [   1    9   11   10    3    5    6    8   23    4  920    2    2    9   15    1    1    1   21    6    6]
 [   1    4    0    0    1    8    4    5    0    1    0  892   29   16    1    5    6   13    4   11   10]
 [   3    3    2    5    0    5    1    6    5    1    1   94  801    3    3    8    3   21   11    8   11]
 [   2    0    2    0    2   16    2    5   22   23    6   13    8  868    6    0    2    3    3    6   12]
 [  17    1    3   19   11    2    0    1   36   13    2    1    7    7  960    1    0    2    7    1    7]
 [   2    4    1    2    6    1    5    0    0    0    1   22   15    5    0  954   12   20    3    5    8]
 [   6   11    1    3   10   11    3    0    6    3    3   13    4    6    1    8  955    3    3    8   14]
 [   1    1    2    5    1    0    5    2    5    5    0   35   38    7    3   13    3  864    4    6    5]
 [   3    3    8   27    4    7    1   37   11    0    7    0    7    3   39    1    0    2  887    0   11]
 [   3    8    2    1    4   10   20   15    1    1    2   28   10    8    1   10    6    2    7  935   14]
 [ 365  250  257  174  330  275  110  257  193  178  188  244  406  462  350  204  319  126  202  336 8706]]

2024-06-06 02:09:56,008 - ==> Best [Top1: 77.132   Top5: 95.897   Sparsity:0.00   Params: 169472 on epoch: 127]
2024-06-06 02:09:56,008 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:09:56,016 - 

2024-06-06 02:09:56,016 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:10:02,289 - Epoch: [133][  100/ 1218]    Overall Loss 0.553179    Objective Loss 0.553179                                        LR 0.000500    Time 0.062708    
2024-06-06 02:10:06,878 - Epoch: [133][  200/ 1218]    Overall Loss 0.549866    Objective Loss 0.549866                                        LR 0.000500    Time 0.054293    
2024-06-06 02:10:11,580 - Epoch: [133][  300/ 1218]    Overall Loss 0.554150    Objective Loss 0.554150                                        LR 0.000500    Time 0.051860    
2024-06-06 02:10:16,149 - Epoch: [133][  400/ 1218]    Overall Loss 0.550731    Objective Loss 0.550731                                        LR 0.000500    Time 0.050314    
2024-06-06 02:10:20,714 - Epoch: [133][  500/ 1218]    Overall Loss 0.546981    Objective Loss 0.546981                                        LR 0.000500    Time 0.049376    
2024-06-06 02:10:25,487 - Epoch: [133][  600/ 1218]    Overall Loss 0.545295    Objective Loss 0.545295                                        LR 0.000500    Time 0.049100    
2024-06-06 02:10:30,323 - Epoch: [133][  700/ 1218]    Overall Loss 0.545557    Objective Loss 0.545557                                        LR 0.000500    Time 0.048991    
2024-06-06 02:10:35,052 - Epoch: [133][  800/ 1218]    Overall Loss 0.547243    Objective Loss 0.547243                                        LR 0.000500    Time 0.048777    
2024-06-06 02:10:39,708 - Epoch: [133][  900/ 1218]    Overall Loss 0.547254    Objective Loss 0.547254                                        LR 0.000500    Time 0.048528    
2024-06-06 02:10:44,394 - Epoch: [133][ 1000/ 1218]    Overall Loss 0.547154    Objective Loss 0.547154                                        LR 0.000500    Time 0.048359    
2024-06-06 02:10:49,211 - Epoch: [133][ 1100/ 1218]    Overall Loss 0.548910    Objective Loss 0.548910                                        LR 0.000500    Time 0.048340    
2024-06-06 02:10:53,951 - Epoch: [133][ 1200/ 1218]    Overall Loss 0.556413    Objective Loss 0.556413                                        LR 0.000500    Time 0.048261    
2024-06-06 02:10:54,717 - Epoch: [133][ 1218/ 1218]    Overall Loss 0.557921    Objective Loss 0.557921    Top1 73.105134    Top5 95.843521    LR 0.000500    Time 0.048176    
2024-06-06 02:10:54,899 - --- validate (epoch=133)-----------
2024-06-06 02:10:54,899 - 34633 samples (256 per mini-batch)
2024-06-06 02:11:00,554 - Epoch: [133][  100/  136]    Loss 0.612087    Top1 73.753906    Top5 95.386719    
2024-06-06 02:11:02,271 - Epoch: [133][  136/  136]    Loss 0.615454    Top1 73.753357    Top5 95.284844    
2024-06-06 02:11:02,444 - ==> Top1: 73.753    Top5: 95.285    Loss: 0.615

2024-06-06 02:11:02,445 - ==> Confusion:
[[ 781    1    1    1   15    2    0    2   11   87    1    2    2    3   12    1    1    3    2    1    2]
 [   5  891    2    0   29   31    7   24    7    2    7    5    4    2   10    1   12    3    6    5   10]
 [  10    1  829   13    5    2   29    7    0    8    6    8    4    7    3    9    8    2    8    3    8]
 [   5    2   14  850    3    7    7    6    3    3   15    5   10    4   42    7    4    8   16    0    5]
 [  32   11    2    1  921    7    3    2    7   24    0    2    3    4    8    8    5    0    6    1    7]
 [   7   24    5    4   22  842    3   39    5    8    3   16    6   19    6    3    5    1    3   13    9]
 [   2    2   17    3    4    7  981    5    2    3    6    4    7    1    1   13    3    2    2   17    4]
 [   4   10    9    2    2   42    7  891    5    2    5    9    8    3    2    0    5    2   40   22    7]
 [   8    3    0    0    1    1    1    1  833   74   14    3    4   13   21    2    6    3    4    3    7]
 [  90    2    1    1    2    2    0    3   49  815    0    0    2   14    9    1    1    3    2    2    2]
 [   2    9   10   14    3    4    4    5   28    1  921    0    3   21   10    0    3    2   17    2    5]
 [   2    0    2    2    2   11    5    5    0    2    0  866   31   10    1   17    6   21    2   21    5]
 [   2    1    1    3    1    8    2    5    4    0    2   66  835    2    5    5    6   37    1    7    2]
 [   5    0    1    0    8   13    0    3   26   28    5   12    4  858    7    1    3    3    2   12   10]
 [  11    3    2   11   14    3    1    0   49   15    5    2    6   11  937    0    0    2   16    3    7]
 [   2    2    3    2    8    5   12    1    1    1    0   18   19    5    1  939   11   18    1    7   10]
 [   9   10    6    0    7   10    1    0    9    1    3    8    8    8    3   14  952    4    3    6   10]
 [   5    2    1    2    1    3    0    0    2    5    0   26   38    3    7   17    3  874    0    6   10]
 [   7   10    8   15    5    7    2   27   11    0    7    6    2    3   27    0    1    1  912    4    3]
 [   1    4    2    4    2   14   14   15    2    0    1   24   11    3    2    7    8    7    4  953   10]
 [ 378  228  313  170  364  262  151  224  265  231  253  235  555  428  375  290  441  156  275  476 7862]]

2024-06-06 02:11:02,446 - ==> Best [Top1: 77.132   Top5: 95.897   Sparsity:0.00   Params: 169472 on epoch: 127]
2024-06-06 02:11:02,446 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:11:02,454 - 

2024-06-06 02:11:02,454 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:11:08,610 - Epoch: [134][  100/ 1218]    Overall Loss 0.642518    Objective Loss 0.642518                                        LR 0.000500    Time 0.061537    
2024-06-06 02:11:13,255 - Epoch: [134][  200/ 1218]    Overall Loss 0.634186    Objective Loss 0.634186                                        LR 0.000500    Time 0.053971    
2024-06-06 02:11:18,026 - Epoch: [134][  300/ 1218]    Overall Loss 0.626074    Objective Loss 0.626074                                        LR 0.000500    Time 0.051879    
2024-06-06 02:11:22,670 - Epoch: [134][  400/ 1218]    Overall Loss 0.620284    Objective Loss 0.620284                                        LR 0.000500    Time 0.050514    
2024-06-06 02:11:27,463 - Epoch: [134][  500/ 1218]    Overall Loss 0.615908    Objective Loss 0.615908                                        LR 0.000500    Time 0.049994    
2024-06-06 02:11:32,254 - Epoch: [134][  600/ 1218]    Overall Loss 0.612883    Objective Loss 0.612883                                        LR 0.000500    Time 0.049644    
2024-06-06 02:11:37,099 - Epoch: [134][  700/ 1218]    Overall Loss 0.611916    Objective Loss 0.611916                                        LR 0.000500    Time 0.049471    
2024-06-06 02:11:41,748 - Epoch: [134][  800/ 1218]    Overall Loss 0.609928    Objective Loss 0.609928                                        LR 0.000500    Time 0.049096    
2024-06-06 02:11:46,518 - Epoch: [134][  900/ 1218]    Overall Loss 0.606939    Objective Loss 0.606939                                        LR 0.000500    Time 0.048938    
2024-06-06 02:11:51,299 - Epoch: [134][ 1000/ 1218]    Overall Loss 0.604629    Objective Loss 0.604629                                        LR 0.000500    Time 0.048824    
2024-06-06 02:11:55,955 - Epoch: [134][ 1100/ 1218]    Overall Loss 0.603017    Objective Loss 0.603017                                        LR 0.000500    Time 0.048617    
2024-06-06 02:12:00,647 - Epoch: [134][ 1200/ 1218]    Overall Loss 0.601011    Objective Loss 0.601011                                        LR 0.000500    Time 0.048474    
2024-06-06 02:12:01,454 - Epoch: [134][ 1218/ 1218]    Overall Loss 0.600805    Objective Loss 0.600805    Top1 71.638142    Top5 95.354523    LR 0.000500    Time 0.048420    
2024-06-06 02:12:01,628 - --- validate (epoch=134)-----------
2024-06-06 02:12:01,628 - 34633 samples (256 per mini-batch)
2024-06-06 02:12:07,243 - Epoch: [134][  100/  136]    Loss 0.536052    Top1 74.648438    Top5 95.343750    
2024-06-06 02:12:08,968 - Epoch: [134][  136/  136]    Loss 0.528101    Top1 74.862126    Top5 95.443652    
2024-06-06 02:12:09,162 - ==> Top1: 74.862    Top5: 95.444    Loss: 0.528

2024-06-06 02:12:09,163 - ==> Confusion:
[[ 807    2    6    0   11    3    0    1    6   68    1    3    1    3    4    5    1    0    4    1    4]
 [   3  930    4    0   22   30    5   15    1    2    5    5    2    1    5    2    7    2   15    1    6]
 [  13    4  827   12    2    3   33    5    1    5   10    3    7    4    3    7    8    0    6    6   11]
 [   9    3   19  872    3    4    3    4    2    2   22    2    9    4   23    1    4    5   15    4    6]
 [  36   17    3    1  911    9    2    3    2   13    1    3    2    1   16    5   13    1    4    0   11]
 [   9   44    2    5   16  857    2   21    1    2    1   13    6   23    3    3    9    2    5    7   12]
 [   0    3   28    2    1    8  971    7    2    1    9    9    1    0    0    9    4    3    1   18    9]
 [   7   25   18    1    1   58    6  865    3    1    4   10    5    3    3    1    1    2   39   17    7]
 [  12    9    1    2    3    2    0    3  820   53   19    0    6   24   25    1    5    0   12    1    4]
 [  87    2    0    1   12    2    0    1   49  797    0    1    0   16   12    0    3    4    1    3   10]
 [   2    4    9    6    2    7    6    4   13    2  962    0    3    8   11    0    2    0   14    2    7]
 [   3    0    0    0    3   12    1   14    2    0    3  845   34    5    1   15    6   28    3   23   13]
 [   2    1    1    9    0    4    2   12    1    0    4   56  828    3    3    9    7   35    5    5    8]
 [   1    3    2    4    3   19    0    5   20   20   17   15    5  852    6    3    5    4    1    8    8]
 [  10    7    4   25    9    1    0    0   38    8    7    1    5    8  935    0    1    3   25    2    9]
 [   6    2    3    1    4    1    7    0    2    0    0   13    9    3    2  977   12   14    0    2    8]
 [   8   12    8    3   11    8    1    0    5    2    2    8    6    2    3   10  952    1    3    8   19]
 [   2    1    0    5    0    3    1    7    3    4    0   25   28    3    2   15    0  887    4    3   12]
 [   2   15   13   30    4    5    1   22    6    0   12    1    6    2   29    0    3    0  897    1    9]
 [   1    6    5    0    3   20   15   10    1    1    1   28   11    6    1   12   12    2    7  931   15]
 [ 348  331  360  189  313  272  123  223  145  133  303  184  513  421  323  221  467  162  307  390 8204]]

2024-06-06 02:12:09,164 - ==> Best [Top1: 77.132   Top5: 95.897   Sparsity:0.00   Params: 169472 on epoch: 127]
2024-06-06 02:12:09,164 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:12:09,172 - 

2024-06-06 02:12:09,172 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:12:15,413 - Epoch: [135][  100/ 1218]    Overall Loss 0.580257    Objective Loss 0.580257                                        LR 0.000500    Time 0.062387    
2024-06-06 02:12:20,125 - Epoch: [135][  200/ 1218]    Overall Loss 0.581490    Objective Loss 0.581490                                        LR 0.000500    Time 0.054747    
2024-06-06 02:12:24,724 - Epoch: [135][  300/ 1218]    Overall Loss 0.576223    Objective Loss 0.576223                                        LR 0.000500    Time 0.051813    
2024-06-06 02:12:29,309 - Epoch: [135][  400/ 1218]    Overall Loss 0.578549    Objective Loss 0.578549                                        LR 0.000500    Time 0.050318    
2024-06-06 02:12:33,946 - Epoch: [135][  500/ 1218]    Overall Loss 0.577135    Objective Loss 0.577135                                        LR 0.000500    Time 0.049526    
2024-06-06 02:12:38,478 - Epoch: [135][  600/ 1218]    Overall Loss 0.573812    Objective Loss 0.573812                                        LR 0.000500    Time 0.048823    
2024-06-06 02:12:43,130 - Epoch: [135][  700/ 1218]    Overall Loss 0.572406    Objective Loss 0.572406                                        LR 0.000500    Time 0.048492    
2024-06-06 02:12:47,856 - Epoch: [135][  800/ 1218]    Overall Loss 0.572131    Objective Loss 0.572131                                        LR 0.000500    Time 0.048336    
2024-06-06 02:12:52,494 - Epoch: [135][  900/ 1218]    Overall Loss 0.571958    Objective Loss 0.571958                                        LR 0.000500    Time 0.048116    
2024-06-06 02:12:57,218 - Epoch: [135][ 1000/ 1218]    Overall Loss 0.571917    Objective Loss 0.571917                                        LR 0.000500    Time 0.048026    
2024-06-06 02:13:01,841 - Epoch: [135][ 1100/ 1218]    Overall Loss 0.573357    Objective Loss 0.573357                                        LR 0.000500    Time 0.047862    
2024-06-06 02:13:06,564 - Epoch: [135][ 1200/ 1218]    Overall Loss 0.574039    Objective Loss 0.574039                                        LR 0.000500    Time 0.047808    
2024-06-06 02:13:07,343 - Epoch: [135][ 1218/ 1218]    Overall Loss 0.573957    Objective Loss 0.573957    Top1 72.127139    Top5 94.865526    LR 0.000500    Time 0.047740    
2024-06-06 02:13:07,524 - --- validate (epoch=135)-----------
2024-06-06 02:13:07,524 - 34633 samples (256 per mini-batch)
2024-06-06 02:13:13,022 - Epoch: [135][  100/  136]    Loss 0.525263    Top1 74.851562    Top5 95.496094    
2024-06-06 02:13:14,686 - Epoch: [135][  136/  136]    Loss 0.523170    Top1 74.821702    Top5 95.585136    
2024-06-06 02:13:14,894 - ==> Top1: 74.822    Top5: 95.585    Loss: 0.523

2024-06-06 02:13:14,895 - ==> Confusion:
[[ 747    2    4    2   15   10    0    4    7   99    1    2    1    6    9    2    3    3    3    3    8]
 [   2  919    1    0   26   36    7   19    5    5    1    4    3    0    6    0    4    0   10    6    9]
 [   4    6  831   13    3    5   27   14    0    9    8    2    1    4    3    8    6    3    5    7   11]
 [   6    3   16  863    2   12    6    3    0    2   18    2    9    5   24    2    2    7   24    1    9]
 [  21   13    8    2  918   19    1    2    3   14    3    4    0    8   16    5    6    1    3    2    5]
 [   3   22    2    2   10  884    5   26    2    3    0   13    4   29    7    1    6    3    5    9    7]
 [   2    3   12    0    3    7 1003    4    0    2    9    4    3    5    2    3    0    5    3    8    8]
 [   3   15   13    7    1   50    8  881    3    2    5   11    7    6    0    1    0    1   32   25    6]
 [   8    8    4    3    1    3    1    1  788   49   21    3    2   38   43    0    4    7    9    1    8]
 [  51    0    4    0    3    3    0    3   48  835    2    0    4   26   10    2    1    1    3    0    5]
 [   0    6    6   17    0    4    4    5   17    3  950    1    0   17   14    0    0    0    9    4    7]
 [   1    3    3    0    2   28    5    6    1    1    2  841   23   19    0   14    5   21    4   25    7]
 [   1    5    2    3    0   17    2    7    0    0    3   80  785    8    3    6    6   41    8   14    4]
 [   1    1    2    2    7   17    2    4   14   17   11    9    4  883    3    1    3    3    2    7    8]
 [   6    1    4   19   11    1    1    1   23    8    8    4    3   15  955    1    1    5   22    0    9]
 [   0    2    2    2    4    7   14    1    2    0    2   23    4    5    0  961   13   12    1    3    8]
 [   4   21   13    3   15   14    5    0    4    1    2    7    2    7    1   14  928    5    2   12   12]
 [   1    2    2    4    1    2    3    2    1    2    1   25   31    6    3   16    3  892    2    2    4]
 [   2   13   10   25    3    8    3   18    8    2    9    3    4    3   18    2    1    2  914    4    6]
 [   0    5    0    1    2   12   21   10    0    1    0   18   10    9    1    5    6    5    1  974    7]
 [ 204  324  354  164  281  358  209  248  127  199  277  249  410  500  287  268  410  172  224  506 8161]]

2024-06-06 02:13:14,896 - ==> Best [Top1: 77.132   Top5: 95.897   Sparsity:0.00   Params: 169472 on epoch: 127]
2024-06-06 02:13:14,897 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:13:14,904 - 

2024-06-06 02:13:14,904 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:13:21,292 - Epoch: [136][  100/ 1218]    Overall Loss 0.575814    Objective Loss 0.575814                                        LR 0.000500    Time 0.063860    
2024-06-06 02:13:26,293 - Epoch: [136][  200/ 1218]    Overall Loss 0.572054    Objective Loss 0.572054                                        LR 0.000500    Time 0.056926    
2024-06-06 02:13:30,912 - Epoch: [136][  300/ 1218]    Overall Loss 0.568911    Objective Loss 0.568911                                        LR 0.000500    Time 0.053340    
2024-06-06 02:13:35,528 - Epoch: [136][  400/ 1218]    Overall Loss 0.568429    Objective Loss 0.568429                                        LR 0.000500    Time 0.051541    
2024-06-06 02:13:40,095 - Epoch: [136][  500/ 1218]    Overall Loss 0.567266    Objective Loss 0.567266                                        LR 0.000500    Time 0.050362    
2024-06-06 02:13:44,874 - Epoch: [136][  600/ 1218]    Overall Loss 0.566252    Objective Loss 0.566252                                        LR 0.000500    Time 0.049931    
2024-06-06 02:13:49,529 - Epoch: [136][  700/ 1218]    Overall Loss 0.565298    Objective Loss 0.565298                                        LR 0.000500    Time 0.049445    
2024-06-06 02:13:54,153 - Epoch: [136][  800/ 1218]    Overall Loss 0.566547    Objective Loss 0.566547                                        LR 0.000500    Time 0.049042    
2024-06-06 02:13:58,809 - Epoch: [136][  900/ 1218]    Overall Loss 0.566296    Objective Loss 0.566296                                        LR 0.000500    Time 0.048765    
2024-06-06 02:14:03,507 - Epoch: [136][ 1000/ 1218]    Overall Loss 0.566770    Objective Loss 0.566770                                        LR 0.000500    Time 0.048584    
2024-06-06 02:14:08,174 - Epoch: [136][ 1100/ 1218]    Overall Loss 0.566040    Objective Loss 0.566040                                        LR 0.000500    Time 0.048409    
2024-06-06 02:14:12,687 - Epoch: [136][ 1200/ 1218]    Overall Loss 0.566916    Objective Loss 0.566916                                        LR 0.000500    Time 0.048135    
2024-06-06 02:14:13,540 - Epoch: [136][ 1218/ 1218]    Overall Loss 0.566729    Objective Loss 0.566729    Top1 74.083130    Top5 95.354523    LR 0.000500    Time 0.048123    
2024-06-06 02:14:13,754 - --- validate (epoch=136)-----------
2024-06-06 02:14:13,754 - 34633 samples (256 per mini-batch)
2024-06-06 02:14:19,460 - Epoch: [136][  100/  136]    Loss 0.528038    Top1 74.597656    Top5 95.558594    
2024-06-06 02:14:21,210 - Epoch: [136][  136/  136]    Loss 0.528444    Top1 74.685993    Top5 95.579361    
2024-06-06 02:14:21,398 - ==> Top1: 74.686    Top5: 95.579    Loss: 0.528

2024-06-06 02:14:21,399 - ==> Confusion:
[[ 783    0    4    1   15    1    2    3    7   74    1    2    2    7    5    1    5    3    2    0   13]
 [   1  927    2    1   19   26    2   25    4    2    3    5    0    3    8    0    9    1   16    2    7]
 [   4    2  847   10    7    4   20    9    0    4    6    4    4    8    5    3    7    2   13    5    6]
 [   4    3   17  836    8   10    2    2    3    3   24    0   14    1   35    2    5    7   27    1   12]
 [  26   13    2    0  924   14    1    3    2   14    1    2    2   10   15    7    5    2    3    4    4]
 [   6   27    3    2   24  832    4   40    6    4    5   18    9   25    7    1    4    0    7    6   13]
 [   1    4   43    3    0    9  958   10    0    0   10    5    4    1    3    3    8    0    6   12    6]
 [   6   21   10    3    3   39    0  887    4    4    4   14    6    4    0    0    1    0   50   14    7]
 [  16    3    0    2    3    4    1    1  796   65   23    3    2   25   35    0    4    3   11    0    5]
 [  79    1    0    1    9    9    0    1   58  786    2    0    0   23   19    0    3    3    1    0    6]
 [   0    6    6   13    3    3    5    5   18    4  942    0    4   13   14    3    0    1   14    3    7]
 [   1    2    4    1    1   19    1    8    4    1    1  847   30   15    1   13    9   19    3   21   10]
 [   2    2    6    8    1    7    1    3    2    0    1   90  785    7    1   12    9   37    8    6    7]
 [   1    2    1    2    2   13    1    4   17   17    8    8    5  883   14    1    3    5    3    5    6]
 [  16    2    2   12    9    2    0    2   30    5    4    0    2   12  958    0    2    5   26    0    9]
 [   2    2    5    3    6    1    9    2    0    1    1   20   10    2    1  950   18   21    4    1    7]
 [   3   11    8    1   13   11    0    0    3    1    0    9    5    5    4   12  960    1    3    5   17]
 [   2    0    2    2    1    2    5    3    0    3    0   22   35    8    7   14    3  885    3    0    8]
 [   1   12    5   13    4    7    0   24    6    2   10    0    5    2   19    0    3    3  932    3    7]
 [   2    5    7    0    3   11   14   21    0    0    1   22   14    5    2   11   10    7    5  938   10]
 [ 269  311  320  164  352  268  133  249  139  154  239  244  479  428  444  169  429  186  352  393 8210]]

2024-06-06 02:14:21,401 - ==> Best [Top1: 77.132   Top5: 95.897   Sparsity:0.00   Params: 169472 on epoch: 127]
2024-06-06 02:14:21,401 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:14:21,410 - 

2024-06-06 02:14:21,410 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:14:27,530 - Epoch: [137][  100/ 1218]    Overall Loss 0.562821    Objective Loss 0.562821                                        LR 0.000500    Time 0.061179    
2024-06-06 02:14:32,140 - Epoch: [137][  200/ 1218]    Overall Loss 0.564960    Objective Loss 0.564960                                        LR 0.000500    Time 0.053632    
2024-06-06 02:14:36,700 - Epoch: [137][  300/ 1218]    Overall Loss 0.562032    Objective Loss 0.562032                                        LR 0.000500    Time 0.050947    
2024-06-06 02:14:41,426 - Epoch: [137][  400/ 1218]    Overall Loss 0.563883    Objective Loss 0.563883                                        LR 0.000500    Time 0.050021    
2024-06-06 02:14:45,936 - Epoch: [137][  500/ 1218]    Overall Loss 0.565862    Objective Loss 0.565862                                        LR 0.000500    Time 0.049033    
2024-06-06 02:14:50,493 - Epoch: [137][  600/ 1218]    Overall Loss 0.565837    Objective Loss 0.565837                                        LR 0.000500    Time 0.048453    
2024-06-06 02:14:55,138 - Epoch: [137][  700/ 1218]    Overall Loss 0.564812    Objective Loss 0.564812                                        LR 0.000500    Time 0.048165    
2024-06-06 02:14:59,915 - Epoch: [137][  800/ 1218]    Overall Loss 0.563986    Objective Loss 0.563986                                        LR 0.000500    Time 0.048113    
2024-06-06 02:15:04,662 - Epoch: [137][  900/ 1218]    Overall Loss 0.563477    Objective Loss 0.563477                                        LR 0.000500    Time 0.048040    
2024-06-06 02:15:09,346 - Epoch: [137][ 1000/ 1218]    Overall Loss 0.561833    Objective Loss 0.561833                                        LR 0.000500    Time 0.047919    
2024-06-06 02:15:13,949 - Epoch: [137][ 1100/ 1218]    Overall Loss 0.562535    Objective Loss 0.562535                                        LR 0.000500    Time 0.047745    
2024-06-06 02:15:18,574 - Epoch: [137][ 1200/ 1218]    Overall Loss 0.561939    Objective Loss 0.561939                                        LR 0.000500    Time 0.047619    
2024-06-06 02:15:19,351 - Epoch: [137][ 1218/ 1218]    Overall Loss 0.561801    Objective Loss 0.561801    Top1 73.594132    Top5 95.843521    LR 0.000500    Time 0.047553    
2024-06-06 02:15:19,535 - --- validate (epoch=137)-----------
2024-06-06 02:15:19,535 - 34633 samples (256 per mini-batch)
2024-06-06 02:15:25,089 - Epoch: [137][  100/  136]    Loss 0.504415    Top1 75.675781    Top5 95.542969    
2024-06-06 02:15:26,819 - Epoch: [137][  136/  136]    Loss 0.508995    Top1 75.630179    Top5 95.518725    
2024-06-06 02:15:26,992 - ==> Top1: 75.630    Top5: 95.519    Loss: 0.509

2024-06-06 02:15:26,994 - ==> Confusion:
[[ 782    0    4    1    7    1    0    1   10   88    0    1    3    3    3    1    4    4    4    3   11]
 [   2  926    1    1   27   28    4   16    5    1    3    9    4    0    3    2   13    1   10    2    5]
 [   9    4  805   16    8    2   40   15    2    2    4    9    2    8    6    5    7    2    8    5   11]
 [   3    5   16  874    5   11    3    1    5    4   13    3   10    1   28    1    2    6   13    3    9]
 [  29   10    5    2  936   12    2    0    2   15    2    2    4    2   12    2    8    0    4    1    4]
 [   7   23    4    3   19  868    3   38    3    5    1   21    7   12    3    1    4    2    5    5    9]
 [   0    2   19    6    3    3  990    8    2    2    2   11    2    0    2   10    0    5    1   10    8]
 [   7   22   10    2    3   46    9  893    1    1    6   12    8    2    2    2    1    3   26   17    4]
 [  12    6    1    2    0    2    0    1  839   56   16    3    5   17   21    0    4    3    7    2    5]
 [  85    0    1    0    8    4    2    2   50  814    1    2    2   11    6    2    0    1    2    0    8]
 [   2    3    4   15    0    4    4    6   23    4  943    2    2   13   16    1    1    1   10    3    7]
 [   5    0    1    1    1   17    4    4    1    2    1  885   28    5    1    5    7   18    2   17    6]
 [   2    1    1    9    1    3    0    0    5    0    1  105  787    1    3    2    2   45    6    8   13]
 [   4    2    1    0    6   27    2    7   27   24   16   17    4  824    3    3    6    5    0    9   14]
 [  15    5    3   20   17    2    0    2   54   11    5    3    6    6  926    0    1    3   12    2    5]
 [   1    3    4    1    7    2   11    0    1    1    1   25    7    3    1  965   12    9    2    6    4]
 [   6   10    4    1    9   13    1    2    3    1    6   15    3    3    2    9  950    2    4    9   19]
 [   4    0    4    3    0    3    1    2    2    3    0   27   29    2    4   15    3  893    2    5    3]
 [   1    8    5   15    7    5    0   34   14    1    6    8    3    0   25    2    1    1  908    5    9]
 [   1   10    3    0    4   11   18   19    1    1    1   34    6    4    0    7    9    3    2  941   13]
 [ 318  361  257  151  379  279  166  241  179  183  251  299  399  309  307  189  426  191  209  394 8444]]

2024-06-06 02:15:26,995 - ==> Best [Top1: 77.132   Top5: 95.897   Sparsity:0.00   Params: 169472 on epoch: 127]
2024-06-06 02:15:26,995 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:15:27,003 - 

2024-06-06 02:15:27,003 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:15:33,166 - Epoch: [138][  100/ 1218]    Overall Loss 0.563116    Objective Loss 0.563116                                        LR 0.000500    Time 0.061605    
2024-06-06 02:15:37,865 - Epoch: [138][  200/ 1218]    Overall Loss 0.560791    Objective Loss 0.560791                                        LR 0.000500    Time 0.054292    
2024-06-06 02:15:42,578 - Epoch: [138][  300/ 1218]    Overall Loss 0.555327    Objective Loss 0.555327                                        LR 0.000500    Time 0.051897    
2024-06-06 02:15:47,303 - Epoch: [138][  400/ 1218]    Overall Loss 0.554848    Objective Loss 0.554848                                        LR 0.000500    Time 0.050731    
2024-06-06 02:15:51,959 - Epoch: [138][  500/ 1218]    Overall Loss 0.556829    Objective Loss 0.556829                                        LR 0.000500    Time 0.049895    
2024-06-06 02:15:56,682 - Epoch: [138][  600/ 1218]    Overall Loss 0.556809    Objective Loss 0.556809                                        LR 0.000500    Time 0.049447    
2024-06-06 02:16:01,416 - Epoch: [138][  700/ 1218]    Overall Loss 0.558785    Objective Loss 0.558785                                        LR 0.000500    Time 0.049143    
2024-06-06 02:16:06,014 - Epoch: [138][  800/ 1218]    Overall Loss 0.559570    Objective Loss 0.559570                                        LR 0.000500    Time 0.048746    
2024-06-06 02:16:10,547 - Epoch: [138][  900/ 1218]    Overall Loss 0.559385    Objective Loss 0.559385                                        LR 0.000500    Time 0.048364    
2024-06-06 02:16:15,364 - Epoch: [138][ 1000/ 1218]    Overall Loss 0.560861    Objective Loss 0.560861                                        LR 0.000500    Time 0.048343    
2024-06-06 02:16:19,982 - Epoch: [138][ 1100/ 1218]    Overall Loss 0.560711    Objective Loss 0.560711                                        LR 0.000500    Time 0.048145    
2024-06-06 02:16:24,521 - Epoch: [138][ 1200/ 1218]    Overall Loss 0.561924    Objective Loss 0.561924                                        LR 0.000500    Time 0.047915    
2024-06-06 02:16:25,303 - Epoch: [138][ 1218/ 1218]    Overall Loss 0.562171    Objective Loss 0.562171    Top1 68.948655    Top5 93.643032    LR 0.000500    Time 0.047848    
2024-06-06 02:16:25,472 - --- validate (epoch=138)-----------
2024-06-06 02:16:25,472 - 34633 samples (256 per mini-batch)
2024-06-06 02:16:30,972 - Epoch: [138][  100/  136]    Loss 0.511019    Top1 74.621094    Top5 94.726562    
2024-06-06 02:16:32,718 - Epoch: [138][  136/  136]    Loss 0.511610    Top1 74.541622    Top5 94.767996    
2024-06-06 02:16:32,908 - ==> Top1: 74.542    Top5: 94.768    Loss: 0.512

2024-06-06 02:16:32,909 - ==> Confusion:
[[ 792    2    1    2   13    0    0    1   10   78    2    3    3    3    7    3    4    0    3    1    3]
 [   2  934    3    1   27   13    4   15    5    2    6   10    3    1    6    2    6    3   15    2    3]
 [   9    9  822   13    4    1   30   15    3    6   11    2    5    2    4    4    6    3    6    4   11]
 [   5    4   18  857    5    8    3    1    6    1   21    3   13    0   37    1    2   10   17    1    3]
 [  32   12    0    0  923   10    1    1    3   14    2    7    5    1   12    6   13    0    4    2    6]
 [   8   44    1    4   18  798    4   42    3    7    6   26   14   19    3    1   10    4    8   17    6]
 [   0    7   27    2    3    1  977    9    4    0    2    6    6    0    1    7    4    2    3   19    6]
 [   6   25   10    2    2   27    9  892    6    3    4   22   10    1    1    0    0    1   42   11    3]
 [  14    1    2    2    2    5    0    3  838   61   11    1    5   17   20    0    0    4    9    0    7]
 [  85    2    3    1   10    2    0    2   76  779    4    0    2   14    7    1    2    3    3    0    5]
 [   1    3    8    8    3    2    6    7   25    4  943    1    2    8    9    1    5    1   14    6    7]
 [   2    2    0    0    2    7    4    4    1    1    0  880   32    6    0    8   10   26    4   11   11]
 [   2    1    1    7    1    5    2    3    4    0    2   78  820    1    2    6    6   34   12    2    6]
 [   3    0    1    0    5   14    0    6   35   39   13    9   11  819    9    4    5    6    2    9   11]
 [  17    3    1   16    9    0    0    0   49    9    4    2    4    5  946    1    2    5   16    1    8]
 [   4    1    3    3    2    0   10    0    1    4    0   15   12    1    1  975    6   18    6    1    3]
 [   4    4    3    2    2   10    1    3    8    3    3    9    6    2    1   16  978    0    5    6    6]
 [   4    2    0    3    1    1    1    1    0    2    0   25   34    4    4   13    2  897    4    2    5]
 [   2   10    7   13    3    3    0   21   13    0    8    3    5    2   19    0    2    3  935    5    4]
 [   2    4    2    1    3    7   13   16    1    1    0   44   16    2    0    7   12   11    5  932    9]
 [ 357  377  282  200  312  195  159  255  260  168  262  254  552  293  287  181  542  241  309  367 8079]]

2024-06-06 02:16:32,911 - ==> Best [Top1: 77.132   Top5: 95.897   Sparsity:0.00   Params: 169472 on epoch: 127]
2024-06-06 02:16:32,911 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:16:32,918 - 

2024-06-06 02:16:32,919 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:16:39,122 - Epoch: [139][  100/ 1218]    Overall Loss 0.556032    Objective Loss 0.556032                                        LR 0.000500    Time 0.062013    
2024-06-06 02:16:43,869 - Epoch: [139][  200/ 1218]    Overall Loss 0.555030    Objective Loss 0.555030                                        LR 0.000500    Time 0.054730    
2024-06-06 02:16:48,523 - Epoch: [139][  300/ 1218]    Overall Loss 0.555981    Objective Loss 0.555981                                        LR 0.000500    Time 0.051996    
2024-06-06 02:16:53,060 - Epoch: [139][  400/ 1218]    Overall Loss 0.556574    Objective Loss 0.556574                                        LR 0.000500    Time 0.050335    
2024-06-06 02:16:57,667 - Epoch: [139][  500/ 1218]    Overall Loss 0.558112    Objective Loss 0.558112                                        LR 0.000500    Time 0.049478    
2024-06-06 02:17:02,266 - Epoch: [139][  600/ 1218]    Overall Loss 0.557437    Objective Loss 0.557437                                        LR 0.000500    Time 0.048894    
2024-06-06 02:17:06,858 - Epoch: [139][  700/ 1218]    Overall Loss 0.558283    Objective Loss 0.558283                                        LR 0.000500    Time 0.048467    
2024-06-06 02:17:11,630 - Epoch: [139][  800/ 1218]    Overall Loss 0.558232    Objective Loss 0.558232                                        LR 0.000500    Time 0.048373    
2024-06-06 02:17:16,221 - Epoch: [139][  900/ 1218]    Overall Loss 0.557370    Objective Loss 0.557370                                        LR 0.000500    Time 0.048096    
2024-06-06 02:17:20,879 - Epoch: [139][ 1000/ 1218]    Overall Loss 0.559286    Objective Loss 0.559286                                        LR 0.000500    Time 0.047943    
2024-06-06 02:17:25,546 - Epoch: [139][ 1100/ 1218]    Overall Loss 0.559629    Objective Loss 0.559629                                        LR 0.000500    Time 0.047826    
2024-06-06 02:17:30,343 - Epoch: [139][ 1200/ 1218]    Overall Loss 0.560366    Objective Loss 0.560366                                        LR 0.000500    Time 0.047837    
2024-06-06 02:17:31,113 - Epoch: [139][ 1218/ 1218]    Overall Loss 0.560282    Objective Loss 0.560282    Top1 74.083130    Top5 94.865526    LR 0.000500    Time 0.047761    
2024-06-06 02:17:31,276 - --- validate (epoch=139)-----------
2024-06-06 02:17:31,277 - 34633 samples (256 per mini-batch)
2024-06-06 02:17:36,872 - Epoch: [139][  100/  136]    Loss 0.510200    Top1 74.929688    Top5 95.187500    
2024-06-06 02:17:38,511 - Epoch: [139][  136/  136]    Loss 0.509918    Top1 74.876563    Top5 95.212658    
2024-06-06 02:17:38,719 - ==> Top1: 74.877    Top5: 95.213    Loss: 0.510

2024-06-06 02:17:38,720 - ==> Confusion:
[[ 778    0    4    1   19    1    0    2    1   79    0    5    3    7    9    3    1    2    8    0    8]
 [   2  904    3    3   34   23    5   15    3    1    5   12    4    1    6    3    9    2   14    3   11]
 [   7    1  832   16    5    4   26    9    1    5    7    5    5    6   10    7    4    1    8    4    7]
 [   5    2   20  869    5    7    4    0    2    4   14    3    9    5   34    2    4    6   15    0    6]
 [  24    3    1    0  954   12    0    2    4   15    0    2    2    5    7    8    4    2    2    1    6]
 [   5   22    2    6   25  838    2   42    4    3    3   23    8   19    5    2    5    4    9    9    7]
 [   1    1   24    4    5    4  975    3    0    3    6    5    4    3    0   18    4    5    4   15    2]
 [   5   16   11    1    7   32    6  886    2    2    3   20   10    9    0    2    1    2   43   13    6]
 [   9    1    0    2    2    1    1    1  805   75   14    2    5   20   36    0    4    3   12    2    7]
 [  77    1    2    1   13    4    0    1   32  830    0    0    2   19    7    3    0    3    2    0    4]
 [   0    2   10   24    2    4    2    6   24    1  912    3    4   20   17    0    2    0   23    4    4]
 [   1    2    1    0    3    8    2    4    1    3    2  885   32    8    4   13    4   22    0   12    4]
 [   2    0    3    8    0    4    3    3    1    1    1   98  795    4    5    7    5   33   11    4    7]
 [   1    0    1    0    9    9    2    5   21   22    9   20    7  869    6    1    0    4    3    5    7]
 [   6    2    2   13   12    0    0    2   23   14    4    0    5   11  989    0    0    4    6    0    5]
 [   4    2    3    1    3    2    4    1    0    0    0   21   15    5    0  975    8   14    4    0    4]
 [   4    5    5    5   16    6    2    2    9    0    3   12    6    6    2   21  938    3    3   11   13]
 [   1    1    0    7    3    2    0    1    1    5    0   25   39    8    7   19    1  877    2    2    4]
 [   8    7    9   22    4    4    0   25    4    1    1    4   11    1   29    1    1    3  919    0    4]
 [   2    7    6    0    3   12   17   10    1    2    1   33   14    7    2    9    9    3    7  932   11]
 [ 303  221  281  206  453  207  127  273  134  212  197  340  511  433  443  299  365  155  259  343 8170]]

2024-06-06 02:17:38,722 - ==> Best [Top1: 77.132   Top5: 95.897   Sparsity:0.00   Params: 169472 on epoch: 127]
2024-06-06 02:17:38,722 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:17:38,730 - 

2024-06-06 02:17:38,731 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:17:45,152 - Epoch: [140][  100/ 1218]    Overall Loss 0.560235    Objective Loss 0.560235                                        LR 0.000250    Time 0.064193    
2024-06-06 02:17:49,842 - Epoch: [140][  200/ 1218]    Overall Loss 0.545008    Objective Loss 0.545008                                        LR 0.000250    Time 0.055539    
2024-06-06 02:17:54,633 - Epoch: [140][  300/ 1218]    Overall Loss 0.536995    Objective Loss 0.536995                                        LR 0.000250    Time 0.052990    
2024-06-06 02:17:59,235 - Epoch: [140][  400/ 1218]    Overall Loss 0.537969    Objective Loss 0.537969                                        LR 0.000250    Time 0.051244    
2024-06-06 02:18:03,774 - Epoch: [140][  500/ 1218]    Overall Loss 0.538246    Objective Loss 0.538246                                        LR 0.000250    Time 0.050069    
2024-06-06 02:18:08,321 - Epoch: [140][  600/ 1218]    Overall Loss 0.538521    Objective Loss 0.538521                                        LR 0.000250    Time 0.049299    
2024-06-06 02:18:12,878 - Epoch: [140][  700/ 1218]    Overall Loss 0.539445    Objective Loss 0.539445                                        LR 0.000250    Time 0.048764    
2024-06-06 02:18:17,678 - Epoch: [140][  800/ 1218]    Overall Loss 0.539482    Objective Loss 0.539482                                        LR 0.000250    Time 0.048666    
2024-06-06 02:18:22,427 - Epoch: [140][  900/ 1218]    Overall Loss 0.539150    Objective Loss 0.539150                                        LR 0.000250    Time 0.048533    
2024-06-06 02:18:27,030 - Epoch: [140][ 1000/ 1218]    Overall Loss 0.539772    Objective Loss 0.539772                                        LR 0.000250    Time 0.048282    
2024-06-06 02:18:31,664 - Epoch: [140][ 1100/ 1218]    Overall Loss 0.539016    Objective Loss 0.539016                                        LR 0.000250    Time 0.048103    
2024-06-06 02:18:36,213 - Epoch: [140][ 1200/ 1218]    Overall Loss 0.538645    Objective Loss 0.538645                                        LR 0.000250    Time 0.047884    
2024-06-06 02:18:37,100 - Epoch: [140][ 1218/ 1218]    Overall Loss 0.538867    Objective Loss 0.538867    Top1 75.550122    Top5 95.110024    LR 0.000250    Time 0.047905    
2024-06-06 02:18:37,284 - --- validate (epoch=140)-----------
2024-06-06 02:18:37,285 - 34633 samples (256 per mini-batch)
2024-06-06 02:18:42,919 - Epoch: [140][  100/  136]    Loss 0.489529    Top1 76.605469    Top5 96.199219    
2024-06-06 02:18:44,704 - Epoch: [140][  136/  136]    Loss 0.493645    Top1 76.464644    Top5 96.165507    
2024-06-06 02:18:44,891 - ==> Top1: 76.465    Top5: 96.166    Loss: 0.494

2024-06-06 02:18:44,893 - ==> Confusion:
[[ 804    0    4    2   11    2    0    2    6   63    1    5    4    4    8    2    1    3    2    1    6]
 [   7  932    2    2   17   28    1   14    3    1    1    3    2    1    8    3    7    2   18    4    7]
 [   7    2  842   19    5    1   15    7    2    3    6    3    5    6    4    8    9    2    9    4   11]
 [   3    1   11  884    3   10    5    3    1    1   14    2    8    5   31    5    4    4   14    1    6]
 [  33   17    3    3  910   10    0    2    2   12    0    5    1    8   12    5   11    1    6    1   12]
 [   5   30    1    6   16  861    3   29    2    3    0   30    5   21    3    2    1    1    4    8   12]
 [   3    8   46    4    2    6  959    8    1    0    4    7    0    2    1    5    8    3    5   10    4]
 [   4   23   14    3    3   40    4  888    2    2    4   11    6    5    1    1    1    6   35   13   11]
 [  15    7    1    2    1    3    0    2  802   55   15    4    6   21   36    1    4    2   13    2   10]
 [  91    2    2    0   12    7    0    1   43  784    0    2    1   22   16    1    4    2    4    3    4]
 [   1    4    7   19    2    7    2    6   14    3  931    3    1   12   16    1    1    0   22    3    9]
 [   6    2    3    2    1   15    1    3    1    0    1  880   20    5    1   15    6   14    8   21    6]
 [   0    1    1    6    0    3    2    4    2    0    1   91  804    3    3    7    4   33   11   12    7]
 [   3    1    2    0    6   10    1    0   11   21   12   16    4  870    9    4    5    6    1    7   12]
 [  14    6    2   15    5    0    0    1   31    8    4    2    4    7  970    2    0    4   11    0   12]
 [   1    4    8    6    5    1    6    1    0    3    0   21    7    1    1  961    9   16    0    6    9]
 [   5    9    5    2    9   12    1    1    3    0    0    7    4    1    2   22  963    3    5    5   13]
 [   4    1    0    2    1    0    1    1    0    0    0   35   32    3    4   16    5  888    4    1    7]
 [   4    8    7   12    1    4    1   18    5    2    3    6    4    2   29    0    3    1  936    3    9]
 [   1   10    6    0    1   14   17   12    0    0    0   36    8    5    2   10    5    3    3  942   13]
 [ 283  272  299  198  282  271  132  216  124  138  182  317  450  391  358  218  363  158  274  335 8671]]

2024-06-06 02:18:44,894 - ==> Best [Top1: 77.132   Top5: 95.897   Sparsity:0.00   Params: 169472 on epoch: 127]
2024-06-06 02:18:44,894 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:18:44,903 - 

2024-06-06 02:18:44,903 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:18:50,918 - Epoch: [141][  100/ 1218]    Overall Loss 0.538412    Objective Loss 0.538412                                        LR 0.000250    Time 0.060127    
2024-06-06 02:18:55,625 - Epoch: [141][  200/ 1218]    Overall Loss 0.541660    Objective Loss 0.541660                                        LR 0.000250    Time 0.053593    
2024-06-06 02:19:00,254 - Epoch: [141][  300/ 1218]    Overall Loss 0.541708    Objective Loss 0.541708                                        LR 0.000250    Time 0.051151    
2024-06-06 02:19:04,839 - Epoch: [141][  400/ 1218]    Overall Loss 0.541601    Objective Loss 0.541601                                        LR 0.000250    Time 0.049821    
2024-06-06 02:19:09,556 - Epoch: [141][  500/ 1218]    Overall Loss 0.540885    Objective Loss 0.540885                                        LR 0.000250    Time 0.049287    
2024-06-06 02:19:14,152 - Epoch: [141][  600/ 1218]    Overall Loss 0.539753    Objective Loss 0.539753                                        LR 0.000250    Time 0.048730    
2024-06-06 02:19:18,993 - Epoch: [141][  700/ 1218]    Overall Loss 0.538981    Objective Loss 0.538981                                        LR 0.000250    Time 0.048682    
2024-06-06 02:19:23,563 - Epoch: [141][  800/ 1218]    Overall Loss 0.539527    Objective Loss 0.539527                                        LR 0.000250    Time 0.048307    
2024-06-06 02:19:28,152 - Epoch: [141][  900/ 1218]    Overall Loss 0.538916    Objective Loss 0.538916                                        LR 0.000250    Time 0.048037    
2024-06-06 02:19:32,809 - Epoch: [141][ 1000/ 1218]    Overall Loss 0.538219    Objective Loss 0.538219                                        LR 0.000250    Time 0.047889    
2024-06-06 02:19:37,571 - Epoch: [141][ 1100/ 1218]    Overall Loss 0.539109    Objective Loss 0.539109                                        LR 0.000250    Time 0.047863    
2024-06-06 02:19:42,232 - Epoch: [141][ 1200/ 1218]    Overall Loss 0.538916    Objective Loss 0.538916                                        LR 0.000250    Time 0.047757    
2024-06-06 02:19:43,016 - Epoch: [141][ 1218/ 1218]    Overall Loss 0.538563    Objective Loss 0.538563    Top1 77.506112    Top5 97.066015    LR 0.000250    Time 0.047694    
2024-06-06 02:19:43,213 - --- validate (epoch=141)-----------
2024-06-06 02:19:43,213 - 34633 samples (256 per mini-batch)
2024-06-06 02:19:48,720 - Epoch: [141][  100/  136]    Loss 0.488947    Top1 77.003906    Top5 95.933594    
2024-06-06 02:19:50,392 - Epoch: [141][  136/  136]    Loss 0.488953    Top1 77.019028    Top5 96.012474    
2024-06-06 02:19:50,568 - ==> Top1: 77.019    Top5: 96.012    Loss: 0.489

2024-06-06 02:19:50,569 - ==> Confusion:
[[ 790    1    4    1   11    3    0    1    9   74    0    3    4    4    4    0    3    1    7    4    7]
 [   1  947    0    2   16   30    6   14    2    2    3    6    0    2    2    1    5    0   14    3    7]
 [   6    3  840   12    2    4   16   15    2   12    9    3    3    9    4    8    3    1    4    4   10]
 [   1    2   21  866    1    5    1    0    1    4   15    3    9    6   31    3    2    8   28    1    8]
 [  24   14    3    1  930    9    2    6    5   14    1    5    2    4   12    2    6    0    4    1    9]
 [   4   35    2    4   10  864    2   32    8    5    2   12    3   26    1    0    8    1    6    8   10]
 [   3    7   27    2    4   11  959   11    0    1    5   10    2    3    1    4    2    6    4   14   10]
 [   4   17   17    2    0   38    2  897    5    1    3   14    5    7    1    1    0    3   32   18   10]
 [  16    2    1    2    3    2    1    3  844   51    6    4    6   23   16    0    2    2    9    0    9]
 [  63    0    2    1    5    3    0    1   57  817    0    0    0   27    9    0    0    2    4    2    8]
 [   3    5    6   18    2    5    0    7   21    1  933    0    2   20   11    0    2    1   16    1   10]
 [   4    2    2    1    2   15    3    9    0    2    0  872   29   16    4    6    4   20    4   13    3]
 [   3    2    5    4    2    7    2    2    3    0    1   81  800    5    2    7    2   38   14    4   11]
 [   5    1    1    1    7    9    0    5   17   23    5   11    3  882    1    0    2    5    1    2   20]
 [   8    3    2   14   10    3    0    0   34   14    2    5    3   14  939    0    2    5   27    2   11]
 [   5    1    4    0    4    3    6    0    1    2    0   21   10    6    0  965   12   16    1    2    7]
 [   5    6   10    2    6   13    1    0    8    1    0    8    5    8    4    8  952    1    5    7   22]
 [   1    1    3    5    1    2    2    2    1    5    0   30   29    5    2    6    3  893    2    6    6]
 [   2    7    7   16    3    2    0   22    5    1    3    3    4    4   20    0    1    1  947    4    6]
 [   1    7    6    1    2   16   13   19    2    0    3   25   11   12    0    5    6    4    0  939   16]
 [ 272  326  278  156  253  251  113  251  160  181  194  252  442  426  275  163  316  173  305  348 8797]]

2024-06-06 02:19:50,570 - ==> Best [Top1: 77.132   Top5: 95.897   Sparsity:0.00   Params: 169472 on epoch: 127]
2024-06-06 02:19:50,571 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:19:50,578 - 

2024-06-06 02:19:50,579 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:19:56,907 - Epoch: [142][  100/ 1218]    Overall Loss 0.530244    Objective Loss 0.530244                                        LR 0.000250    Time 0.063261    
2024-06-06 02:20:01,760 - Epoch: [142][  200/ 1218]    Overall Loss 0.527780    Objective Loss 0.527780                                        LR 0.000250    Time 0.055886    
2024-06-06 02:20:06,516 - Epoch: [142][  300/ 1218]    Overall Loss 0.532018    Objective Loss 0.532018                                        LR 0.000250    Time 0.053106    
2024-06-06 02:20:11,489 - Epoch: [142][  400/ 1218]    Overall Loss 0.532971    Objective Loss 0.532971                                        LR 0.000250    Time 0.052258    
2024-06-06 02:20:16,476 - Epoch: [142][  500/ 1218]    Overall Loss 0.533198    Objective Loss 0.533198                                        LR 0.000250    Time 0.051776    
2024-06-06 02:20:21,499 - Epoch: [142][  600/ 1218]    Overall Loss 0.534315    Objective Loss 0.534315                                        LR 0.000250    Time 0.051516    
2024-06-06 02:20:26,552 - Epoch: [142][  700/ 1218]    Overall Loss 0.535722    Objective Loss 0.535722                                        LR 0.000250    Time 0.051372    
2024-06-06 02:20:31,450 - Epoch: [142][  800/ 1218]    Overall Loss 0.533348    Objective Loss 0.533348                                        LR 0.000250    Time 0.051070    
2024-06-06 02:20:36,005 - Epoch: [142][  900/ 1218]    Overall Loss 0.535004    Objective Loss 0.535004                                        LR 0.000250    Time 0.050455    
2024-06-06 02:20:40,611 - Epoch: [142][ 1000/ 1218]    Overall Loss 0.535051    Objective Loss 0.535051                                        LR 0.000250    Time 0.050014    
2024-06-06 02:20:45,249 - Epoch: [142][ 1100/ 1218]    Overall Loss 0.536043    Objective Loss 0.536043                                        LR 0.000250    Time 0.049682    
2024-06-06 02:20:49,868 - Epoch: [142][ 1200/ 1218]    Overall Loss 0.535300    Objective Loss 0.535300                                        LR 0.000250    Time 0.049390    
2024-06-06 02:20:50,703 - Epoch: [142][ 1218/ 1218]    Overall Loss 0.535396    Objective Loss 0.535396    Top1 78.239609    Top5 95.354523    LR 0.000250    Time 0.049345    
2024-06-06 02:20:50,875 - --- validate (epoch=142)-----------
2024-06-06 02:20:50,875 - 34633 samples (256 per mini-batch)
2024-06-06 02:20:56,641 - Epoch: [142][  100/  136]    Loss 0.494764    Top1 76.035156    Top5 95.828125    
2024-06-06 02:20:58,293 - Epoch: [142][  136/  136]    Loss 0.496371    Top1 76.103716    Top5 95.778593    
2024-06-06 02:20:58,484 - ==> Top1: 76.104    Top5: 95.779    Loss: 0.496

2024-06-06 02:20:58,485 - ==> Confusion:
[[ 769    0    5    1   17    6    2    2    8   86    1    2    1    3    9    5    2    3    5    0    4]
 [   3  919    2    1   21   34    3   20    2    2    1    2    3    4    6    2    6    1   18    6    7]
 [   7    3  846    9    3    4   19   13    1    4   10    3    1    8    6   11    2    1    8    3    8]
 [   6    3   25  857    4    4    3    3    2    3   22    4    6    5   29    2    2   11   16    0    9]
 [  26   11    2    1  917   12    2    0    2   26    0    3    3    3    9    5    9    4   10    0    9]
 [   4   33    2    2   16  829    4   55    7    7    3   16    6   28    5    3    7    4    5    4    3]
 [   1    1   21    3    4    3  980   18    2    0    6    4    2    1    1    7    5    4    4   11    8]
 [   3   10   10    1    1   41    4  901    3    1    8    9    7    4    3    1    0    6   38   13   13]
 [  16    3    1    0    1    2    1    1  812   73   14    1    8   22   21    0    0    5   13    2    6]
 [  71    1    2    0    4    1    0    1   39  828    1    1    0   27    9    3    0    2    3    2    6]
 [   1    2    9   12    0    3    8    7   27    4  933    0    2   15   11    1    2    0   17    1    9]
 [   4    3    1    1    4   17    3    8    1    1    2  860   28   10    3   10    6   28    8    9    4]
 [   0    1    3    2    0    6    1    3    2    0    3   90  790    7    3    7    3   43   12    9   10]
 [   0    2    2    1    5    7    0    8   23   26   10   10    7  864    6    2    7    4    2    5   10]
 [   9    5    5   15   12    1    0    3   35   13    6    2    4   11  936    0    1    7   23    2    8]
 [   3    1    6    0    3    1    6    1    1    0    0   28    9    6    0  963   14   11    3    5    5]
 [   4   12   10    0    8    8    3    2    7    0    1   14    4    8    3   12  945    3    6    8   14]
 [   3    0    1    3    1    2    4    0    5    1    0   17   30    4    6   13    0  904    1    3    7]
 [   3    7    6    8    2    4    1   18   10    0    7    0    3    3   14    1    2    0  960    3    6]
 [   1    6    5    0    1   10   11   17    1    1    1   37    8   10    1    5    6    3    6  951    7]
 [ 278  256  312  129  299  236  131  276  200  179  253  247  442  392  315  191  346  180  325  352 8593]]

2024-06-06 02:20:58,487 - ==> Best [Top1: 77.132   Top5: 95.897   Sparsity:0.00   Params: 169472 on epoch: 127]
2024-06-06 02:20:58,487 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:20:58,495 - 

2024-06-06 02:20:58,495 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:21:04,503 - Epoch: [143][  100/ 1218]    Overall Loss 0.540361    Objective Loss 0.540361                                        LR 0.000250    Time 0.060065    
2024-06-06 02:21:09,067 - Epoch: [143][  200/ 1218]    Overall Loss 0.532526    Objective Loss 0.532526                                        LR 0.000250    Time 0.052840    
2024-06-06 02:21:13,654 - Epoch: [143][  300/ 1218]    Overall Loss 0.533264    Objective Loss 0.533264                                        LR 0.000250    Time 0.050513    
2024-06-06 02:21:18,348 - Epoch: [143][  400/ 1218]    Overall Loss 0.533324    Objective Loss 0.533324                                        LR 0.000250    Time 0.049607    
2024-06-06 02:21:22,881 - Epoch: [143][  500/ 1218]    Overall Loss 0.536322    Objective Loss 0.536322                                        LR 0.000250    Time 0.048746    
2024-06-06 02:21:27,491 - Epoch: [143][  600/ 1218]    Overall Loss 0.536519    Objective Loss 0.536519                                        LR 0.000250    Time 0.048302    
2024-06-06 02:21:32,024 - Epoch: [143][  700/ 1218]    Overall Loss 0.535960    Objective Loss 0.535960                                        LR 0.000250    Time 0.047875    
2024-06-06 02:21:36,538 - Epoch: [143][  800/ 1218]    Overall Loss 0.536209    Objective Loss 0.536209                                        LR 0.000250    Time 0.047530    
2024-06-06 02:21:41,192 - Epoch: [143][  900/ 1218]    Overall Loss 0.536316    Objective Loss 0.536316                                        LR 0.000250    Time 0.047419    
2024-06-06 02:21:45,826 - Epoch: [143][ 1000/ 1218]    Overall Loss 0.536368    Objective Loss 0.536368                                        LR 0.000250    Time 0.047309    
2024-06-06 02:21:50,511 - Epoch: [143][ 1100/ 1218]    Overall Loss 0.534871    Objective Loss 0.534871                                        LR 0.000250    Time 0.047266    
2024-06-06 02:21:55,273 - Epoch: [143][ 1200/ 1218]    Overall Loss 0.534000    Objective Loss 0.534000                                        LR 0.000250    Time 0.047294    
2024-06-06 02:21:56,040 - Epoch: [143][ 1218/ 1218]    Overall Loss 0.534104    Objective Loss 0.534104    Top1 76.772616    Top5 95.354523    LR 0.000250    Time 0.047225    
2024-06-06 02:21:56,204 - --- validate (epoch=143)-----------
2024-06-06 02:21:56,204 - 34633 samples (256 per mini-batch)
2024-06-06 02:22:01,773 - Epoch: [143][  100/  136]    Loss 0.494095    Top1 75.992188    Top5 95.472656    
2024-06-06 02:22:03,399 - Epoch: [143][  136/  136]    Loss 0.490345    Top1 76.002656    Top5 95.590910    
2024-06-06 02:22:03,580 - ==> Top1: 76.003    Top5: 95.591    Loss: 0.490

2024-06-06 02:22:03,581 - ==> Confusion:
[[ 794    2    5    0   13    1    0    1    8   73    0    4    3    4    2    1    3    6    4    1    6]
 [   2  910    3    3   21   32    1   18    3    3    4    7    6    2    5    3   13    5   10    5    7]
 [   9    3  835   19    6    4   31    9    0    5    5    5    7    7    1    7    4    2    3    2    6]
 [   3    2   17  856    6   11    3    2    3    2   18    2    5    5   44    2    5   13   15    0    2]
 [  25   14    4    2  934    8    2    1    1   17    1    5    2    5    8    6    9    1    5    1    3]
 [   4   28    3    0   20  842    1   35    5    9    2   19    3   25    2    4    9    6    1   13   12]
 [   2    5   32    1    1    5  969    8    2    2    1    5    5    4    0    7    7    3    4   14    9]
 [   2   19   15    2    1   46    4  896    2    1    3   13    6    2    2    2    1    3   25   23    9]
 [  15    4    1    1    1    3    1    0  822   69    9    6    5   21   21    0    2    5   10    2    4]
 [  75    2    3    0    7    1    1    2   43  830    2    0    5   11    9    2    2    1    2    0    3]
 [   2    3   11   15    1    6    1    6   23    4  933    0    3   20   14    0    2    1    9    3    7]
 [   1    2    1    1    5   10    1    2    0    1    0  879   23    9    4   11    5   29    1   21    5]
 [   1    2    1    5    1    1    0    1    3    1    2   93  798    3    3    9    2   48    7    4   10]
 [   2    0    1    1    5   10    1    2   14   21    9   20    6  859    6    4    3   12    1   16    8]
 [   6    7    2   11    9    2    0    2   28   16    3    4    2   12  964    1    1    6   13    0    9]
 [   2    0    3    0    2    2    9    0    0    2    0   20   11    3    1  971    9   16    3    5    7]
 [   6   11    7    2    4    9    1    2    2    1    0   15    7    2    2    8  964    3    3    7   16]
 [   2    3    2    1    0    0    0    3    1    2    0   26   25    2    1   11    0  918    1    1    6]
 [   1    7   11   16    6    2    1   26   10    1    6    6    6    3   25    0    2    2  919    4    4]
 [   1    9    3    0    3   11   14    9    0    1    0   29    9    4    0    9    6    7    4  961    8]
 [ 286  289  296  135  308  245  126  222  146  169  197  314  473  396  303  231  435  234  228  431 8468]]

2024-06-06 02:22:03,583 - ==> Best [Top1: 77.132   Top5: 95.897   Sparsity:0.00   Params: 169472 on epoch: 127]
2024-06-06 02:22:03,583 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:22:03,592 - 

2024-06-06 02:22:03,592 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:22:09,779 - Epoch: [144][  100/ 1218]    Overall Loss 0.525217    Objective Loss 0.525217                                        LR 0.000250    Time 0.061840    
2024-06-06 02:22:14,366 - Epoch: [144][  200/ 1218]    Overall Loss 0.535557    Objective Loss 0.535557                                        LR 0.000250    Time 0.053851    
2024-06-06 02:22:18,972 - Epoch: [144][  300/ 1218]    Overall Loss 0.532060    Objective Loss 0.532060                                        LR 0.000250    Time 0.051246    
2024-06-06 02:22:23,564 - Epoch: [144][  400/ 1218]    Overall Loss 0.530807    Objective Loss 0.530807                                        LR 0.000250    Time 0.049911    
2024-06-06 02:22:28,139 - Epoch: [144][  500/ 1218]    Overall Loss 0.531347    Objective Loss 0.531347                                        LR 0.000250    Time 0.049076    
2024-06-06 02:22:32,970 - Epoch: [144][  600/ 1218]    Overall Loss 0.531299    Objective Loss 0.531299                                        LR 0.000250    Time 0.048946    
2024-06-06 02:22:37,702 - Epoch: [144][  700/ 1218]    Overall Loss 0.530826    Objective Loss 0.530826                                        LR 0.000250    Time 0.048711    
2024-06-06 02:22:42,406 - Epoch: [144][  800/ 1218]    Overall Loss 0.530484    Objective Loss 0.530484                                        LR 0.000250    Time 0.048500    
2024-06-06 02:22:47,229 - Epoch: [144][  900/ 1218]    Overall Loss 0.530037    Objective Loss 0.530037                                        LR 0.000250    Time 0.048468    
2024-06-06 02:22:51,898 - Epoch: [144][ 1000/ 1218]    Overall Loss 0.528804    Objective Loss 0.528804                                        LR 0.000250    Time 0.048288    
2024-06-06 02:22:56,638 - Epoch: [144][ 1100/ 1218]    Overall Loss 0.528844    Objective Loss 0.528844                                        LR 0.000250    Time 0.048206    
2024-06-06 02:23:01,228 - Epoch: [144][ 1200/ 1218]    Overall Loss 0.528584    Objective Loss 0.528584                                        LR 0.000250    Time 0.048012    
2024-06-06 02:23:02,001 - Epoch: [144][ 1218/ 1218]    Overall Loss 0.528461    Objective Loss 0.528461    Top1 80.929095    Top5 95.599022    LR 0.000250    Time 0.047937    
2024-06-06 02:23:02,183 - --- validate (epoch=144)-----------
2024-06-06 02:23:02,183 - 34633 samples (256 per mini-batch)
2024-06-06 02:23:07,826 - Epoch: [144][  100/  136]    Loss 0.495398    Top1 75.667969    Top5 95.410156    
2024-06-06 02:23:09,537 - Epoch: [144][  136/  136]    Loss 0.498013    Top1 75.387636    Top5 95.345480    
2024-06-06 02:23:09,720 - ==> Top1: 75.388    Top5: 95.345    Loss: 0.498

2024-06-06 02:23:09,721 - ==> Confusion:
[[ 811    1    4    0   13    4    0    5    5   59    1    3    3    2    9    2    3    1    2    0    3]
 [   6  933    2    0   20   20    3   26    2    3    2    3    3    1    5    2    3    4   10    5   10]
 [  10    3  810   14    4    1   39   14    0    9    5    8    5    5    4    8    6    0    7    3   15]
 [   5    0   13  872    3   11    1    5    5    1   17    2   12    2   31    1    1    9   16    0    9]
 [  30   17    5    0  940   11    1    4    0   13    2    3    3    2    8    4    6    0    0    2    3]
 [   4   34    3    3   14  840    6   40    4   11    2   15   12   20    3    2    8    2    3    8    9]
 [   1    5   19    5    0    8  991   11    0    2    5    7    1    1    0    2    7    5    5    9    2]
 [   5   17   10    2    1   36    7  913    5    2    3   16   11    1    0    0    3    4   28    9    4]
 [  14    6    0    1    1   10    0    3  836   53   10    2    5   15   20    0    4    1   10    3    8]
 [  96    5    2    0    7    1    0    1   42  807    2    1    2   14    4    2    2    0    2    2    9]
 [   2    2    9   15    1    3    3    7   30    3  939    0    1   16    4    0    4    2   16    1    6]
 [   5    2    1    0    1   15    1   12    1    1    0  871   25   14    0   12    7   22    5   12    4]
 [   4    1    5    7    0    6    4    5    5    3    2   76  792    6    1    4    1   44   11    7   11]
 [   6    3    2    1    3   17    0    7   20   30   13   14    9  856    3    0    3    5    1    4    4]
 [   9    3    1   19   13    4    0    3   36   15    9    3    4    7  941    1    1    4   17    2    6]
 [   4    1    2    2    4    2    4    1    0    2    0   20   12    3    0  958   17   22    0    3    9]
 [   4   17    3    1   14    8    4    4    7    0    1   10    5    3    2   14  950    2    4    3   16]
 [   2    0    1    3    0    4    0    3    3    3    2   22   30    4    3    8    1  907    4    2    3]
 [   5   11    8   18    2    4    0   22    9    2    3    4    8    2   20    0    1    1  930    0    8]
 [   1    5    3    1    3   16   13   12    1    0    2   30    7    9    3    4    9    4    5  951    9]
 [ 359  307  237  195  330  274  161  292  202  179  284  262  458  379  305  198  420  198  263  368 8261]]

2024-06-06 02:23:09,723 - ==> Best [Top1: 77.132   Top5: 95.897   Sparsity:0.00   Params: 169472 on epoch: 127]
2024-06-06 02:23:09,723 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:23:09,731 - 

2024-06-06 02:23:09,731 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:23:15,864 - Epoch: [145][  100/ 1218]    Overall Loss 0.541514    Objective Loss 0.541514                                        LR 0.000250    Time 0.061306    
2024-06-06 02:23:20,398 - Epoch: [145][  200/ 1218]    Overall Loss 0.539766    Objective Loss 0.539766                                        LR 0.000250    Time 0.053313    
2024-06-06 02:23:24,989 - Epoch: [145][  300/ 1218]    Overall Loss 0.538567    Objective Loss 0.538567                                        LR 0.000250    Time 0.050840    
2024-06-06 02:23:29,602 - Epoch: [145][  400/ 1218]    Overall Loss 0.536564    Objective Loss 0.536564                                        LR 0.000250    Time 0.049658    
2024-06-06 02:23:34,133 - Epoch: [145][  500/ 1218]    Overall Loss 0.536178    Objective Loss 0.536178                                        LR 0.000250    Time 0.048786    
2024-06-06 02:23:38,800 - Epoch: [145][  600/ 1218]    Overall Loss 0.534601    Objective Loss 0.534601                                        LR 0.000250    Time 0.048429    
2024-06-06 02:23:43,321 - Epoch: [145][  700/ 1218]    Overall Loss 0.535052    Objective Loss 0.535052                                        LR 0.000250    Time 0.047967    
2024-06-06 02:23:47,844 - Epoch: [145][  800/ 1218]    Overall Loss 0.533259    Objective Loss 0.533259                                        LR 0.000250    Time 0.047623    
2024-06-06 02:23:52,421 - Epoch: [145][  900/ 1218]    Overall Loss 0.532822    Objective Loss 0.532822                                        LR 0.000250    Time 0.047415    
2024-06-06 02:23:57,191 - Epoch: [145][ 1000/ 1218]    Overall Loss 0.533266    Objective Loss 0.533266                                        LR 0.000250    Time 0.047442    
2024-06-06 02:24:01,838 - Epoch: [145][ 1100/ 1218]    Overall Loss 0.532635    Objective Loss 0.532635                                        LR 0.000250    Time 0.047352    
2024-06-06 02:24:06,576 - Epoch: [145][ 1200/ 1218]    Overall Loss 0.531941    Objective Loss 0.531941                                        LR 0.000250    Time 0.047353    
2024-06-06 02:24:07,355 - Epoch: [145][ 1218/ 1218]    Overall Loss 0.532191    Objective Loss 0.532191    Top1 79.462103    Top5 95.599022    LR 0.000250    Time 0.047291    
2024-06-06 02:24:07,521 - --- validate (epoch=145)-----------
2024-06-06 02:24:07,522 - 34633 samples (256 per mini-batch)
2024-06-06 02:24:13,218 - Epoch: [145][  100/  136]    Loss 0.482985    Top1 76.960938    Top5 95.875000    
2024-06-06 02:24:14,919 - Epoch: [145][  136/  136]    Loss 0.478993    Top1 77.019028    Top5 95.925851    
2024-06-06 02:24:15,107 - ==> Top1: 77.019    Top5: 95.926    Loss: 0.479

2024-06-06 02:24:15,108 - ==> Confusion:
[[ 767    0    5    1   16    3    0    2   11   85    1    4    2    7    3    2    7    1    3    0   11]
 [   5  920    2    3   24   24    3   19    2    1    6    3    2    2    4    2    9    2   11    5   14]
 [   6    2  850   11    7    4   18    9    1    7   13    2    7    5    1    3    5    1    4    2   12]
 [   0    3   19  880    6    9    3    2    0    3   13    1    6    4   32    1    2   11   13    1    7]
 [  19   12    5    1  942    5    0    1    4   13    3    3    1    4    7    4   11    0    6    0   13]
 [   3   28    1    7   18  874    1   22    3    5    3   13    4   20    3    3    7    2    9    6   11]
 [   1    7   42    3    5    4  963    5    2    1    5    2    3    0    1   10    6    1    5    9   11]
 [   6   19   16    0    2   52    4  871    1    2    7   14    5    3    6    0    2    4   37   15   11]
 [  10    4    1    1    4    5    1    1  822   58   17    3    5   22   24    0    4    2    9    2    7]
 [  67    0    3    0    6    3    0    2   50  821    2    1    4   17   15    1    2    1    1    1    4]
 [   0    3   12   15    4    2    2    2   15    0  951    1    4   10   16    2    1    1   16    1    6]
 [   3    3    1    0    1   19    2    9    0    1    1  860   36   14    1   13    5   17    4   13    8]
 [   0    1    5    9    1    5    2    1    0    0    3   73  830    6    2   11    5   22    7    4    8]
 [   2    1    0    1    7    8    0    3   24   23    5   11    6  881    6    1    2    3    5    5    7]
 [   8    1    1   13   12    1    0    1   31    7    4    3    5    7  979    0    1    6   11    0    7]
 [   3    2    2    1    4    7    3    0    0    1    1   19   10    2    1  975   11   17    1    2    4]
 [   2    9    7    0    8    9    1    0    7    1    1    8    4    6    2    8  969    0    3    7   20]
 [   0    2    1    5    3    1    1    1    0    2    0   18   34    2    3   10    2  907    3    2    8]
 [   0    8   10   12    6    6    2   17    7    2    9    5    7    1   27    1    1    2  932    0    3]
 [   3    4    7    2    5   14   18   18    0    0    2   37    7   10    2    7    9    2    4  927   10]
 [ 232  257  305  153  402  245  111  217  147  156  229  220  454  425  321  180  365  133  260  367 8753]]

2024-06-06 02:24:15,110 - ==> Best [Top1: 77.132   Top5: 95.897   Sparsity:0.00   Params: 169472 on epoch: 127]
2024-06-06 02:24:15,110 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:24:15,117 - 

2024-06-06 02:24:15,117 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:24:21,522 - Epoch: [146][  100/ 1218]    Overall Loss 0.523108    Objective Loss 0.523108                                        LR 0.000250    Time 0.064019    
2024-06-06 02:24:26,226 - Epoch: [146][  200/ 1218]    Overall Loss 0.521996    Objective Loss 0.521996                                        LR 0.000250    Time 0.055523    
2024-06-06 02:24:31,003 - Epoch: [146][  300/ 1218]    Overall Loss 0.527122    Objective Loss 0.527122                                        LR 0.000250    Time 0.052930    
2024-06-06 02:24:35,794 - Epoch: [146][  400/ 1218]    Overall Loss 0.529072    Objective Loss 0.529072                                        LR 0.000250    Time 0.051672    
2024-06-06 02:24:40,449 - Epoch: [146][  500/ 1218]    Overall Loss 0.528899    Objective Loss 0.528899                                        LR 0.000250    Time 0.050643    
2024-06-06 02:24:45,165 - Epoch: [146][  600/ 1218]    Overall Loss 0.528361    Objective Loss 0.528361                                        LR 0.000250    Time 0.050060    
2024-06-06 02:24:49,956 - Epoch: [146][  700/ 1218]    Overall Loss 0.528944    Objective Loss 0.528944                                        LR 0.000250    Time 0.049750    
2024-06-06 02:24:54,849 - Epoch: [146][  800/ 1218]    Overall Loss 0.527416    Objective Loss 0.527416                                        LR 0.000250    Time 0.049646    
2024-06-06 02:24:59,425 - Epoch: [146][  900/ 1218]    Overall Loss 0.526947    Objective Loss 0.526947                                        LR 0.000250    Time 0.049212    
2024-06-06 02:25:04,037 - Epoch: [146][ 1000/ 1218]    Overall Loss 0.528971    Objective Loss 0.528971                                        LR 0.000250    Time 0.048901    
2024-06-06 02:25:08,738 - Epoch: [146][ 1100/ 1218]    Overall Loss 0.527392    Objective Loss 0.527392                                        LR 0.000250    Time 0.048727    
2024-06-06 02:25:13,276 - Epoch: [146][ 1200/ 1218]    Overall Loss 0.527135    Objective Loss 0.527135                                        LR 0.000250    Time 0.048446    
2024-06-06 02:25:14,077 - Epoch: [146][ 1218/ 1218]    Overall Loss 0.526883    Objective Loss 0.526883    Top1 73.838631    Top5 94.621027    LR 0.000250    Time 0.048388    
2024-06-06 02:25:14,258 - --- validate (epoch=146)-----------
2024-06-06 02:25:14,258 - 34633 samples (256 per mini-batch)
2024-06-06 02:25:19,767 - Epoch: [146][  100/  136]    Loss 0.492720    Top1 75.992188    Top5 95.562500    
2024-06-06 02:25:21,400 - Epoch: [146][  136/  136]    Loss 0.490871    Top1 76.005544    Top5 95.550487    
2024-06-06 02:25:21,577 - ==> Top1: 76.006    Top5: 95.550    Loss: 0.491

2024-06-06 02:25:21,578 - ==> Confusion:
[[ 794    2    0    1   13    2    3    4    8   62    0    2    4    4   10    1    3    6    1    0   11]
 [   2  919    4    2   26   18    4   21    9    0    3    5    4    0    5    3   17    3   10    4    4]
 [   7    0  844   12    6    3   26    8    3    5    5    6    4    7    4    1    6    4    7    4    8]
 [   3    5   14  873    4    6    4    4    1    3   20    0   10    6   30    2    5    8    9    1    8]
 [  30    8    4    1  938    7    2    3    2   15    0    4    3    3   12    6    4    0    4    2    6]
 [   7   31    3    4   23  833    5   38    4    7    1   13    8   15    8    4    6    0    1   20   12]
 [   2    3   23    1    3    6  990   11    3    1    3    3    4    0    2   13    3    4    2    7    2]
 [   4   17   14    4    3   33    6  899    5    2    3   11   10    4    2    2    3    3   31   13    8]
 [  15    3    0    0    2    4    0    1  850   49    8    2    6   16   20    0    4    4    9    1    8]
 [ 114    2    3    1    8    4    3    4   50  761    1    1    1   21    7    1    5    4    2    0    8]
 [   1    3    7    7    3    4    8    3   28    3  934    0    1   17   10    0    3    0   16    3   13]
 [   1    0    5    0    1   14    3    6    1    2    0  874   26    4    0   18    8   20    1   22    5]
 [   0    1    4    8    0    7    1    1    1    0    3   71  831    1    1    7    5   26   14    7    6]
 [   2    1    1    0    5    9    0    2   17   31   11   14   10  854    8    2    9    9    1    5   10]
 [  16    2    1   16   14    2    2    0   37    8    7    4    6    4  950    0    1    3   14    0   11]
 [   2    0    2    4    3    3   10    1    1    1    0   21   12    1    0  961   19   14    2    3    6]
 [   1    7    3    2   10    5    5    0    7    1    4    9    5    4    4   11  971    4    3    8    8]
 [   1    1    1    2    1    2    2    4    1    2    0   32   30    1    6   14    1  896    2    3    3]
 [   1    4    7   17    7    5    1   19   10    1    4    3    8    0   27    1    2    2  935    2    2]
 [   2    6    1    2    1    5   24   11    2    1    0   39   10    5    0    9    5    3    3  951    8]
 [ 261  282  299  158  316  216  156  234  192  139  214  278  508  326  344  202  460  197  275  410 8465]]

2024-06-06 02:25:21,579 - ==> Best [Top1: 77.132   Top5: 95.897   Sparsity:0.00   Params: 169472 on epoch: 127]
2024-06-06 02:25:21,580 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:25:21,587 - 

2024-06-06 02:25:21,587 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:25:27,781 - Epoch: [147][  100/ 1218]    Overall Loss 0.531572    Objective Loss 0.531572                                        LR 0.000250    Time 0.061917    
2024-06-06 02:25:32,433 - Epoch: [147][  200/ 1218]    Overall Loss 0.526037    Objective Loss 0.526037                                        LR 0.000250    Time 0.054208    
2024-06-06 02:25:37,142 - Epoch: [147][  300/ 1218]    Overall Loss 0.522195    Objective Loss 0.522195                                        LR 0.000250    Time 0.051831    
2024-06-06 02:25:41,727 - Epoch: [147][  400/ 1218]    Overall Loss 0.523117    Objective Loss 0.523117                                        LR 0.000250    Time 0.050331    
2024-06-06 02:25:46,263 - Epoch: [147][  500/ 1218]    Overall Loss 0.525037    Objective Loss 0.525037                                        LR 0.000250    Time 0.049333    
2024-06-06 02:25:51,274 - Epoch: [147][  600/ 1218]    Overall Loss 0.524246    Objective Loss 0.524246                                        LR 0.000250    Time 0.049461    
2024-06-06 02:25:56,252 - Epoch: [147][  700/ 1218]    Overall Loss 0.524179    Objective Loss 0.524179                                        LR 0.000250    Time 0.049497    
2024-06-06 02:26:01,217 - Epoch: [147][  800/ 1218]    Overall Loss 0.522874    Objective Loss 0.522874                                        LR 0.000250    Time 0.049513    
2024-06-06 02:26:06,107 - Epoch: [147][  900/ 1218]    Overall Loss 0.523333    Objective Loss 0.523333                                        LR 0.000250    Time 0.049444    
2024-06-06 02:26:10,693 - Epoch: [147][ 1000/ 1218]    Overall Loss 0.523045    Objective Loss 0.523045                                        LR 0.000250    Time 0.049084    
2024-06-06 02:26:15,420 - Epoch: [147][ 1100/ 1218]    Overall Loss 0.522497    Objective Loss 0.522497                                        LR 0.000250    Time 0.048917    
2024-06-06 02:26:20,141 - Epoch: [147][ 1200/ 1218]    Overall Loss 0.522528    Objective Loss 0.522528                                        LR 0.000250    Time 0.048773    
2024-06-06 02:26:20,956 - Epoch: [147][ 1218/ 1218]    Overall Loss 0.522798    Objective Loss 0.522798    Top1 77.750611    Top5 96.332518    LR 0.000250    Time 0.048721    
2024-06-06 02:26:21,124 - --- validate (epoch=147)-----------
2024-06-06 02:26:21,125 - 34633 samples (256 per mini-batch)
2024-06-06 02:26:26,828 - Epoch: [147][  100/  136]    Loss 0.487648    Top1 76.589844    Top5 95.683594    
2024-06-06 02:26:28,506 - Epoch: [147][  136/  136]    Loss 0.482812    Top1 76.594577    Top5 95.686195    
2024-06-06 02:26:28,683 - ==> Top1: 76.595    Top5: 95.686    Loss: 0.483

2024-06-06 02:26:28,685 - ==> Confusion:
[[ 800    1    3    1    5    1    1    1    9   70    2    3    1    9    6    4    1    1    6    0    6]
 [   0  917    4    2   19   26    5   18    5    2    1    5    4    1    5    1    9    1   23    4   11]
 [   8    7  860   11    5    1   16    9    1    2    3    5    2    3    3    2    6    1   13    2   10]
 [   7    3   18  869    1    5    3    3    1    1   15    0    6    3   31    1    3   12   21    2   11]
 [  29   18    2    2  918   10    1    1    2   17    1    1    4    1   12    3    9    2    8    1   12]
 [   3   31    3    7   15  847    3   43    5    7    0   14    7   20    4    2    6    3    7    8    8]
 [   1    5   33    8    3    5  973   10    3    0    3    4    2    1    1    7    4    6    3    5    9]
 [   5   14    5    1    1   28   11  915    2    2    5    7    3    3    4    0    0    2   45   17    7]
 [   8    5    0    0    2    1    0    1  838   57    9    2    4   13   34    1    3    8   10    2    4]
 [  80    0    1    3    7    2    2    3   71  794    2    0    1   11   10    3    1    2    3    2    3]
 [   3    7   12   11    1    6    4    8   15    1  938    2    1   12   15    0    0    0   21    4    3]
 [   1    0    2    0    0   11    2    7    0    1    0  872   35   11    0   11    5   31    4   11    7]
 [   0    1    4    7    1    7    1    5    5    0    3   61  802    3    2    6    1   62   12    6    6]
 [   2    1    3    1    2   15    1    5   27   30    8   16    4  845    6    4    3    7    2   11    8]
 [  10    3    1   18    8    1    3    2   22    8    2    1    1    7  970    1    2    8   23    3    4]
 [   1    2    3    0    3    1    5    0    0    0    0   23   16    2    1  961   10   26    5    2    5]
 [   3    8    5    2    3   11    2    2    6    3    2    9    6    6    2   18  953    5    4    5   17]
 [   1    0    3    2    0    2    1    1    2    3    1    8   21    1    4    6    2  932    6    4    5]
 [   0    6    7   15    3    7    0   21    6    4    4    3    4    1   19    0    0    1  949    1    7]
 [   3    5    8    1    0   10   16   12    0    0    1   31   12    7    1    4   11    7   12  938    9]
 [ 274  310  322  182  262  177  124  274  167  160  214  248  437  320  358  189  339  237  343  359 8636]]

2024-06-06 02:26:28,686 - ==> Best [Top1: 77.132   Top5: 95.897   Sparsity:0.00   Params: 169472 on epoch: 127]
2024-06-06 02:26:28,686 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:26:28,694 - 

2024-06-06 02:26:28,694 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:26:34,691 - Epoch: [148][  100/ 1218]    Overall Loss 0.518337    Objective Loss 0.518337                                        LR 0.000250    Time 0.059949    
2024-06-06 02:26:39,360 - Epoch: [148][  200/ 1218]    Overall Loss 0.517576    Objective Loss 0.517576                                        LR 0.000250    Time 0.053313    
2024-06-06 02:26:44,076 - Epoch: [148][  300/ 1218]    Overall Loss 0.518153    Objective Loss 0.518153                                        LR 0.000250    Time 0.051255    
2024-06-06 02:26:48,782 - Epoch: [148][  400/ 1218]    Overall Loss 0.519333    Objective Loss 0.519333                                        LR 0.000250    Time 0.050201    
2024-06-06 02:26:53,534 - Epoch: [148][  500/ 1218]    Overall Loss 0.521864    Objective Loss 0.521864                                        LR 0.000250    Time 0.049661    
2024-06-06 02:26:58,220 - Epoch: [148][  600/ 1218]    Overall Loss 0.521107    Objective Loss 0.521107                                        LR 0.000250    Time 0.049191    
2024-06-06 02:27:02,911 - Epoch: [148][  700/ 1218]    Overall Loss 0.521443    Objective Loss 0.521443                                        LR 0.000250    Time 0.048863    
2024-06-06 02:27:07,865 - Epoch: [148][  800/ 1218]    Overall Loss 0.522665    Objective Loss 0.522665                                        LR 0.000250    Time 0.048945    
2024-06-06 02:27:12,511 - Epoch: [148][  900/ 1218]    Overall Loss 0.523080    Objective Loss 0.523080                                        LR 0.000250    Time 0.048667    
2024-06-06 02:27:17,208 - Epoch: [148][ 1000/ 1218]    Overall Loss 0.522478    Objective Loss 0.522478                                        LR 0.000250    Time 0.048496    
2024-06-06 02:27:21,836 - Epoch: [148][ 1100/ 1218]    Overall Loss 0.522797    Objective Loss 0.522797                                        LR 0.000250    Time 0.048293    
2024-06-06 02:27:26,358 - Epoch: [148][ 1200/ 1218]    Overall Loss 0.523094    Objective Loss 0.523094                                        LR 0.000250    Time 0.048035    
2024-06-06 02:27:27,203 - Epoch: [148][ 1218/ 1218]    Overall Loss 0.523370    Objective Loss 0.523370    Top1 77.261614    Top5 96.821516    LR 0.000250    Time 0.048016    
2024-06-06 02:27:27,391 - --- validate (epoch=148)-----------
2024-06-06 02:27:27,391 - 34633 samples (256 per mini-batch)
2024-06-06 02:27:32,951 - Epoch: [148][  100/  136]    Loss 0.486390    Top1 76.234375    Top5 95.828125    
2024-06-06 02:27:34,643 - Epoch: [148][  136/  136]    Loss 0.483299    Top1 76.097941    Top5 95.767043    
2024-06-06 02:27:34,822 - ==> Top1: 76.098    Top5: 95.767    Loss: 0.483

2024-06-06 02:27:34,823 - ==> Confusion:
[[ 804    1    2    2    8    1    0    2   11   68    2    2    1    7    5    1    3    1    3    1    6]
 [   4  934    3    3   24   33    4   12    9    0    2    2    4    1    4    1    8    0   13    1    1]
 [   8    4  836   10    6    2   20    8    1    4   15    4    7    6    3    6    7    3    6    4   10]
 [   6    5   11  882    3   10    3    2    0    4   14    1    4    4   33    3    2    5   15    2    7]
 [  23    8    2    0  922   12    1    2    4   12    0    2    4    9   27    5    6    2    4    0    9]
 [   1   30    4    5   15  832    4   37    6    6    1   11   12   24    5    2   11    4    6   11   16]
 [   2    5   28    3    3    5  995    8    2    2    5    1    1    0    0    7    3    1    3    9    3]
 [   4   17   12    5    4   50    9  887    3    1    6   12    6    1    1    1    0    3   31   16    8]
 [   9    6    2    2    1    1    0    3  862   33   15    4    3   13   31    0    1    3    8    1    4]
 [  88    1    1    0    8    5    1    1   66  773    1    1    0   28   14    3    1    2    3    1    3]
 [   2    3    6   16    2    6    6    6   17    2  937    0    2   13   22    0    2    0   16    2    4]
 [   1    3    3    0    2   12    8    2    0    1    1  865   28   13    2   16    7   25    1   18    3]
 [   2    0    4    8    2    5    1    2    1    0    1   50  812    5    3   10    6   57    7    9   10]
 [   8    2    1    0    7   12    1    3   24   22   13    9    6  848    5    6    5    8    0   11   10]
 [   9    4    2   12    5    3    0    1   25    2    3    3    5    9  976    1    2    8   18    1    9]
 [   1    2    3    0    6    2    3    1    0    0    0   13   12    2    2  977   11   18    1    3    9]
 [   3    8    5    3   10    8    2    0    5    1    2    5    8    5    3   13  964    7    1    4   15]
 [   1    0    0    3    1    0    1    2    1    2    0   26   22    0    5   10    1  918    3    3    6]
 [   1    8    6   18    1    2    1   24    8    0    2    1    4    1   30    0    0    3  942    2    4]
 [   3   10    2    3    3   14   24   11    1    0    1   16   11    3    3    8   11    5    1  947   11]
 [ 274  340  288  185  266  234  162  209  179  132  255  198  465  386  381  225  428  197  260  426 8442]]

2024-06-06 02:27:34,825 - ==> Best [Top1: 77.132   Top5: 95.897   Sparsity:0.00   Params: 169472 on epoch: 127]
2024-06-06 02:27:34,825 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:27:34,833 - 

2024-06-06 02:27:34,833 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:27:40,781 - Epoch: [149][  100/ 1218]    Overall Loss 0.521901    Objective Loss 0.521901                                        LR 0.000250    Time 0.059454    
2024-06-06 02:27:45,330 - Epoch: [149][  200/ 1218]    Overall Loss 0.518500    Objective Loss 0.518500                                        LR 0.000250    Time 0.052467    
2024-06-06 02:27:50,067 - Epoch: [149][  300/ 1218]    Overall Loss 0.518392    Objective Loss 0.518392                                        LR 0.000250    Time 0.050763    
2024-06-06 02:27:54,899 - Epoch: [149][  400/ 1218]    Overall Loss 0.519886    Objective Loss 0.519886                                        LR 0.000250    Time 0.050147    
2024-06-06 02:27:59,704 - Epoch: [149][  500/ 1218]    Overall Loss 0.522506    Objective Loss 0.522506                                        LR 0.000250    Time 0.049723    
2024-06-06 02:28:04,524 - Epoch: [149][  600/ 1218]    Overall Loss 0.521524    Objective Loss 0.521524                                        LR 0.000250    Time 0.049466    
2024-06-06 02:28:09,329 - Epoch: [149][  700/ 1218]    Overall Loss 0.521922    Objective Loss 0.521922                                        LR 0.000250    Time 0.049262    
2024-06-06 02:28:14,159 - Epoch: [149][  800/ 1218]    Overall Loss 0.523548    Objective Loss 0.523548                                        LR 0.000250    Time 0.049139    
2024-06-06 02:28:19,097 - Epoch: [149][  900/ 1218]    Overall Loss 0.525125    Objective Loss 0.525125                                        LR 0.000250    Time 0.049164    
2024-06-06 02:28:23,790 - Epoch: [149][ 1000/ 1218]    Overall Loss 0.524160    Objective Loss 0.524160                                        LR 0.000250    Time 0.048939    
2024-06-06 02:28:28,380 - Epoch: [149][ 1100/ 1218]    Overall Loss 0.524392    Objective Loss 0.524392                                        LR 0.000250    Time 0.048661    
2024-06-06 02:28:32,965 - Epoch: [149][ 1200/ 1218]    Overall Loss 0.524347    Objective Loss 0.524347                                        LR 0.000250    Time 0.048425    
2024-06-06 02:28:33,821 - Epoch: [149][ 1218/ 1218]    Overall Loss 0.524609    Objective Loss 0.524609    Top1 76.528117    Top5 95.599022    LR 0.000250    Time 0.048412    
2024-06-06 02:28:34,016 - --- validate (epoch=149)-----------
2024-06-06 02:28:34,016 - 34633 samples (256 per mini-batch)
2024-06-06 02:28:39,540 - Epoch: [149][  100/  136]    Loss 0.473251    Top1 77.476562    Top5 96.183594    
2024-06-06 02:28:41,248 - Epoch: [149][  136/  136]    Loss 0.478847    Top1 77.374181    Top5 96.061560    
2024-06-06 02:28:41,445 - ==> Top1: 77.374    Top5: 96.062    Loss: 0.479

2024-06-06 02:28:41,446 - ==> Confusion:
[[ 777    2    1    2    7    1    1    2   15   88    0    1    0    4    9    2    3    3    6    0    7]
 [   2  940    5    0   21   20    3    7    5    1    5    2    2    1    7    0   16    0   12    5    9]
 [   5    4  849   10    6    1   19   10    0    2   10    6    4    8    3    5    6    1    9    5    7]
 [   1    5   19  862    3    5    2    1    3    2   21    2   14    2   40    3    3    9   10    2    7]
 [  17   14    5    1  937    4    2    1    6   15    0    4    3    7   10    4   11    1    2    1    9]
 [   4   57    3    5   24  812    0   22    5    7    3   17   10   30    4    4   10    1    4    5   16]
 [   3    6   40    3    7    1  961    5    1    0    5    4    4    4    1   13    4    2    4   14    4]
 [   3   30   16    1    3   47    8  852    2    0    9   16    4    4    2    0    2    3   52   13   10]
 [  11    3    1    1    1    1    0    2  829   43   19    5    3   23   40    0    2    3    9    1    5]
 [  66    4    4    1    5    2    1    1   58  800    2    0    2   27   11    1    4    2    3    0    7]
 [   1    5   10    9    1    1    5    5   11    2  954    1    2   13   13    1    3    0   17    0   10]
 [   3    4    0    0    0   13    1    3    2    0    1  852   38   22    2   22    5   19    3   14    7]
 [   0    2    5    6    1    2    0    0    1    0    0   73  831    4    3    9    5   29   10    1   13]
 [   3    3    2    1    3    6    2    3   16   16   10    7    6  883    9    1    4    6    3    7   10]
 [   6    1    1   15   10    1    0    1   33    9    4    1    9    7  979    0    0    3   10    1    7]
 [   1    2    1    3    6    0    5    1    1    3    0   13   12    5    2  979   15    8    2    1    6]
 [   7   10    9    0   11    3    1    1    6    0    0    7    3    5    4   12  971    1    3    3   15]
 [   0    3    2    1    1    0    1    1    0    2    0   24   29    9    4   13    2  901    3    2    7]
 [   2    7   14   15    2    1    0   12    3    0    4    1    6    1   31    0    0    0  948    3    8]
 [   3   11    8    3    2   11   10    9    3    0    2   26   17    6    0    8   15    8    8  926   12]
 [ 246  313  308  149  268  147  116  178  183  147  222  220  416  367  359  211  372  167  288  301 8954]]

2024-06-06 02:28:41,447 - ==> Best [Top1: 77.374   Top5: 96.062   Sparsity:0.00   Params: 169472 on epoch: 149]
2024-06-06 02:28:41,447 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:28:41,457 - 

2024-06-06 02:28:41,457 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:28:47,393 - Epoch: [150][  100/ 1218]    Overall Loss 0.522173    Objective Loss 0.522173                                        LR 0.000250    Time 0.059339    
2024-06-06 02:28:52,176 - Epoch: [150][  200/ 1218]    Overall Loss 0.519112    Objective Loss 0.519112                                        LR 0.000250    Time 0.053572    
2024-06-06 02:28:56,815 - Epoch: [150][  300/ 1218]    Overall Loss 0.518050    Objective Loss 0.518050                                        LR 0.000250    Time 0.051174    
2024-06-06 02:29:01,454 - Epoch: [150][  400/ 1218]    Overall Loss 0.517725    Objective Loss 0.517725                                        LR 0.000250    Time 0.049974    
2024-06-06 02:29:06,160 - Epoch: [150][  500/ 1218]    Overall Loss 0.517461    Objective Loss 0.517461                                        LR 0.000250    Time 0.049387    
2024-06-06 02:29:10,803 - Epoch: [150][  600/ 1218]    Overall Loss 0.517274    Objective Loss 0.517274                                        LR 0.000250    Time 0.048891    
2024-06-06 02:29:15,420 - Epoch: [150][  700/ 1218]    Overall Loss 0.519037    Objective Loss 0.519037                                        LR 0.000250    Time 0.048500    
2024-06-06 02:29:20,363 - Epoch: [150][  800/ 1218]    Overall Loss 0.519791    Objective Loss 0.519791                                        LR 0.000250    Time 0.048614    
2024-06-06 02:29:25,302 - Epoch: [150][  900/ 1218]    Overall Loss 0.520383    Objective Loss 0.520383                                        LR 0.000250    Time 0.048698    
2024-06-06 02:29:30,138 - Epoch: [150][ 1000/ 1218]    Overall Loss 0.521133    Objective Loss 0.521133                                        LR 0.000250    Time 0.048662    
2024-06-06 02:29:34,765 - Epoch: [150][ 1100/ 1218]    Overall Loss 0.520843    Objective Loss 0.520843                                        LR 0.000250    Time 0.048444    
2024-06-06 02:29:39,453 - Epoch: [150][ 1200/ 1218]    Overall Loss 0.520996    Objective Loss 0.520996                                        LR 0.000250    Time 0.048312    
2024-06-06 02:29:40,280 - Epoch: [150][ 1218/ 1218]    Overall Loss 0.521159    Objective Loss 0.521159    Top1 75.794621    Top5 94.621027    LR 0.000250    Time 0.048276    
2024-06-06 02:29:40,449 - --- validate (epoch=150)-----------
2024-06-06 02:29:40,449 - 34633 samples (256 per mini-batch)
2024-06-06 02:29:46,070 - Epoch: [150][  100/  136]    Loss 0.482054    Top1 77.304688    Top5 95.812500    
2024-06-06 02:29:47,692 - Epoch: [150][  136/  136]    Loss 0.481869    Top1 77.183611    Top5 95.868103    
2024-06-06 02:29:47,876 - ==> Top1: 77.184    Top5: 95.868    Loss: 0.482

2024-06-06 02:29:47,877 - ==> Confusion:
[[ 775    3    1    0    8    2    0    1    8  100    0    5    2    3    5    0    2    5    5    0    6]
 [   3  943    5    1   19   14    4   13   10    2    2    2    2    0   10    1    8    3   10    1   10]
 [   6    4  834    6    5    2   27   17    0   10    3    6    3    7    3    6    4    2    6    3   16]
 [   5    2   16  856    6    5    3    2    6    2   18    1    8    5   31    5    4    9   21    0   11]
 [  31   12    3    2  912   12    1    4    2   23    1    5    2    5    8    2    6    0    9    4   10]
 [   8   28    0    4   12  845    3   44    4    6    2   15    7   28    2    4    3    1    9    8   10]
 [   2   11   18    3    1    4  992    8    1    0    5    5    2    2    0   10    6    4    3    5    4]
 [   3   13    8    3    3   36    7  896    8    2    6   16    6    4    2    3    1    4   34   12   10]
 [  13    4    1    1    0    3    1    4  842   63    8    4    4   14   20    0    2    3   10    0    5]
 [  52    3    2    0    3    1    2    2   46  843    1    1    1   23    7    2    1    2    2    1    6]
 [   1    7    4   13    5    5    7    4   23    5  929    2    2   13   14    0    1    0   19    2    8]
 [   1    4    3    0    2   20    5    4    3    2    0  857   33   10    0   22    5   15    2   17    6]
 [   1    2    4    3    3    5    1    2    3    0    1   80  816    3    1    8    2   36    6    5   13]
 [   2    0    3    1    4   10    0    0   22   28   11   14    2  874    3    5    3    5    1    4    9]
 [   7    7    1   13   10    0    0    1   41   16    2    4    3   12  955    1    0    5   16    0    4]
 [   2    1    1    0    1    4    9    0    0    2    1   25   12    2    1  971    9   12    2    1   10]
 [   7   12    5    2    4   10    0    0    7    3    1   13    3    3    2   16  952    4    3    7   18]
 [   1    1    1    3    1    1    2    1    3    5    0   21   33    4    4   14    2  894    5    1    8]
 [   2    9    5   19    4    0    3   20    9    2    3    3    3    3   15    0    1    1  950    3    3]
 [   2   17    1    1    3    6   14   16    2    1    2   24   13    3    0    9    5    4    5  954    6]
 [ 253  312  241  126  281  219  176  239  164  187  186  257  412  379  296  216  312  159  340  336 8841]]

2024-06-06 02:29:47,879 - ==> Best [Top1: 77.374   Top5: 96.062   Sparsity:0.00   Params: 169472 on epoch: 149]
2024-06-06 02:29:47,879 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:29:47,887 - 

2024-06-06 02:29:47,887 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:29:54,229 - Epoch: [151][  100/ 1218]    Overall Loss 0.511309    Objective Loss 0.511309                                        LR 0.000250    Time 0.063398    
2024-06-06 02:29:59,106 - Epoch: [151][  200/ 1218]    Overall Loss 0.515728    Objective Loss 0.515728                                        LR 0.000250    Time 0.056077    
2024-06-06 02:30:03,846 - Epoch: [151][  300/ 1218]    Overall Loss 0.517690    Objective Loss 0.517690                                        LR 0.000250    Time 0.053180    
2024-06-06 02:30:08,681 - Epoch: [151][  400/ 1218]    Overall Loss 0.517062    Objective Loss 0.517062                                        LR 0.000250    Time 0.051967    
2024-06-06 02:30:13,306 - Epoch: [151][  500/ 1218]    Overall Loss 0.518838    Objective Loss 0.518838                                        LR 0.000250    Time 0.050820    
2024-06-06 02:30:17,996 - Epoch: [151][  600/ 1218]    Overall Loss 0.519700    Objective Loss 0.519700                                        LR 0.000250    Time 0.050164    
2024-06-06 02:30:22,973 - Epoch: [151][  700/ 1218]    Overall Loss 0.520497    Objective Loss 0.520497                                        LR 0.000250    Time 0.050105    
2024-06-06 02:30:27,650 - Epoch: [151][  800/ 1218]    Overall Loss 0.519878    Objective Loss 0.519878                                        LR 0.000250    Time 0.049685    
2024-06-06 02:30:32,481 - Epoch: [151][  900/ 1218]    Overall Loss 0.520992    Objective Loss 0.520992                                        LR 0.000250    Time 0.049532    
2024-06-06 02:30:37,196 - Epoch: [151][ 1000/ 1218]    Overall Loss 0.522009    Objective Loss 0.522009                                        LR 0.000250    Time 0.049291    
2024-06-06 02:30:41,895 - Epoch: [151][ 1100/ 1218]    Overall Loss 0.521755    Objective Loss 0.521755                                        LR 0.000250    Time 0.049078    
2024-06-06 02:30:46,566 - Epoch: [151][ 1200/ 1218]    Overall Loss 0.521519    Objective Loss 0.521519                                        LR 0.000250    Time 0.048880    
2024-06-06 02:30:47,409 - Epoch: [151][ 1218/ 1218]    Overall Loss 0.521961    Objective Loss 0.521961    Top1 75.794621    Top5 94.865526    LR 0.000250    Time 0.048849    
2024-06-06 02:30:47,592 - --- validate (epoch=151)-----------
2024-06-06 02:30:47,592 - 34633 samples (256 per mini-batch)
2024-06-06 02:30:53,039 - Epoch: [151][  100/  136]    Loss 0.468387    Top1 77.253906    Top5 95.937500    
2024-06-06 02:30:54,667 - Epoch: [151][  136/  136]    Loss 0.474055    Top1 77.091214    Top5 95.920076    
2024-06-06 02:30:54,863 - ==> Top1: 77.091    Top5: 95.920    Loss: 0.474

2024-06-06 02:30:54,864 - ==> Confusion:
[[ 824    1    5    0    8    1    0    0    8   56    2    4    0    3    3    1    1    4    2    1    7]
 [   3  931    5    2   25   15    1   20    8    2    2    2    1    2    2    5    6    0   21    6    4]
 [   6    2  857    4    4    1   12   12    0    7    7    7    4    4    4    4    8    1    6    3   17]
 [   2    2   13  864    5    9    4    5    3    2   21    1    5    5   29    1    5    6   19    3   12]
 [  20   12    1    1  941    8    1    2    1   14    2    2    0    1   13    5    9    0    9    1   11]
 [   4   45    1    4   14  834    2   49    3    4    1   23    5   19    3    3    5    2    7    8    7]
 [   1    4   32    2    4    3  966   11    1    1    4    6    0    2    0   16    5    3    4   13    8]
 [   5   16    7    0    2   31    4  930    3    3    5   11    4    5    3    0    0    4   29   10    5]
 [  13    5    1    1    0    2    0    2  825   62   23    2    4   24   20    0    3    1    7    2    5]
 [  84    0    2    4    8    1    1    3   46  803    1    1    0   13   10    2    5    4    1    3    9]
 [   4    3   12   12    2    2    2   10   19    2  946    1    2   14    9    0    1    0   14    4    5]
 [   3    4    2    0    1   10    0   10    2    3    1  880   20    8    0   14    6   22    1   14   10]
 [   2    2    1    2    1    5    0    5    0    1    4   88  809    2    3    7    8   35    9    4    7]
 [   4    1    3    1    7   10    0    4   24   24    9   15    1  868    4    1    1    4    1    6   13]
 [  14    7    0   19   10    1    0    2   34    8    4    0    6    6  953    1    2    3   20    1    7]
 [   6    2    7    2    7    2    8    1    0    1    0   23   11    1    1  951   13   19    3    2    6]
 [   5    7    4    1    7    7    2    0    9    1    4    9    5    7    2   17  957    3    4    6   15]
 [   1    1    1    5    1    4    3    1    2    3    2   23   21    4    4    9    1  901    4    6    8]
 [   1   10    6   17    6    1    0   23    6    2    9    4    3    2   22    0    2    2  938    1    3]
 [   3   10    3    2    2    9   15   12    0    0    0   37   12    6    0    8    9    3    2  946    9]
 [ 339  291  322  125  339  163   99  275  164  175  266  267  431  348  295  168  339  147  273  332 8774]]

2024-06-06 02:30:54,866 - ==> Best [Top1: 77.374   Top5: 96.062   Sparsity:0.00   Params: 169472 on epoch: 149]
2024-06-06 02:30:54,866 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:30:54,873 - 

2024-06-06 02:30:54,873 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:31:01,251 - Epoch: [152][  100/ 1218]    Overall Loss 0.511943    Objective Loss 0.511943                                        LR 0.000250    Time 0.063754    
2024-06-06 02:31:05,825 - Epoch: [152][  200/ 1218]    Overall Loss 0.512168    Objective Loss 0.512168                                        LR 0.000250    Time 0.054736    
2024-06-06 02:31:10,421 - Epoch: [152][  300/ 1218]    Overall Loss 0.516125    Objective Loss 0.516125                                        LR 0.000250    Time 0.051806    
2024-06-06 02:31:15,104 - Epoch: [152][  400/ 1218]    Overall Loss 0.521273    Objective Loss 0.521273                                        LR 0.000250    Time 0.050558    
2024-06-06 02:31:19,751 - Epoch: [152][  500/ 1218]    Overall Loss 0.521024    Objective Loss 0.521024                                        LR 0.000250    Time 0.049737    
2024-06-06 02:31:24,587 - Epoch: [152][  600/ 1218]    Overall Loss 0.521978    Objective Loss 0.521978                                        LR 0.000250    Time 0.049505    
2024-06-06 02:31:29,379 - Epoch: [152][  700/ 1218]    Overall Loss 0.520064    Objective Loss 0.520064                                        LR 0.000250    Time 0.049276    
2024-06-06 02:31:34,169 - Epoch: [152][  800/ 1218]    Overall Loss 0.520699    Objective Loss 0.520699                                        LR 0.000250    Time 0.049102    
2024-06-06 02:31:38,910 - Epoch: [152][  900/ 1218]    Overall Loss 0.520340    Objective Loss 0.520340                                        LR 0.000250    Time 0.048912    
2024-06-06 02:31:43,493 - Epoch: [152][ 1000/ 1218]    Overall Loss 0.520666    Objective Loss 0.520666                                        LR 0.000250    Time 0.048601    
2024-06-06 02:31:48,186 - Epoch: [152][ 1100/ 1218]    Overall Loss 0.521069    Objective Loss 0.521069                                        LR 0.000250    Time 0.048447    
2024-06-06 02:31:52,844 - Epoch: [152][ 1200/ 1218]    Overall Loss 0.520374    Objective Loss 0.520374                                        LR 0.000250    Time 0.048290    
2024-06-06 02:31:53,656 - Epoch: [152][ 1218/ 1218]    Overall Loss 0.519861    Objective Loss 0.519861    Top1 73.105134    Top5 96.332518    LR 0.000250    Time 0.048243    
2024-06-06 02:31:53,838 - --- validate (epoch=152)-----------
2024-06-06 02:31:53,838 - 34633 samples (256 per mini-batch)
2024-06-06 02:31:59,461 - Epoch: [152][  100/  136]    Loss 0.486971    Top1 76.554688    Top5 95.839844    
2024-06-06 02:32:01,217 - Epoch: [152][  136/  136]    Loss 0.487800    Top1 76.536829    Top5 95.813242    
2024-06-06 02:32:01,407 - ==> Top1: 76.537    Top5: 95.813    Loss: 0.488

2024-06-06 02:32:01,408 - ==> Confusion:
[[ 810    1    0    0    9    2    0    4   10   67    0    2    3    4    7    1    2    1    0    2    6]
 [   0  915    4    1   25   33    4   22    7    1    4    4    0    3    6    3    6    4   11    3    7]
 [  17    3  823   13    6    3   15   21    2    6    8    5    2    8    4    5    4    1    8    4   12]
 [   6    3   14  851    4   11    5    2    0    2   14    2   10    3   49    3    2    8   17    0   10]
 [  31    9    3    5  933   14    0    2    2   10    0    3    2    4    9    6    3    1    7    1    9]
 [   3   26    2    2   23  852    2   45    3    2    1   18    4   24    2    2    7    4    8    8    5]
 [   1    3   35    2    7    8  955   14    1    2    5    8    4    2    0    6    4    4    4   12    9]
 [   4    8    5    2    3   18    4  939    3    1    8   13    4    7    0    0    1    1   37   15    4]
 [  10    5    1    1    3    0    0    3  838   55   10    3    7   17   20    0    7    1   11    0   10]
 [  88    1    0    0    7    2    0    2   56  796    1    0    2   24    4    0    1    1    5    0   11]
 [   2    3    9   10    1    6    1    7   34    3  934    3    3   13   12    0    1    0   13    1    8]
 [   1    1    2    0    3   11    1   11    1    2    2  862   32   12    2   11   10   17    2   17   11]
 [   2    3    4    9    0    7    2    4    3    0    3   91  797    4    4    9    4   26    7    5   11]
 [   3    0    1    1    3    7    2    7   27   18   13   13    6  861    6    1    2    3    4    7   16]
 [  12    2    4    9    7    3    0    2   43   10    3    3    1    8  956    0    1    4   18    1   11]
 [   2    1    5    0    9    3    5    1    1    2    0   23    9    3    0  955   17   18    1    3    8]
 [   4    7    5    5   10   14    1    3   10    0    1   12    3    6    5    9  952    1    4    6   14]
 [   3    0    0    4    0    2    2    3    1    2    0   31   33    4    4    8    1  896    3    4    4]
 [   1    5    7    9    4    3    1   27   11    1    6    2    2    2   22    1    1    1  948    3    1]
 [   2    2    3    1    2   14   11   30    1    0    2   27   17   11    2    7   10    4    8  925    9]
 [ 321  249  239  135  363  231  110  296  207  149  183  309  412  379  330  162  292  147  310  399 8709]]

2024-06-06 02:32:01,410 - ==> Best [Top1: 77.374   Top5: 96.062   Sparsity:0.00   Params: 169472 on epoch: 149]
2024-06-06 02:32:01,410 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:32:01,418 - 

2024-06-06 02:32:01,418 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:32:07,601 - Epoch: [153][  100/ 1218]    Overall Loss 0.524411    Objective Loss 0.524411                                        LR 0.000250    Time 0.061808    
2024-06-06 02:32:12,195 - Epoch: [153][  200/ 1218]    Overall Loss 0.522720    Objective Loss 0.522720                                        LR 0.000250    Time 0.053867    
2024-06-06 02:32:16,936 - Epoch: [153][  300/ 1218]    Overall Loss 0.521893    Objective Loss 0.521893                                        LR 0.000250    Time 0.051709    
2024-06-06 02:32:21,727 - Epoch: [153][  400/ 1218]    Overall Loss 0.518120    Objective Loss 0.518120                                        LR 0.000250    Time 0.050755    
2024-06-06 02:32:26,562 - Epoch: [153][  500/ 1218]    Overall Loss 0.522547    Objective Loss 0.522547                                        LR 0.000250    Time 0.050270    
2024-06-06 02:32:31,302 - Epoch: [153][  600/ 1218]    Overall Loss 0.521098    Objective Loss 0.521098                                        LR 0.000250    Time 0.049786    
2024-06-06 02:32:35,876 - Epoch: [153][  700/ 1218]    Overall Loss 0.521953    Objective Loss 0.521953                                        LR 0.000250    Time 0.049205    
2024-06-06 02:32:40,490 - Epoch: [153][  800/ 1218]    Overall Loss 0.520474    Objective Loss 0.520474                                        LR 0.000250    Time 0.048820    
2024-06-06 02:32:45,391 - Epoch: [153][  900/ 1218]    Overall Loss 0.519182    Objective Loss 0.519182                                        LR 0.000250    Time 0.048839    
2024-06-06 02:32:50,006 - Epoch: [153][ 1000/ 1218]    Overall Loss 0.518567    Objective Loss 0.518567                                        LR 0.000250    Time 0.048569    
2024-06-06 02:32:54,621 - Epoch: [153][ 1100/ 1218]    Overall Loss 0.519740    Objective Loss 0.519740                                        LR 0.000250    Time 0.048347    
2024-06-06 02:32:59,456 - Epoch: [153][ 1200/ 1218]    Overall Loss 0.520341    Objective Loss 0.520341                                        LR 0.000250    Time 0.048345    
2024-06-06 02:33:00,259 - Epoch: [153][ 1218/ 1218]    Overall Loss 0.520331    Objective Loss 0.520331    Top1 74.327628    Top5 94.865526    LR 0.000250    Time 0.048288    
2024-06-06 02:33:00,447 - --- validate (epoch=153)-----------
2024-06-06 02:33:00,448 - 34633 samples (256 per mini-batch)
2024-06-06 02:33:05,968 - Epoch: [153][  100/  136]    Loss 0.488734    Top1 77.367188    Top5 95.828125    
2024-06-06 02:33:07,667 - Epoch: [153][  136/  136]    Loss 0.483537    Top1 77.434817    Top5 95.853666    
2024-06-06 02:33:07,840 - ==> Top1: 77.435    Top5: 95.854    Loss: 0.484

2024-06-06 02:33:07,841 - ==> Confusion:
[[ 803    0    6    0   10    2    0    0    8   73    0    2    2    5    5    3    1    0    1    1    9]
 [   5  926    2    1   25   30    3   17    2    2    5    6    4    2    4    4    7    4   10    0    4]
 [  10    3  854   12    1    2   25   14    0    8    5    2    5    6    4    3    5    1    1    5    4]
 [   4    0   22  880    3    9    0    4    3    4   17    4    7    3   25    2    3    5   13    0    8]
 [  24    8    1    1  939   10    2    3    3   16    0    5    1    7    8    9    6    0    2    0    9]
 [   3   38    7    2   18  805    4   45    5    6    1   22   11   33    2    1    6    5    5   11   13]
 [   2    5   35    0    1    3  984    7    0    1    2    3    2    1    1   11    3    1    3   14    7]
 [   6   19   15    6    3   28    5  905    2    1    8   13    6    5    3    1    1    2   27   15    6]
 [  11    6    3    0    3    0    0    0  842   56   18    4    6   18   21    0    1    3    5    1    4]
 [  79    0    2    1    9    3    0    1   41  819    2    2    1   24    4    0    1    2    3    0    7]
 [   2    5   10   12    3    4    5    5   23    3  935    0    2   21   11    1    3    1   12    0    6]
 [   4    3    3    0    1    8    1    8    1    3    0  887   22   19    1   11    5   17    2   10    5]
 [   2    0    1    8    0    3    2    3    3    0    1   72  808    4    2    8    2   41    8   10   17]
 [   4    0    3    2    6    4    1    3    9   22   12   12    5  883    2    6    4    4    2    6   11]
 [   9    6    5   12   12    2    0    1   33   10    6    1    7    7  963    1    1    0   11    0   11]
 [   3    1    7    0    3    1    7    2    0    2    1   21    6    5    1  974    7   16    1    2    6]
 [   2   14    7    0    9    9    2    2    5    0    4   11    3    9    2   19  946    1    2    9   16]
 [   2    2    5    0    0    0    3    2    1    2    2   21   26    6    1   15    2  902    6    2    5]
 [   6   11   11   21    6    1    4   27   13    3    5    4    5    4   21    1    2    1  904    3    5]
 [   7    6   12    2    4    9   23   19    1    0    0   25    7    5    0    9    5    3    2  937   12]
 [ 276  262  351  153  375  198  126  212  150  174  202  230  400  397  248  246  330  157  207  316 8922]]

2024-06-06 02:33:07,842 - ==> Best [Top1: 77.435   Top5: 95.854   Sparsity:0.00   Params: 169472 on epoch: 153]
2024-06-06 02:33:07,842 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:33:07,858 - 

2024-06-06 02:33:07,858 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:33:13,912 - Epoch: [154][  100/ 1218]    Overall Loss 0.520521    Objective Loss 0.520521                                        LR 0.000250    Time 0.060517    
2024-06-06 02:33:18,577 - Epoch: [154][  200/ 1218]    Overall Loss 0.522137    Objective Loss 0.522137                                        LR 0.000250    Time 0.053579    
2024-06-06 02:33:23,388 - Epoch: [154][  300/ 1218]    Overall Loss 0.520370    Objective Loss 0.520370                                        LR 0.000250    Time 0.051749    
2024-06-06 02:33:28,075 - Epoch: [154][  400/ 1218]    Overall Loss 0.520266    Objective Loss 0.520266                                        LR 0.000250    Time 0.050524    
2024-06-06 02:33:32,797 - Epoch: [154][  500/ 1218]    Overall Loss 0.519676    Objective Loss 0.519676                                        LR 0.000250    Time 0.049860    
2024-06-06 02:33:37,420 - Epoch: [154][  600/ 1218]    Overall Loss 0.521154    Objective Loss 0.521154                                        LR 0.000250    Time 0.049251    
2024-06-06 02:33:41,980 - Epoch: [154][  700/ 1218]    Overall Loss 0.521427    Objective Loss 0.521427                                        LR 0.000250    Time 0.048727    
2024-06-06 02:33:46,580 - Epoch: [154][  800/ 1218]    Overall Loss 0.521609    Objective Loss 0.521609                                        LR 0.000250    Time 0.048385    
2024-06-06 02:33:51,334 - Epoch: [154][  900/ 1218]    Overall Loss 0.522716    Objective Loss 0.522716                                        LR 0.000250    Time 0.048287    
2024-06-06 02:33:56,051 - Epoch: [154][ 1000/ 1218]    Overall Loss 0.521574    Objective Loss 0.521574                                        LR 0.000250    Time 0.048174    
2024-06-06 02:34:00,818 - Epoch: [154][ 1100/ 1218]    Overall Loss 0.522084    Objective Loss 0.522084                                        LR 0.000250    Time 0.048125    
2024-06-06 02:34:05,454 - Epoch: [154][ 1200/ 1218]    Overall Loss 0.520818    Objective Loss 0.520818                                        LR 0.000250    Time 0.047977    
2024-06-06 02:34:06,236 - Epoch: [154][ 1218/ 1218]    Overall Loss 0.520422    Objective Loss 0.520422    Top1 77.750611    Top5 97.555012    LR 0.000250    Time 0.047910    
2024-06-06 02:34:06,438 - --- validate (epoch=154)-----------
2024-06-06 02:34:06,439 - 34633 samples (256 per mini-batch)
2024-06-06 02:34:11,956 - Epoch: [154][  100/  136]    Loss 0.475112    Top1 77.718750    Top5 96.050781    
2024-06-06 02:34:13,651 - Epoch: [154][  136/  136]    Loss 0.476939    Top1 77.654260    Top5 96.107758    
2024-06-06 02:34:13,842 - ==> Top1: 77.654    Top5: 96.108    Loss: 0.477

2024-06-06 02:34:13,843 - ==> Confusion:
[[ 798    1    3    0   11    6    0    0    6   67    0    1    5    2    7    0    2    6    8    0    8]
 [   3  928    1    2   16   18    5   22    3    1    7    4    5    4    7    3    6    0   16    2   10]
 [   6    1  829   17    4    3   25    9    2    8    4    3    3    4    8   10    7    2   13    3    9]
 [   6    1   13  876    1    5    3    3    1    4   17    1    8    5   31    3    2    7   19    1    9]
 [  28   11    5    0  925   11    0    1    1   18    1    7    3    2   16    2    4    1    9    2    7]
 [   4   36    5    2   10  810    9   52    1    4    0   17   10   26    5    2   10    1    9   16   14]
 [   1    5   28    1    2    4  986    4    3    0    7    3    1    1    1   14    4    4    5    8    4]
 [   4   17   15    2    4   29    6  905    3    2    2   11    6    4    1    1    2    2   42   12    7]
 [   8    6    1    1    1    2    0    3  819   60   14    4    6   11   35    0    2    1   15    0   13]
 [  77    2    3    1    7    3    0    1   47  818    1    1    5   12    7    2    2    6    1    0    5]
 [   1    2   13   16    2    4    3    9   16    3  927    3    4   14   15    0    0    1   21    3    7]
 [   2    1    6    1    1   11    3    5    1    0    1  853   42   12    0   12    6   23    4   22    5]
 [   0    1    6    6    1    3    4    5    1    1    1   53  830    5    3   13    2   31    9    7   13]
 [   2    2    1    1    1   11    0    6   12   27   15   13    5  861    9    1    2    2    0   16   14]
 [  15    2    2   20    6    3    1    1   24    9    3    2    5    7  970    0    3    3   12    2    8]
 [   2    0    3    1    4    1    4    0    0    3    0   15   14    2    1  975   11   14    3    4    9]
 [   3    5    5    1    3    7    5    0    8    0    1   10    5    4    1   12  967    3    6   13   13]
 [   2    0    3    3    2    0    2    2    1    4    0   15   25    7    5    7    4  918    0    1    4]
 [   3    6   10   18    2    3    2   17    2    1    3    3    9    1   26    0    0    0  942    3    7]
 [   2    6    4    3    0    9   19    8    0    1    2   24    7    5    0    5    8    4    7  965    9]
 [ 244  272  316  200  256  189  126  216  140  160  158  216  440  285  325  153  324  180  354  386 8992]]

2024-06-06 02:34:13,845 - ==> Best [Top1: 77.654   Top5: 96.108   Sparsity:0.00   Params: 169472 on epoch: 154]
2024-06-06 02:34:13,845 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:34:13,860 - 

2024-06-06 02:34:13,861 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:34:20,107 - Epoch: [155][  100/ 1218]    Overall Loss 0.510839    Objective Loss 0.510839                                        LR 0.000250    Time 0.062441    
2024-06-06 02:34:24,738 - Epoch: [155][  200/ 1218]    Overall Loss 0.515209    Objective Loss 0.515209                                        LR 0.000250    Time 0.054366    
2024-06-06 02:34:29,559 - Epoch: [155][  300/ 1218]    Overall Loss 0.517665    Objective Loss 0.517665                                        LR 0.000250    Time 0.052310    
2024-06-06 02:34:34,212 - Epoch: [155][  400/ 1218]    Overall Loss 0.518095    Objective Loss 0.518095                                        LR 0.000250    Time 0.050860    
2024-06-06 02:34:38,851 - Epoch: [155][  500/ 1218]    Overall Loss 0.517267    Objective Loss 0.517267                                        LR 0.000250    Time 0.049962    
2024-06-06 02:34:43,439 - Epoch: [155][  600/ 1218]    Overall Loss 0.517847    Objective Loss 0.517847                                        LR 0.000250    Time 0.049280    
2024-06-06 02:34:48,239 - Epoch: [155][  700/ 1218]    Overall Loss 0.519710    Objective Loss 0.519710                                        LR 0.000250    Time 0.049085    
2024-06-06 02:34:52,817 - Epoch: [155][  800/ 1218]    Overall Loss 0.519580    Objective Loss 0.519580                                        LR 0.000250    Time 0.048670    
2024-06-06 02:34:57,439 - Epoch: [155][  900/ 1218]    Overall Loss 0.517207    Objective Loss 0.517207                                        LR 0.000250    Time 0.048395    
2024-06-06 02:35:02,134 - Epoch: [155][ 1000/ 1218]    Overall Loss 0.517570    Objective Loss 0.517570                                        LR 0.000250    Time 0.048249    
2024-06-06 02:35:06,846 - Epoch: [155][ 1100/ 1218]    Overall Loss 0.516900    Objective Loss 0.516900                                        LR 0.000250    Time 0.048145    
2024-06-06 02:35:11,389 - Epoch: [155][ 1200/ 1218]    Overall Loss 0.517607    Objective Loss 0.517607                                        LR 0.000250    Time 0.047917    
2024-06-06 02:35:12,166 - Epoch: [155][ 1218/ 1218]    Overall Loss 0.518354    Objective Loss 0.518354    Top1 73.838631    Top5 93.643032    LR 0.000250    Time 0.047846    
2024-06-06 02:35:12,359 - --- validate (epoch=155)-----------
2024-06-06 02:35:12,359 - 34633 samples (256 per mini-batch)
2024-06-06 02:35:17,897 - Epoch: [155][  100/  136]    Loss 0.480170    Top1 77.414062    Top5 95.929688    
2024-06-06 02:35:19,591 - Epoch: [155][  136/  136]    Loss 0.474050    Top1 77.362631    Top5 96.024023    
2024-06-06 02:35:19,767 - ==> Top1: 77.363    Top5: 96.024    Loss: 0.474

2024-06-06 02:35:19,768 - ==> Confusion:
[[ 817    2    1    2    4    3    0    4    6   57    0    4    3    3    2    3    0    1    2    5   12]
 [   2  911    3    2   25   23    7   11   10    3    6    3    4    0    8    3    7    2   22    6    5]
 [  14    1  834   13    3    0   34    7    0    5    6    1    3    7    4    3    9    3    9    5    9]
 [  10    3   19  877    2    4    2    1    0    0   10    1   12    6   32    3    0    9   16    3    6]
 [  29    5    3    3  940   10    2    2    2   16    0    4    2    4    5    4    6    2    6    1    8]
 [   7   37    1    6   19  828    5   38    5    8    1   18    5   18    6    1    7    6    6   13    8]
 [   4    9   22    0    2    2  991    3    1    0    7    4    1    2    1   11    3    4    3   10    6]
 [   3   25   12    3    0   24    3  911    2    3    5   15    2    2    2    3    2    6   31   18    5]
 [  10    3    1    1    3    3    0    1  827   67   12    5    2   15   27    0    3    4   11    1    6]
 [  99    0    1    0    4    1    1    3   45  797    1    1    2   15   15    0    1    5    2    0    8]
 [   2    3    5   20    3    3    6    3   23    2  940    3    0   13   14    0    0    2   13    5    4]
 [   5    2    2    0    0    7    2    5    0    3    1  845   49   19    2   12    8   24    2   15    8]
 [   1    0    3    5    2    3    0    3    4    0    3   56  827    5    7    6    5   42    7    8    8]
 [   4    2    1    0    8    8    0    4   20   28    8   10    2  861    4    2    8   10    2    7   12]
 [  15    6    4   21    7    2    1    3   24    9    5    1    2    5  960    0    2    6   16    1    8]
 [   2    1    3    2    5    2    7    1    0    1    0   12   12    1    0  971   10   23    2    3    8]
 [   3    9    4    0    6    5    4    1    4    2    3   11    8    2    2   15  967    2    3    5   16]
 [   3    0    0    2    1    2    1    1    0    2    0    6   23    7    1   16    3  924    3    2    8]
 [   1    4    6   20    3    5    3   17    6    1    9    1    5    1   24    0    1    0  942    3    6]
 [   2    9    5    1    2    7   13   10    0    0    2   24   15    4    1   10    9    6    4  952   12]
 [ 309  299  227  171  284  162  114  209  165  169  206  204  420  348  320  200  382  206  289  377 8871]]

2024-06-06 02:35:19,770 - ==> Best [Top1: 77.654   Top5: 96.108   Sparsity:0.00   Params: 169472 on epoch: 154]
2024-06-06 02:35:19,770 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:35:19,778 - 

2024-06-06 02:35:19,778 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:35:25,964 - Epoch: [156][  100/ 1218]    Overall Loss 0.520390    Objective Loss 0.520390                                        LR 0.000250    Time 0.061840    
2024-06-06 02:35:30,827 - Epoch: [156][  200/ 1218]    Overall Loss 0.516373    Objective Loss 0.516373                                        LR 0.000250    Time 0.055225    
2024-06-06 02:35:35,574 - Epoch: [156][  300/ 1218]    Overall Loss 0.519157    Objective Loss 0.519157                                        LR 0.000250    Time 0.052633    
2024-06-06 02:35:40,120 - Epoch: [156][  400/ 1218]    Overall Loss 0.518736    Objective Loss 0.518736                                        LR 0.000250    Time 0.050836    
2024-06-06 02:35:44,849 - Epoch: [156][  500/ 1218]    Overall Loss 0.518770    Objective Loss 0.518770                                        LR 0.000250    Time 0.050123    
2024-06-06 02:35:49,397 - Epoch: [156][  600/ 1218]    Overall Loss 0.519873    Objective Loss 0.519873                                        LR 0.000250    Time 0.049345    
2024-06-06 02:35:53,973 - Epoch: [156][  700/ 1218]    Overall Loss 0.521095    Objective Loss 0.521095                                        LR 0.000250    Time 0.048831    
2024-06-06 02:35:58,541 - Epoch: [156][  800/ 1218]    Overall Loss 0.521117    Objective Loss 0.521117                                        LR 0.000250    Time 0.048435    
2024-06-06 02:36:03,103 - Epoch: [156][  900/ 1218]    Overall Loss 0.520778    Objective Loss 0.520778                                        LR 0.000250    Time 0.048120    
2024-06-06 02:36:07,683 - Epoch: [156][ 1000/ 1218]    Overall Loss 0.519952    Objective Loss 0.519952                                        LR 0.000250    Time 0.047887    
2024-06-06 02:36:12,373 - Epoch: [156][ 1100/ 1218]    Overall Loss 0.519539    Objective Loss 0.519539                                        LR 0.000250    Time 0.047796    
2024-06-06 02:36:17,152 - Epoch: [156][ 1200/ 1218]    Overall Loss 0.518997    Objective Loss 0.518997                                        LR 0.000250    Time 0.047793    
2024-06-06 02:36:18,035 - Epoch: [156][ 1218/ 1218]    Overall Loss 0.518475    Objective Loss 0.518475    Top1 77.506112    Top5 96.088020    LR 0.000250    Time 0.047812    
2024-06-06 02:36:18,233 - --- validate (epoch=156)-----------
2024-06-06 02:36:18,234 - 34633 samples (256 per mini-batch)
2024-06-06 02:36:23,734 - Epoch: [156][  100/  136]    Loss 0.471625    Top1 76.804688    Top5 95.660156    
2024-06-06 02:36:25,589 - Epoch: [156][  136/  136]    Loss 0.475300    Top1 76.646551    Top5 95.625559    
2024-06-06 02:36:25,782 - ==> Top1: 76.647    Top5: 95.626    Loss: 0.475

2024-06-06 02:36:25,784 - ==> Confusion:
[[ 823    2    7    2   14    1    0    1    3   45    1    1    1    4   13    2    3    2    1    0    5]
 [   4  932    1    0   26   24    5   15    1    3    0    0    5    2    6    0    6    0   18    8    7]
 [   7    2  833   10    8    3   31   11    0    5    7    6    5    5    2    9    4    1    8    5    8]
 [   5    0   16  877    6    4    5    1    3    2   23    0    7    3   35    3    2    5   10    0    9]
 [  27    2    1    2  950   11    1    1    2    7    1    3    3    3   12    3    8    1    6    4    6]
 [   3   32    1    3   22  834    7   39    4    4    2   20    9   22    8    4    7    3    5    8    6]
 [   2    3   24    1    0    6 1001    5    1    1    1    5    1    2    0   10    5    1    4    7    6]
 [   5   19   14    1    6   46    4  882    8    3    4   14    1    4    2    0    1    1   40   12   10]
 [  22   10    1    1    1    3    0    0  796   65   13    1    6   16   38    1    6    1   12    2    7]
 [ 103    2    2    2   15    3    1    2   26  798    2    1    2   20   12    0    0    1    2    0    7]
 [   1   12   11   13    2    2    5    3   20    0  927    0    3   11   17    0    1    0   26    5    5]
 [   6    6    2    0    1   15    5    3    1    2    0  865   30    8    1   13    4   18    3   20    8]
 [   1    1    1    8    1    3    1    3    1    0    2   54  832    3    4   10    7   36    8    8   11]
 [  10    0    2    1    8   13    1    3   19   14    9   10    3  871   11    3    2    4    0    9    8]
 [  10    7    1   13   10    2    0    1   22    4    5    1    5    6  982    0    0    2   16    1   10]
 [   2    3    4    1    6    1   10    1    0    0    1   18    9    2    0  971    7   17    4    5    4]
 [   2   14    4    4   12   14    2    1    7    1    2    6    3    4    5   18  944    1    2   10   16]
 [   6    2    3    5    2    2    3    1    4    1    0   17   30    4    3   18    1  893    4    4    2]
 [   3    5    6   18    9    2    1   17    7    2    3    5    3    1   29    2    2    0  932    3    8]
 [   2    5    3    1    3    9   25   12    1    0    1   18   16    6    2    7    4    3    3  960    7]
 [ 291  276  347  154  428  225  164  199  127  151  203  219  417  362  412  211  357  138  277  332 8642]]

2024-06-06 02:36:25,785 - ==> Best [Top1: 77.654   Top5: 96.108   Sparsity:0.00   Params: 169472 on epoch: 154]
2024-06-06 02:36:25,785 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:36:25,793 - 

2024-06-06 02:36:25,793 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:36:31,764 - Epoch: [157][  100/ 1218]    Overall Loss 0.524355    Objective Loss 0.524355                                        LR 0.000250    Time 0.059695    
2024-06-06 02:36:36,415 - Epoch: [157][  200/ 1218]    Overall Loss 0.512452    Objective Loss 0.512452                                        LR 0.000250    Time 0.053093    
2024-06-06 02:36:41,115 - Epoch: [157][  300/ 1218]    Overall Loss 0.512666    Objective Loss 0.512666                                        LR 0.000250    Time 0.051057    
2024-06-06 02:36:45,787 - Epoch: [157][  400/ 1218]    Overall Loss 0.511026    Objective Loss 0.511026                                        LR 0.000250    Time 0.049966    
2024-06-06 02:36:50,697 - Epoch: [157][  500/ 1218]    Overall Loss 0.511899    Objective Loss 0.511899                                        LR 0.000250    Time 0.049791    
2024-06-06 02:36:55,384 - Epoch: [157][  600/ 1218]    Overall Loss 0.513333    Objective Loss 0.513333                                        LR 0.000250    Time 0.049300    
2024-06-06 02:37:00,001 - Epoch: [157][  700/ 1218]    Overall Loss 0.516051    Objective Loss 0.516051                                        LR 0.000250    Time 0.048850    
2024-06-06 02:37:04,542 - Epoch: [157][  800/ 1218]    Overall Loss 0.514237    Objective Loss 0.514237                                        LR 0.000250    Time 0.048418    
2024-06-06 02:37:09,279 - Epoch: [157][  900/ 1218]    Overall Loss 0.515668    Objective Loss 0.515668                                        LR 0.000250    Time 0.048300    
2024-06-06 02:37:13,913 - Epoch: [157][ 1000/ 1218]    Overall Loss 0.514344    Objective Loss 0.514344                                        LR 0.000250    Time 0.048103    
2024-06-06 02:37:18,708 - Epoch: [157][ 1100/ 1218]    Overall Loss 0.514246    Objective Loss 0.514246                                        LR 0.000250    Time 0.048087    
2024-06-06 02:37:23,292 - Epoch: [157][ 1200/ 1218]    Overall Loss 0.513778    Objective Loss 0.513778                                        LR 0.000250    Time 0.047899    
2024-06-06 02:37:24,108 - Epoch: [157][ 1218/ 1218]    Overall Loss 0.513599    Objective Loss 0.513599    Top1 78.973105    Top5 96.332518    LR 0.000250    Time 0.047860    
2024-06-06 02:37:24,309 - --- validate (epoch=157)-----------
2024-06-06 02:37:24,309 - 34633 samples (256 per mini-batch)
2024-06-06 02:37:29,786 - Epoch: [157][  100/  136]    Loss 0.467366    Top1 77.566406    Top5 96.183594    
2024-06-06 02:37:31,451 - Epoch: [157][  136/  136]    Loss 0.467858    Top1 77.504115    Top5 96.252129    
2024-06-06 02:37:31,623 - ==> Top1: 77.504    Top5: 96.252    Loss: 0.468

2024-06-06 02:37:31,624 - ==> Confusion:
[[ 794    2    1    2   10    2    0    2    8   76    1    2    2    2    9    4    2    0    1    1   10]
 [   3  925    3    4   15   30    4   15    7    0    2    1    4    3    7    3    8    2   15    2   10]
 [   5    8  825   13    4    4   35   18    0    4   11    1    3    4    6    6    4    1    4    3   11]
 [   4    4   18  901    1    4    3    3    3    1   16    2    9    4   20    3    3    6    8    0    3]
 [  23   18    2    2  924   13    1    1    2   15    2    1    3    5   10    5    7    2    6    0   12]
 [   1   25    1    4   21  870    5   33    2    6    2   10   10   18    3    3    9    3    3    8    6]
 [   1    9   14    3    2    6 1000    6    2    2    3    2    1    2    0   11    6    3    2    5    6]
 [   1   16    9    0    1   49   11  898    3    1    7   11    9    2    2    2    4    1   33    8    9]
 [  18    4    0    2    3    2    0    3  823   44   13    1    8   26   29    1    4    5   10    2    4]
 [  79    2    1    1    6    8    1    0   68  796    3    0    1   20    6    1    1    2    2    0    3]
 [   3    2    5   16    2    8    4    5   22    0  949    1    1   16    9    0    2    1   15    0    3]
 [   2    0    2    0    2   11    2   14    1    0    0  858   41   15    0   23    4   15    1   15    5]
 [   0    3    2    7    0    4    3    3    2    0    0   62  836    4    2   12    2   31    3    4   15]
 [   3    0    0    0    9   16    1    1   18   18   13    6    6  874    5    4    1    5    2   11    8]
 [  16    5    3   19    7    2    0    1   30   10    6    2    4    6  959    1    4    3   15    0    5]
 [   3    1    5    0    5    2    8    0    1    1    1   24   11    2    1  970   10    7    0    1   13]
 [   7   13    4    2    7   10    2    0    6    0    3    9    6    5    4   14  951    3    6    8   12]
 [   2    1    0    5    2    2    3    1    5    2    1   19   43    1    5   20    1  881    1    2    8]
 [   2   10    8   24    6    2    2   23    5    1    5    2    4    2   19    1    5    2  925    1    9]
 [   2    5    3    0    1   14   20   13    1    0    1   25    8    3    1   10    6    3    5  960    7]
 [ 233  292  295  169  268  269  171  234  166  132  215  184  450  359  298  240  348  150  203  334 8922]]

2024-06-06 02:37:31,626 - ==> Best [Top1: 77.654   Top5: 96.108   Sparsity:0.00   Params: 169472 on epoch: 154]
2024-06-06 02:37:31,626 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:37:31,634 - 

2024-06-06 02:37:31,634 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:37:37,773 - Epoch: [158][  100/ 1218]    Overall Loss 0.513393    Objective Loss 0.513393                                        LR 0.000250    Time 0.061374    
2024-06-06 02:37:42,380 - Epoch: [158][  200/ 1218]    Overall Loss 0.510490    Objective Loss 0.510490                                        LR 0.000250    Time 0.053709    
2024-06-06 02:37:46,989 - Epoch: [158][  300/ 1218]    Overall Loss 0.513101    Objective Loss 0.513101                                        LR 0.000250    Time 0.051164    
2024-06-06 02:37:51,782 - Epoch: [158][  400/ 1218]    Overall Loss 0.516246    Objective Loss 0.516246                                        LR 0.000250    Time 0.050351    
2024-06-06 02:37:56,356 - Epoch: [158][  500/ 1218]    Overall Loss 0.517698    Objective Loss 0.517698                                        LR 0.000250    Time 0.049424    
2024-06-06 02:38:00,902 - Epoch: [158][  600/ 1218]    Overall Loss 0.519047    Objective Loss 0.519047                                        LR 0.000250    Time 0.048760    
2024-06-06 02:38:05,444 - Epoch: [158][  700/ 1218]    Overall Loss 0.519623    Objective Loss 0.519623                                        LR 0.000250    Time 0.048281    
2024-06-06 02:38:10,047 - Epoch: [158][  800/ 1218]    Overall Loss 0.521960    Objective Loss 0.521960                                        LR 0.000250    Time 0.047998    
2024-06-06 02:38:14,862 - Epoch: [158][  900/ 1218]    Overall Loss 0.520766    Objective Loss 0.520766                                        LR 0.000250    Time 0.048012    
2024-06-06 02:38:19,423 - Epoch: [158][ 1000/ 1218]    Overall Loss 0.519199    Objective Loss 0.519199                                        LR 0.000250    Time 0.047770    
2024-06-06 02:38:24,207 - Epoch: [158][ 1100/ 1218]    Overall Loss 0.518946    Objective Loss 0.518946                                        LR 0.000250    Time 0.047774    
2024-06-06 02:38:28,780 - Epoch: [158][ 1200/ 1218]    Overall Loss 0.519143    Objective Loss 0.519143                                        LR 0.000250    Time 0.047603    
2024-06-06 02:38:29,643 - Epoch: [158][ 1218/ 1218]    Overall Loss 0.519328    Objective Loss 0.519328    Top1 76.772616    Top5 95.843521    LR 0.000250    Time 0.047607    
2024-06-06 02:38:29,820 - --- validate (epoch=158)-----------
2024-06-06 02:38:29,820 - 34633 samples (256 per mini-batch)
2024-06-06 02:38:35,372 - Epoch: [158][  100/  136]    Loss 0.475308    Top1 77.246094    Top5 95.972656    
2024-06-06 02:38:37,036 - Epoch: [158][  136/  136]    Loss 0.474892    Top1 77.226922    Top5 95.986487    
2024-06-06 02:38:37,219 - ==> Top1: 77.227    Top5: 95.986    Loss: 0.475

2024-06-06 02:38:37,220 - ==> Confusion:
[[ 815    1    4    0   13    1    1    0   12   46    1    4    1    3    9    2    2    3    0    3   10]
 [   4  927    4    2   27   26    9   13    4    1    3    6    1    1    7    1    4    0   14    3    6]
 [   8    3  856   13    5    2   31    2    2    5    1    7    5    2    2    4    5    0    3    3   11]
 [   5    5   21  877    5   10    5    2    4    1   17    2    8    3   22    4    1    5    8    0   11]
 [  17   10    5    1  946   10    6    1    1   11    1    1    0    2   10    9    9    3    3    2    6]
 [   4   24    2    3   14  858    3   37    6    5    2   14    7   22    5    1    7    4    6   15    4]
 [   1    2   28    0    4    6 1001    3    1    1    2    4    3    2    0    8    4    2    2    7    5]
 [   2   19   20    2    1   41    9  880    3    4    6   13    7    2    3    0    2    2   33   22    6]
 [  21    4    0    2    1    0    1    1  826   52   25    1    8   17   20    0    5    2    7    0    9]
 [ 110    3    5    1    9    1    0    0   49  773    1    4    2   19    7    1    5    4    3    1    3]
 [   1    3    7   14    4    4   10    4   17    3  946    1    5   10    7    0    2    0   14    1   11]
 [   0    2    4    1    3   12    2    7    2    1    0  860   38    9    0    8    7   28    1   21    5]
 [   1    2    3    6    1    2    3    3    5    1    5   73  791    7    3    8    5   51    5    9   11]
 [   5    1    5    1    4   14    0    3   14   22    9   12    5  869    6    1    6    7    2    6    9]
 [  21    8    2   18   12    1    1    3   34    6    8    2    4    9  948    0    1    3   10    0    7]
 [   1    1    6    0    4    1   13    1    1    1    0   21    8    3    0  960   10   24    2    4    5]
 [   2    8    1    1    6    9    2    1    6    0    1    6    4    6    2   14  974    2    3    6   18]
 [   3    3    0    2    1    2    3    0    0    3    0   19   17    2    2    7    5  925    0    5    6]
 [   5   14   16   17    7    2    0   19    7    0    0    4    4    3   21    0    0    2  925    5    7]
 [   2    6    3    1    4   12   17    8    1    0    1   26    9    7    0   12    3    5    4  957   10]
 [ 295  286  320  151  319  242  174  205  135  129  223  198  443  368  260  180  379  188  217  388 8832]]

2024-06-06 02:38:37,223 - ==> Best [Top1: 77.654   Top5: 96.108   Sparsity:0.00   Params: 169472 on epoch: 154]
2024-06-06 02:38:37,223 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:38:37,234 - 

2024-06-06 02:38:37,234 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:38:43,427 - Epoch: [159][  100/ 1218]    Overall Loss 0.514889    Objective Loss 0.514889                                        LR 0.000250    Time 0.061902    
2024-06-06 02:38:48,046 - Epoch: [159][  200/ 1218]    Overall Loss 0.514919    Objective Loss 0.514919                                        LR 0.000250    Time 0.054037    
2024-06-06 02:38:52,659 - Epoch: [159][  300/ 1218]    Overall Loss 0.516757    Objective Loss 0.516757                                        LR 0.000250    Time 0.051395    
2024-06-06 02:38:57,219 - Epoch: [159][  400/ 1218]    Overall Loss 0.515343    Objective Loss 0.515343                                        LR 0.000250    Time 0.049941    
2024-06-06 02:39:01,955 - Epoch: [159][  500/ 1218]    Overall Loss 0.517266    Objective Loss 0.517266                                        LR 0.000250    Time 0.049422    
2024-06-06 02:39:06,821 - Epoch: [159][  600/ 1218]    Overall Loss 0.516149    Objective Loss 0.516149                                        LR 0.000250    Time 0.049291    
2024-06-06 02:39:11,518 - Epoch: [159][  700/ 1218]    Overall Loss 0.515257    Objective Loss 0.515257                                        LR 0.000250    Time 0.048957    
2024-06-06 02:39:16,248 - Epoch: [159][  800/ 1218]    Overall Loss 0.515234    Objective Loss 0.515234                                        LR 0.000250    Time 0.048748    
2024-06-06 02:39:20,990 - Epoch: [159][  900/ 1218]    Overall Loss 0.515156    Objective Loss 0.515156                                        LR 0.000250    Time 0.048598    
2024-06-06 02:39:25,725 - Epoch: [159][ 1000/ 1218]    Overall Loss 0.515952    Objective Loss 0.515952                                        LR 0.000250    Time 0.048472    
2024-06-06 02:39:30,432 - Epoch: [159][ 1100/ 1218]    Overall Loss 0.516225    Objective Loss 0.516225                                        LR 0.000250    Time 0.048342    
2024-06-06 02:39:34,996 - Epoch: [159][ 1200/ 1218]    Overall Loss 0.517010    Objective Loss 0.517010                                        LR 0.000250    Time 0.048116    
2024-06-06 02:39:35,827 - Epoch: [159][ 1218/ 1218]    Overall Loss 0.517080    Objective Loss 0.517080    Top1 78.484108    Top5 96.332518    LR 0.000250    Time 0.048087    
2024-06-06 02:39:36,007 - --- validate (epoch=159)-----------
2024-06-06 02:39:36,007 - 34633 samples (256 per mini-batch)
2024-06-06 02:39:41,652 - Epoch: [159][  100/  136]    Loss 0.479031    Top1 77.128906    Top5 95.691406    
2024-06-06 02:39:43,418 - Epoch: [159][  136/  136]    Loss 0.477614    Top1 77.045015    Top5 95.712182    
2024-06-06 02:39:43,607 - ==> Top1: 77.045    Top5: 95.712    Loss: 0.478

2024-06-06 02:39:43,608 - ==> Confusion:
[[ 806    2    5    0    9    1    1    1   14   62    1    1    5    3    1    1    1    2    4    5    6]
 [   5  912    5    4   22   22    7   19   10    2    4    2    4    3   10    2    3    2   14    5    6]
 [   6    4  839   12    4    5   24   16    1    4    5    3    3    5    5   10    2    1    8    5    8]
 [   7    3   16  867    1    7    2    3    5    3   13    1   12    2   36    4    2    4   19    3    6]
 [  29    8    1    1  941    7    2    4    2   14    0    4    2    2   14    8    9    0    0    2    4]
 [   9   39    4    3   16  843    3   34    8    4    4    9   11   27    2    2    4    1    4    6   10]
 [   2    6   26    2    1    5  981    7    3    0    2    4    2    2    0   11    4    3    3   11   11]
 [   8   15   10    1    2   23    9  909    4    2    2   11    5    4    5    0    0    5   42   18    2]
 [  11    1    3    0    1    1    0    0  855   52   11    2    4   19   17    0    3    3    7    3    9]
 [ 105    1    3    0    5    3    1    1   43  789    2    2    2   21    4    0    0    5    5    2    7]
 [   1    4   14   14    3    5    2    5   21    6  920    3    4   12   15    0    4    0   18    3   10]
 [   2    4    2    0    3   11    2    5    2    3    1  836   38   11    1   14    5   30    1   33    7]
 [   2    3    2    5    0    7    2    4    3    0    3   56  835    2    2    7    1   34    3   14   10]
 [   1    0    1    0    4   14    1    3   19   24    7   13    9  874    6    2    7    2    0    5    9]
 [  14    3    3    9    9    3    0    4   37   16    4    1    3    6  959    0    1    2   13    1   10]
 [   2    2    3    0    3    2    7    0    1    1    0   22   13    3    1  972   16   10    1    1    6]
 [   2   11    7    0    8    5    2    2   10    0    4   10    2    4    5   16  953    1    1    9   20]
 [   2    3    1    1    0    2    1    0    1    1    0   20   31    1    6   14    2  904    2    7    6]
 [   4    6   11   13    4    1    1   14    6    4    6    4    6    1   23    2    1    2  942    5    2]
 [   1    7    4    0    3    9   15   14    1    2    0   23    4    8    3   12    7    5    4  958    8]
 [ 308  235  287  138  296  176  118  215  188  186  211  223  460  372  309  213  356  157  251  445 8788]]

2024-06-06 02:39:43,610 - ==> Best [Top1: 77.654   Top5: 96.108   Sparsity:0.00   Params: 169472 on epoch: 154]
2024-06-06 02:39:43,610 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:39:43,619 - 

2024-06-06 02:39:43,619 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:39:49,826 - Epoch: [160][  100/ 1218]    Overall Loss 0.505627    Objective Loss 0.505627                                        LR 0.000250    Time 0.062044    
2024-06-06 02:39:54,508 - Epoch: [160][  200/ 1218]    Overall Loss 0.507263    Objective Loss 0.507263                                        LR 0.000250    Time 0.054426    
2024-06-06 02:39:59,266 - Epoch: [160][  300/ 1218]    Overall Loss 0.510189    Objective Loss 0.510189                                        LR 0.000250    Time 0.052135    
2024-06-06 02:40:03,886 - Epoch: [160][  400/ 1218]    Overall Loss 0.511709    Objective Loss 0.511709                                        LR 0.000250    Time 0.050647    
2024-06-06 02:40:08,583 - Epoch: [160][  500/ 1218]    Overall Loss 0.513405    Objective Loss 0.513405                                        LR 0.000250    Time 0.049910    
2024-06-06 02:40:13,183 - Epoch: [160][  600/ 1218]    Overall Loss 0.513076    Objective Loss 0.513076                                        LR 0.000250    Time 0.049255    
2024-06-06 02:40:17,813 - Epoch: [160][  700/ 1218]    Overall Loss 0.514244    Objective Loss 0.514244                                        LR 0.000250    Time 0.048830    
2024-06-06 02:40:22,348 - Epoch: [160][  800/ 1218]    Overall Loss 0.512995    Objective Loss 0.512995                                        LR 0.000250    Time 0.048392    
2024-06-06 02:40:26,930 - Epoch: [160][  900/ 1218]    Overall Loss 0.514095    Objective Loss 0.514095                                        LR 0.000250    Time 0.048105    
2024-06-06 02:40:31,576 - Epoch: [160][ 1000/ 1218]    Overall Loss 0.513198    Objective Loss 0.513198                                        LR 0.000250    Time 0.047939    
2024-06-06 02:40:36,377 - Epoch: [160][ 1100/ 1218]    Overall Loss 0.513570    Objective Loss 0.513570                                        LR 0.000250    Time 0.047943    
2024-06-06 02:40:41,083 - Epoch: [160][ 1200/ 1218]    Overall Loss 0.513264    Objective Loss 0.513264                                        LR 0.000250    Time 0.047868    
2024-06-06 02:40:41,951 - Epoch: [160][ 1218/ 1218]    Overall Loss 0.513440    Objective Loss 0.513440    Top1 73.349633    Top5 93.398533    LR 0.000250    Time 0.047871    
2024-06-06 02:40:42,135 - --- validate (epoch=160)-----------
2024-06-06 02:40:42,135 - 34633 samples (256 per mini-batch)
2024-06-06 02:40:47,916 - Epoch: [160][  100/  136]    Loss 0.471238    Top1 77.316406    Top5 96.164062    
2024-06-06 02:40:49,597 - Epoch: [160][  136/  136]    Loss 0.474937    Top1 77.195161    Top5 96.119308    
2024-06-06 02:40:49,785 - ==> Top1: 77.195    Top5: 96.119    Loss: 0.475

2024-06-06 02:40:49,786 - ==> Confusion:
[[ 810    3    5    0   13    2    1    0    4   58    0    1    1    7    5    2    3    1    0    3   12]
 [   3  914    4    1   27   24    6   21    4    3    3    3    1    0    7    3    5    3   16    1   14]
 [   9    4  843   11    6    3   29   11    2   10    7    3    3    2    1    6    4    2    4    3    7]
 [   6    1   21  873    1    7    4    2    3    1   23    1    5    1   34    1    1    5   16    3    7]
 [  26    9    1    1  942   11    0    5    2   10    2    4    1    5    6    4    6    2    7    3    7]
 [   4   27    6    4   22  838    4   45    4    2    4   11    8   24    3    0    9    2    7   10    9]
 [   0    3   36    1    2    3  975   11    1    3    3    7    1    1    0    6    4    4    6    9   10]
 [   6   16   12    3    4   38    9  902    3    0   10   10    7    1    1    0    1    0   29   19    6]
 [  16    2    2    1    2    6    0    0  823   58   21    0    4   23   25    0    4    0    9    1    5]
 [  86    2    2    0   11    4    0    3   42  801    3    2    0   21   10    1    1    3    0    0    9]
 [   2    7   15    9    0    4    3   10   12    1  940    2    1   17   12    1    0    1   19    2    6]
 [   3    2    2    2    1   20    5    6    0    1    1  854   28   15    1   10    4   17    5   23   11]
 [   1    1    4    8    1    6    5    5    2    0    1   83  804    2    4   11    3   30    8    8    8]
 [   2    1    6    1    5   14    2    0   16   20    9    9    5  881    5    4    3    2    2    5    9]
 [  11    4    3    9   13    2    0    3   28    9    6    2    6    6  965    2    2    3   14    1    9]
 [   0    0    6    0    7    2    6    2    1    0    0   12   10    1    0  976   17    8    3    4   11]
 [   1   13    4    1   11   11    1    1    5    1    4    7    2    3    3    7  964    0    7    6   20]
 [   3    2    0    3    0    3    1    0    0    5    0   28   32    5    2   15    3  889    1    7    6]
 [   6    7    8   14    5    4    3   34   11    0    5    5    8    2   17    0    0    0  922    2    5]
 [   2    6    2    0    1   12   16   18    2    0    0   22    8    6    3   10   10    2    3  952   13]
 [ 299  277  320  145  319  236  142  261  159  151  248  206  371  357  284  159  359  132  253  387 8867]]

2024-06-06 02:40:49,788 - ==> Best [Top1: 77.654   Top5: 96.108   Sparsity:0.00   Params: 169472 on epoch: 154]
2024-06-06 02:40:49,788 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:40:49,795 - 

2024-06-06 02:40:49,795 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:40:56,101 - Epoch: [161][  100/ 1218]    Overall Loss 0.498581    Objective Loss 0.498581                                        LR 0.000250    Time 0.063036    
2024-06-06 02:41:00,889 - Epoch: [161][  200/ 1218]    Overall Loss 0.506185    Objective Loss 0.506185                                        LR 0.000250    Time 0.055449    
2024-06-06 02:41:05,579 - Epoch: [161][  300/ 1218]    Overall Loss 0.511054    Objective Loss 0.511054                                        LR 0.000250    Time 0.052593    
2024-06-06 02:41:10,348 - Epoch: [161][  400/ 1218]    Overall Loss 0.509800    Objective Loss 0.509800                                        LR 0.000250    Time 0.051362    
2024-06-06 02:41:15,088 - Epoch: [161][  500/ 1218]    Overall Loss 0.508600    Objective Loss 0.508600                                        LR 0.000250    Time 0.050566    
2024-06-06 02:41:19,887 - Epoch: [161][  600/ 1218]    Overall Loss 0.509855    Objective Loss 0.509855                                        LR 0.000250    Time 0.050134    
2024-06-06 02:41:24,573 - Epoch: [161][  700/ 1218]    Overall Loss 0.512401    Objective Loss 0.512401                                        LR 0.000250    Time 0.049664    
2024-06-06 02:41:29,269 - Epoch: [161][  800/ 1218]    Overall Loss 0.511439    Objective Loss 0.511439                                        LR 0.000250    Time 0.049323    
2024-06-06 02:41:33,817 - Epoch: [161][  900/ 1218]    Overall Loss 0.512365    Objective Loss 0.512365                                        LR 0.000250    Time 0.048895    
2024-06-06 02:41:38,395 - Epoch: [161][ 1000/ 1218]    Overall Loss 0.512082    Objective Loss 0.512082                                        LR 0.000250    Time 0.048582    
2024-06-06 02:41:43,112 - Epoch: [161][ 1100/ 1218]    Overall Loss 0.513061    Objective Loss 0.513061                                        LR 0.000250    Time 0.048451    
2024-06-06 02:41:47,680 - Epoch: [161][ 1200/ 1218]    Overall Loss 0.513662    Objective Loss 0.513662                                        LR 0.000250    Time 0.048219    
2024-06-06 02:41:48,523 - Epoch: [161][ 1218/ 1218]    Overall Loss 0.514352    Objective Loss 0.514352    Top1 76.283619    Top5 94.132029    LR 0.000250    Time 0.048198    
2024-06-06 02:41:48,699 - --- validate (epoch=161)-----------
2024-06-06 02:41:48,700 - 34633 samples (256 per mini-batch)
2024-06-06 02:41:54,283 - Epoch: [161][  100/  136]    Loss 0.482081    Top1 76.527344    Top5 95.601562    
2024-06-06 02:41:56,002 - Epoch: [161][  136/  136]    Loss 0.482463    Top1 76.646551    Top5 95.668871    
2024-06-06 02:41:56,182 - ==> Top1: 76.647    Top5: 95.669    Loss: 0.482

2024-06-06 02:41:56,183 - ==> Confusion:
[[ 840    0    1    1   13    0    0    1    4   43    1    1    4    7    5    2    0    1    1    1    5]
 [   4  911    4    1   28   40    7   13    3    0    6    6    2    5    6    3    8    1    7    3    5]
 [   9    1  842   12    4    5   31    8    1    6    5    3    5    3    2    8    4    2    5    3   11]
 [   4    3   14  871    6   15    4    2    1    0   22    3    6    4   29    3    2    8   11    0    8]
 [  21   12    8    2  935   18    1    4    4   14    0    3    1    3    9    8    3    1    3    1    3]
 [  12   16    2    6   12  863    6   26    3    2    0   20    8   27    3    0    6    2    6   11   12]
 [   1    5   17    1    2    8  988   10    1    0    4    4    3    1    1    8    2    5    4   10   11]
 [   6    8   13    5    4   47   12  899    2    1    7   10    1    3    1    0    0    3   31   18    6]
 [  16    5    3    3    2    3    0    2  826   57   13    5    1   16   28    0    2    2   11    1    6]
 [ 103    5    4    2    2    2    1    0   37  803    1    0    1   18   12    0    0    3    2    0    5]
 [   0    0    9   22    1    5    5    3   21    4  944    2    4   13   11    1    0    1   12    2    4]
 [   6    3    1    0    2   23    3    7    5    1    1  860   28    9    0   12    8   21    0   15    6]
 [   1    0    2    6    1    5    1    2    4    0    3   79  821    4    1    7    3   36    7    5    7]
 [   3    2    2    1    4   15    1    0   13   25    9   12    1  875    6    4    5    6    3   10    4]
 [  13    3    3   20    7    3    0    0   36   15    2    2    6    5  950    2    4    5   13    1    8]
 [   2    1    4    4    6    4   11    1    0    2    1   17   16    2    0  947   12   20    2    1   13]
 [  10    9    5    2   15    7    0    1    3    1    4    7    9    5    1    8  963    1    3    4   14]
 [   4    2    1    2    1    4    2    1    0    3    0   31   30    3    3   14    0  895    1    3    5]
 [   4    4    5   16    2    4    3   26    4    0    4    4   10    1   23    2    1    1  934    2    8]
 [   2    5    2    1    4   11   19   10    0    2    0   30   10    6    1    7    4    4    3  957   10]
 [ 378  238  302  151  331  342  156  218  148  167  223  275  453  369  285  157  357  148  225  388 8621]]

2024-06-06 02:41:56,185 - ==> Best [Top1: 77.654   Top5: 96.108   Sparsity:0.00   Params: 169472 on epoch: 154]
2024-06-06 02:41:56,185 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:41:56,192 - 

2024-06-06 02:41:56,192 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:42:02,743 - Epoch: [162][  100/ 1218]    Overall Loss 0.507287    Objective Loss 0.507287                                        LR 0.000250    Time 0.065481    
2024-06-06 02:42:07,307 - Epoch: [162][  200/ 1218]    Overall Loss 0.512004    Objective Loss 0.512004                                        LR 0.000250    Time 0.055554    
2024-06-06 02:42:11,860 - Epoch: [162][  300/ 1218]    Overall Loss 0.512650    Objective Loss 0.512650                                        LR 0.000250    Time 0.052206    
2024-06-06 02:42:16,454 - Epoch: [162][  400/ 1218]    Overall Loss 0.514664    Objective Loss 0.514664                                        LR 0.000250    Time 0.050636    
2024-06-06 02:42:21,259 - Epoch: [162][  500/ 1218]    Overall Loss 0.513650    Objective Loss 0.513650                                        LR 0.000250    Time 0.050116    
2024-06-06 02:42:25,894 - Epoch: [162][  600/ 1218]    Overall Loss 0.513471    Objective Loss 0.513471                                        LR 0.000250    Time 0.049485    
2024-06-06 02:42:30,596 - Epoch: [162][  700/ 1218]    Overall Loss 0.512178    Objective Loss 0.512178                                        LR 0.000250    Time 0.049130    
2024-06-06 02:42:35,249 - Epoch: [162][  800/ 1218]    Overall Loss 0.511530    Objective Loss 0.511530                                        LR 0.000250    Time 0.048803    
2024-06-06 02:42:39,811 - Epoch: [162][  900/ 1218]    Overall Loss 0.512678    Objective Loss 0.512678                                        LR 0.000250    Time 0.048448    
2024-06-06 02:42:44,469 - Epoch: [162][ 1000/ 1218]    Overall Loss 0.512963    Objective Loss 0.512963                                        LR 0.000250    Time 0.048258    
2024-06-06 02:42:49,165 - Epoch: [162][ 1100/ 1218]    Overall Loss 0.513155    Objective Loss 0.513155                                        LR 0.000250    Time 0.048139    
2024-06-06 02:42:53,737 - Epoch: [162][ 1200/ 1218]    Overall Loss 0.512766    Objective Loss 0.512766                                        LR 0.000250    Time 0.047936    
2024-06-06 02:42:54,506 - Epoch: [162][ 1218/ 1218]    Overall Loss 0.512231    Objective Loss 0.512231    Top1 76.772616    Top5 97.310513    LR 0.000250    Time 0.047858    
2024-06-06 02:42:54,696 - --- validate (epoch=162)-----------
2024-06-06 02:42:54,696 - 34633 samples (256 per mini-batch)
2024-06-06 02:43:00,149 - Epoch: [162][  100/  136]    Loss 0.468746    Top1 77.652344    Top5 96.171875    
2024-06-06 02:43:01,809 - Epoch: [162][  136/  136]    Loss 0.471550    Top1 77.619611    Top5 96.185719    
2024-06-06 02:43:01,989 - ==> Top1: 77.620    Top5: 96.186    Loss: 0.472

2024-06-06 02:43:01,990 - ==> Confusion:
[[ 818    2    3    1   10    3    0    3    8   60    0    2    2    3    4    0    1    0    4    2    5]
 [   3  945    4    1   19   20    2   16    5    2    5    2    1    4    8    3    1    3   12    2    5]
 [  12    2  840    9    2    2   24   12    0    9    5    7    4    4    3    5    6    3    8    3   10]
 [   6    5   13  865    1   10    5    5    2    3   19    0    6    2   25    2    4    9   19    0   15]
 [  30   17    3    0  924   13    1    3    6   16    1    2    2    3   13    1    7    1    3    1    7]
 [   7   47    4    6   16  826    2   43    3    9    7   16    4   15    4    3    2    5    8    7    9]
 [   4    6   26    0    0    6  991    7    2    2    2   10    2    2    1    4    2    2    5    8    4]
 [   7   18    9    2    0   26    4  923    2    1    5   14    7    2    1    0    3    4   22   12   15]
 [  14    5    0    0    2    1    0    3  857   54    8    1    6   10   22    0    3    1    8    2    5]
 [  88    0    1    0    8    4    0    3   55  807    0    2    2   12    3    0    0    4    2    1    9]
 [   1    5   11   15    2    1    3    8   27    4  922    1    3   14   22    0    2    0   19    0    4]
 [   3    4    0    4    1   11    8   10    1    2    1  854   29    9    0    7    3   34    1   19   10]
 [   0    2    1   11    0    3    2    9    2    0    3   62  807    1    4    4    4   47    3   10   20]
 [   5    2    1    0    6    8    1    5   17   33   12   11    1  880    3    0    0    7    1    2    6]
 [  20    3    1   12   11    0    0    2   34    9    2    2    3   11  948    0    1    7   22    1    9]
 [   7    1    6    3    2    4    8    0    0    1    0   15   13    1    0  956   11   18    1    9   10]
 [   5   11    2    2   15   11    0    2   12    4    1    7    5    3    2    9  946    3    7    6   19]
 [   4    2    1    1    2    1    2    1    5    2    0   16   20    2    3    5    2  925    0    2    9]
 [   3   13    4   14    6    5    0   30   10    1    7    3    2    1   18    1    1    3  926    2    8]
 [   3    7    1    0    5   15   17   18    0    0    2   29    9    6    0    7    6    4    4  940   15]
 [ 298  332  252  164  238  231  136  258  163  190  177  235  435  305  293  141  285  207  259  352 8981]]

2024-06-06 02:43:01,992 - ==> Best [Top1: 77.654   Top5: 96.108   Sparsity:0.00   Params: 169472 on epoch: 154]
2024-06-06 02:43:01,992 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:43:02,000 - 

2024-06-06 02:43:02,000 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:43:08,488 - Epoch: [163][  100/ 1218]    Overall Loss 0.508684    Objective Loss 0.508684                                        LR 0.000250    Time 0.064851    
2024-06-06 02:43:13,437 - Epoch: [163][  200/ 1218]    Overall Loss 0.509035    Objective Loss 0.509035                                        LR 0.000250    Time 0.057162    
2024-06-06 02:43:18,096 - Epoch: [163][  300/ 1218]    Overall Loss 0.512407    Objective Loss 0.512407                                        LR 0.000250    Time 0.053633    
2024-06-06 02:43:22,906 - Epoch: [163][  400/ 1218]    Overall Loss 0.512124    Objective Loss 0.512124                                        LR 0.000250    Time 0.052245    
2024-06-06 02:43:27,700 - Epoch: [163][  500/ 1218]    Overall Loss 0.513224    Objective Loss 0.513224                                        LR 0.000250    Time 0.051381    
2024-06-06 02:43:32,521 - Epoch: [163][  600/ 1218]    Overall Loss 0.511716    Objective Loss 0.511716                                        LR 0.000250    Time 0.050849    
2024-06-06 02:43:37,212 - Epoch: [163][  700/ 1218]    Overall Loss 0.511039    Objective Loss 0.511039                                        LR 0.000250    Time 0.050284    
2024-06-06 02:43:41,918 - Epoch: [163][  800/ 1218]    Overall Loss 0.511783    Objective Loss 0.511783                                        LR 0.000250    Time 0.049878    
2024-06-06 02:43:46,780 - Epoch: [163][  900/ 1218]    Overall Loss 0.511607    Objective Loss 0.511607                                        LR 0.000250    Time 0.049736    
2024-06-06 02:43:51,635 - Epoch: [163][ 1000/ 1218]    Overall Loss 0.510651    Objective Loss 0.510651                                        LR 0.000250    Time 0.049616    
2024-06-06 02:43:56,456 - Epoch: [163][ 1100/ 1218]    Overall Loss 0.510652    Objective Loss 0.510652                                        LR 0.000250    Time 0.049486    
2024-06-06 02:44:01,114 - Epoch: [163][ 1200/ 1218]    Overall Loss 0.511597    Objective Loss 0.511597                                        LR 0.000250    Time 0.049242    
2024-06-06 02:44:01,905 - Epoch: [163][ 1218/ 1218]    Overall Loss 0.511890    Objective Loss 0.511890    Top1 77.261614    Top5 95.354523    LR 0.000250    Time 0.049164    
2024-06-06 02:44:02,100 - --- validate (epoch=163)-----------
2024-06-06 02:44:02,101 - 34633 samples (256 per mini-batch)
2024-06-06 02:44:07,587 - Epoch: [163][  100/  136]    Loss 0.472561    Top1 77.289062    Top5 95.968750    
2024-06-06 02:44:09,287 - Epoch: [163][  136/  136]    Loss 0.469365    Top1 77.284671    Top5 96.064447    
2024-06-06 02:44:09,487 - ==> Top1: 77.285    Top5: 96.064    Loss: 0.469

2024-06-06 02:44:09,488 - ==> Confusion:
[[ 806    1    8    0   12    0    2    2    7   65    2    1    1    6    7    0    4    0    2    0    5]
 [   2  928    3    2   24   19    2   23    1    1    4    2    3    0    6    2    6    1   18    3   13]
 [   5    3  867    6    5    3   18    5    2    4    5    3    4    6    1    2    4    2    9    6   10]
 [   3    2   28  873    4    6    3    2    4    2   18    1    6    4   26    3    2    3   14    1   11]
 [  26    7    7    1  940    6    0    3    5   12    0    2    2    4    7    5    8    1    7    1   10]
 [   6   35    6    5   23  843    0   36    6    6    2   14    6   20    2    2    6    3    5    8    9]
 [   5    4   24    0    1    6  974    9    1    0    8    2    4    3    0   17    5    3    2   12    6]
 [   1   17   10    0    5   38    8  906    2    1    8    7    6    1    1    2    4    3   33   12   12]
 [  11    4    1    1    4    0    1    1  839   53   19    0    2   18   21    1    1    2   15    0    8]
 [  90    1    4    0    6    1    0    2   64  793    2    1    1   17    7    1    2    2    1    1    5]
 [   2    4   11    6    0    3    0    7   17    2  967    0    2   13    7    0    1    0   15    1    6]
 [   3    2    3    1    3   21    1    9    0    2    0  841   44   14    0   15    7   17    1   20    7]
 [   0    1    5    8    1    5    2    8    0    1    1   63  824    4    4    5    5   32   12    5    9]
 [   2    1    0    1    7   10    0    7   13   23    5    9    4  886    2    6    5    3    1    6   10]
 [   8    3    5   11    9    1    0    0   43   10   12    1    4   10  951    2    2    0   16    0   10]
 [   2    4    3    0    6    2    6    0    0    2    0   13   10    3    1  975   13   14    1    1   10]
 [   3   13    8    3   12    7    1    0    9    0    4    6    3    5    2   10  962    4    5    3   12]
 [   4    1    1    5    0    1    1    5    1    2    0   23   35    7    4   12    3  889    1    3    7]
 [   3    8    7   14    5    2    2   21    6    4   11    2    3    2   23    1    2    1  925    2   14]
 [   2    7    5    1    1   11   13   24    1    1    0   23   15    4    0    7    9    1    8  938   17]
 [ 281  249  344  135  365  232  129  243  163  176  237  197  428  353  248  201  339  135  286  352 8839]]

2024-06-06 02:44:09,490 - ==> Best [Top1: 77.654   Top5: 96.108   Sparsity:0.00   Params: 169472 on epoch: 154]
2024-06-06 02:44:09,490 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:44:09,502 - 

2024-06-06 02:44:09,502 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:44:15,919 - Epoch: [164][  100/ 1218]    Overall Loss 0.519859    Objective Loss 0.519859                                        LR 0.000250    Time 0.064144    
2024-06-06 02:44:20,505 - Epoch: [164][  200/ 1218]    Overall Loss 0.516124    Objective Loss 0.516124                                        LR 0.000250    Time 0.054995    
2024-06-06 02:44:25,175 - Epoch: [164][  300/ 1218]    Overall Loss 0.517043    Objective Loss 0.517043                                        LR 0.000250    Time 0.052221    
2024-06-06 02:44:29,741 - Epoch: [164][  400/ 1218]    Overall Loss 0.516450    Objective Loss 0.516450                                        LR 0.000250    Time 0.050577    
2024-06-06 02:44:34,336 - Epoch: [164][  500/ 1218]    Overall Loss 0.517835    Objective Loss 0.517835                                        LR 0.000250    Time 0.049649    
2024-06-06 02:44:38,939 - Epoch: [164][  600/ 1218]    Overall Loss 0.518606    Objective Loss 0.518606                                        LR 0.000250    Time 0.049041    
2024-06-06 02:44:43,569 - Epoch: [164][  700/ 1218]    Overall Loss 0.516631    Objective Loss 0.516631                                        LR 0.000250    Time 0.048647    
2024-06-06 02:44:48,463 - Epoch: [164][  800/ 1218]    Overall Loss 0.516611    Objective Loss 0.516611                                        LR 0.000250    Time 0.048682    
2024-06-06 02:44:53,445 - Epoch: [164][  900/ 1218]    Overall Loss 0.517205    Objective Loss 0.517205                                        LR 0.000250    Time 0.048807    
2024-06-06 02:44:58,187 - Epoch: [164][ 1000/ 1218]    Overall Loss 0.515876    Objective Loss 0.515876                                        LR 0.000250    Time 0.048666    
2024-06-06 02:45:02,960 - Epoch: [164][ 1100/ 1218]    Overall Loss 0.515069    Objective Loss 0.515069                                        LR 0.000250    Time 0.048579    
2024-06-06 02:45:07,564 - Epoch: [164][ 1200/ 1218]    Overall Loss 0.515129    Objective Loss 0.515129                                        LR 0.000250    Time 0.048366    
2024-06-06 02:45:08,333 - Epoch: [164][ 1218/ 1218]    Overall Loss 0.515229    Objective Loss 0.515229    Top1 75.305623    Top5 96.332518    LR 0.000250    Time 0.048283    
2024-06-06 02:45:08,539 - --- validate (epoch=164)-----------
2024-06-06 02:45:08,539 - 34633 samples (256 per mini-batch)
2024-06-06 02:45:14,256 - Epoch: [164][  100/  136]    Loss 0.467834    Top1 76.683594    Top5 95.804688    
2024-06-06 02:45:15,846 - Epoch: [164][  136/  136]    Loss 0.474340    Top1 76.450206    Top5 95.795917    
2024-06-06 02:45:16,025 - ==> Top1: 76.450    Top5: 95.796    Loss: 0.474

2024-06-06 02:45:16,026 - ==> Confusion:
[[ 796    0    5    0   10    1    0    3   10   78    0    1    2    5    5    1    2    1    4    0    7]
 [   5  912    8    3   24   33    4   21    2    2    2    5    3    2    6    1    6    1   16    2    5]
 [  10    3  839   10    3    2   28   18    1    7    6    5    4    5    0    6    4    1    8    3    7]
 [   6    1   15  879    1    5    6    4    1    0   28    2    8    3   28    2    0   10    8    2    7]
 [  26   11    3    1  932    5    3    3    2   18    1    7    4    4    9    5    9    2    4    2    3]
 [   3   38    1    5   15  847    4   42    5    5    6   12    4   23    3    2    4    4    4    9    7]
 [   3    4   24    0    1    5 1009    5    0    0    3    5    3    0    2    7    3    1    2    3    6]
 [   7   10   10    1    1   29    9  918    1    4    9   10    6    2    0    1    2    4   34   11    8]
 [  10    2    1    0    0    3    0    0  833   57   22    1    6   20   22    0    5    3   10    0    7]
 [  72    2    2    0    3    4    0    1   55  810    3    1    1   20   17    1    0    2    2    0    5]
 [   1    2   10    8    4    3    7    5   16    3  959    1    0    7    8    0    5    0   15    4    6]
 [   4    1    3    0    1   23    5    7    2    3    0  851   30   15    1   15    8   14    2   22    4]
 [   0    0    2    4    1    4    1    6    2    1    1   72  828    3    2    7    5   31   12    9    4]
 [   4    4    5    1    4   13    2    3   22   31    9    7    6  859    6    2    2    5    3    7    6]
 [  13    2    5   16   10    2    0    1   35    8    8    1    6    5  958    0    5    2   15    0    6]
 [   1    1    4    3    7    2   11    2    0    0    0   26    9    5    0  955   10   17    2    5    6]
 [   3    8    5    1   11    7    2    0    5    1    5    8    4    2    1    9  972    2    4   10   12]
 [   3    0    0    3    0    3    3    4    2    4    0   18   29    9    8   14    2  890    4    3    6]
 [   1    9    7   12    7    1    2   25    9    1   11    2    4    0   17    0    1    1  942    2    4]
 [   1    6    4    0    3   10   19   14    3    0    1   23    7   10    0    5   11    4   10  952    5]
 [ 303  260  333  158  292  254  184  240  178  190  252  205  440  380  336  186  366  159  295  385 8536]]

2024-06-06 02:45:16,028 - ==> Best [Top1: 77.654   Top5: 96.108   Sparsity:0.00   Params: 169472 on epoch: 154]
2024-06-06 02:45:16,028 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:45:16,036 - 

2024-06-06 02:45:16,037 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:45:22,101 - Epoch: [165][  100/ 1218]    Overall Loss 0.509236    Objective Loss 0.509236                                        LR 0.000250    Time 0.060627    
2024-06-06 02:45:26,733 - Epoch: [165][  200/ 1218]    Overall Loss 0.512513    Objective Loss 0.512513                                        LR 0.000250    Time 0.053462    
2024-06-06 02:45:31,264 - Epoch: [165][  300/ 1218]    Overall Loss 0.511850    Objective Loss 0.511850                                        LR 0.000250    Time 0.050739    
2024-06-06 02:45:35,860 - Epoch: [165][  400/ 1218]    Overall Loss 0.512714    Objective Loss 0.512714                                        LR 0.000250    Time 0.049540    
2024-06-06 02:45:40,585 - Epoch: [165][  500/ 1218]    Overall Loss 0.512926    Objective Loss 0.512926                                        LR 0.000250    Time 0.049077    
2024-06-06 02:45:45,309 - Epoch: [165][  600/ 1218]    Overall Loss 0.514695    Objective Loss 0.514695                                        LR 0.000250    Time 0.048768    
2024-06-06 02:45:49,877 - Epoch: [165][  700/ 1218]    Overall Loss 0.515109    Objective Loss 0.515109                                        LR 0.000250    Time 0.048325    
2024-06-06 02:45:54,417 - Epoch: [165][  800/ 1218]    Overall Loss 0.513816    Objective Loss 0.513816                                        LR 0.000250    Time 0.047957    
2024-06-06 02:45:59,048 - Epoch: [165][  900/ 1218]    Overall Loss 0.513287    Objective Loss 0.513287                                        LR 0.000250    Time 0.047772    
2024-06-06 02:46:03,796 - Epoch: [165][ 1000/ 1218]    Overall Loss 0.513488    Objective Loss 0.513488                                        LR 0.000250    Time 0.047739    
2024-06-06 02:46:08,440 - Epoch: [165][ 1100/ 1218]    Overall Loss 0.512520    Objective Loss 0.512520                                        LR 0.000250    Time 0.047618    
2024-06-06 02:46:13,141 - Epoch: [165][ 1200/ 1218]    Overall Loss 0.512334    Objective Loss 0.512334                                        LR 0.000250    Time 0.047566    
2024-06-06 02:46:13,906 - Epoch: [165][ 1218/ 1218]    Overall Loss 0.512828    Objective Loss 0.512828    Top1 73.838631    Top5 95.110024    LR 0.000250    Time 0.047490    
2024-06-06 02:46:14,085 - --- validate (epoch=165)-----------
2024-06-06 02:46:14,085 - 34633 samples (256 per mini-batch)
2024-06-06 02:46:20,004 - Epoch: [165][  100/  136]    Loss 0.473743    Top1 77.628906    Top5 96.234375    
2024-06-06 02:46:21,678 - Epoch: [165][  136/  136]    Loss 0.472869    Top1 77.694684    Top5 96.197269    
2024-06-06 02:46:21,857 - ==> Top1: 77.695    Top5: 96.197    Loss: 0.473

2024-06-06 02:46:21,858 - ==> Confusion:
[[ 817    0    3    0   13    1    1    1    5   58    1    4    0    4    6    1    2    1    4    0    9]
 [   3  926    5    1   29   19    2   15    5    1    3    5    2    0   10    1    8    3   15    1    9]
 [   7    3  850    9    2    9   16   15    1    6    3    2    4    4    7    7    4    0    7    0   14]
 [   4    5   16  870    4    5    2    2    5    2   12    2    7    2   37    1    1    9   25    0    5]
 [  39    8    3    1  929    8    4    0    0   10    2    4    0    3   14    8    3    2    4    1   11]
 [   8   50    2    6   21  818    6   41    2    7    0   14    8   22    4    1    9    4    5   11    4]
 [   4    8   43    1    5    8  957   11    2    1    4    5    2    2    2    8    2    4    1    6   10]
 [   4   25    9    3    2   30    3  901    3    3    1    6   10    0    3    1    1    2   45   15   10]
 [  15    6    2    1    1    2    0    2  815   64   18    1    3   13   32    2    6    3    5    0   11]
 [  92    2    5    0    9    1    0    1   41  799    2    0    1   14   13    1    1    3    2    3   11]
 [   1   13    9    7    2    5    5    9   13    3  921    3    1   14   20    0    1    2   16    1   18]
 [   0    4    2    0    1   12    2    9    2    3    1  853   29    8    2   13    7   30    5   16   12]
 [   2    1    5    7    2    2    3    9    3    0    4   62  800    4    3   13    4   42    8   10   11]
 [   3    2    5    1    4    8    0    5   22   30   17   11    5  847    9    2    8    7    2    6    7]
 [  15    1    2   13    6    3    0    0   21   10    4    0    4    2  988    0    5    3   18    0    3]
 [   4    1    4    1    6    2    4    1    0    2    0    9    8    5    0  981    9   15    2    6    6]
 [   6    8    5    5    8    9    0    4    5    1    3    4    2    4    2   17  966    3    4    2   14]
 [   3    5    1    0    2    3    2    4    3    0    0   18   21    0    5   14    3  913    0    5    3]
 [   1    5    4   13    2    3    1   20    3    2    7    1    3    0   21    0    2    3  950    4   13]
 [   2   11    5    0    2    6   16   13    1    0    2   22    8    4    1    8    9    1    8  956   13]
 [ 295  270  254  150  305  192   95  247  132  157  211  185  424  292  336  180  339  168  288  361 9051]]

2024-06-06 02:46:21,860 - ==> Best [Top1: 77.695   Top5: 96.197   Sparsity:0.00   Params: 169472 on epoch: 165]
2024-06-06 02:46:21,860 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:46:21,875 - 

2024-06-06 02:46:21,875 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:46:28,010 - Epoch: [166][  100/ 1218]    Overall Loss 0.510447    Objective Loss 0.510447                                        LR 0.000250    Time 0.061324    
2024-06-06 02:46:32,870 - Epoch: [166][  200/ 1218]    Overall Loss 0.511167    Objective Loss 0.511167                                        LR 0.000250    Time 0.054954    
2024-06-06 02:46:37,585 - Epoch: [166][  300/ 1218]    Overall Loss 0.511512    Objective Loss 0.511512                                        LR 0.000250    Time 0.052344    
2024-06-06 02:46:42,163 - Epoch: [166][  400/ 1218]    Overall Loss 0.510326    Objective Loss 0.510326                                        LR 0.000250    Time 0.050700    
2024-06-06 02:46:46,742 - Epoch: [166][  500/ 1218]    Overall Loss 0.509866    Objective Loss 0.509866                                        LR 0.000250    Time 0.049714    
2024-06-06 02:46:51,454 - Epoch: [166][  600/ 1218]    Overall Loss 0.509467    Objective Loss 0.509467                                        LR 0.000250    Time 0.049278    
2024-06-06 02:46:56,068 - Epoch: [166][  700/ 1218]    Overall Loss 0.508272    Objective Loss 0.508272                                        LR 0.000250    Time 0.048828    
2024-06-06 02:47:00,634 - Epoch: [166][  800/ 1218]    Overall Loss 0.508054    Objective Loss 0.508054                                        LR 0.000250    Time 0.048428    
2024-06-06 02:47:05,180 - Epoch: [166][  900/ 1218]    Overall Loss 0.511458    Objective Loss 0.511458                                        LR 0.000250    Time 0.048097    
2024-06-06 02:47:09,734 - Epoch: [166][ 1000/ 1218]    Overall Loss 0.512950    Objective Loss 0.512950                                        LR 0.000250    Time 0.047840    
2024-06-06 02:47:14,431 - Epoch: [166][ 1100/ 1218]    Overall Loss 0.513521    Objective Loss 0.513521                                        LR 0.000250    Time 0.047759    
2024-06-06 02:47:19,145 - Epoch: [166][ 1200/ 1218]    Overall Loss 0.514208    Objective Loss 0.514208                                        LR 0.000250    Time 0.047706    
2024-06-06 02:47:19,927 - Epoch: [166][ 1218/ 1218]    Overall Loss 0.513995    Objective Loss 0.513995    Top1 74.327628    Top5 95.354523    LR 0.000250    Time 0.047643    
2024-06-06 02:47:20,104 - --- validate (epoch=166)-----------
2024-06-06 02:47:20,105 - 34633 samples (256 per mini-batch)
2024-06-06 02:47:25,602 - Epoch: [166][  100/  136]    Loss 0.468557    Top1 77.824219    Top5 96.140625    
2024-06-06 02:47:27,296 - Epoch: [166][  136/  136]    Loss 0.467006    Top1 77.824618    Top5 96.127970    
2024-06-06 02:47:27,491 - ==> Top1: 77.825    Top5: 96.128    Loss: 0.467

2024-06-06 02:47:27,492 - ==> Confusion:
[[ 796    0    4    1   13    1    0    3    2   77    0    4    2    1    7    2    4    3    4    1    6]
 [   3  922    5    3   26   25    1   19    7    1    4    2    2    0    5    2    4    2   16    2   12]
 [   8    4  833   12    5    4   24    9    0    7    7    1    4    7    6    5    3    2    7    7   15]
 [   3    2   23  873    2    8    3    1    2    2   15    1    4    4   33    4    5    7   16    0    8]
 [  27    8    2    1  939    6    2    2    2   17    0    3    4    5    6    5    7    3    6    3    6]
 [   3   27    5    4   21  851    4   35    4    4    1   12    5   30    4    3    8    1    2    8   11]
 [   2   10   31    1    2    6  958    7    0    3    7    4    3    3    1    7    2    6    3   19   11]
 [   6   15   11    2    4   31    3  906    4    1    6   13    7    6    0    0    1    4   27   20   10]
 [   9    3    0    2    0    1    0    0  818   72   11    4    6   23   34    0    4    3    5    2    5]
 [  69    0    1    0    4    2    0    0   42  826    0    2    1   29    7    1    0    5    3    3    6]
 [   0    7   13   12    2    4    1    6   14    7  949    1    3   16   12    0    0    0   10    0    7]
 [   4    2    4    0    3   12    1    4    1    2    0  856   38   15    0   18    3   12    4   22   10]
 [   4    0    3    9    1    2    1    7    3    1    2   72  811    5    1   11    4   37    3    8   10]
 [   2    1    0    0    6    7    0    4   11   23    6    8    5  903    6    3    2    6    1    1    6]
 [  13    5    1   14    8    1    0    1   27   12    6    2    3    6  973    2    2    2   12    0    8]
 [   4    1    4    0    3    2    8    0    0    1    0   17    6    3    2  973    6   20    2    5    9]
 [   5    8    1    3    6   10    0    1    6    0    1    7    4    5    1   20  955    3    3    9   24]
 [   3    1    1    4    0    1    3    2    2    5    0   31   27    3    6   17    4  884    0    3    8]
 [   1    6    5   11    1    3    2   26   10    3    5    1    2    0   22    0    2    3  941    5    9]
 [   0    5    1    3    1    9   16   16    4    0    1   15    7    6    0   12   11    6    4  962    9]
 [ 246  243  255  122  287  217  106  210  147  218  197  219  397  421  282  207  342  160  233  398 9025]]

2024-06-06 02:47:27,494 - ==> Best [Top1: 77.825   Top5: 96.128   Sparsity:0.00   Params: 169472 on epoch: 166]
2024-06-06 02:47:27,494 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:47:27,509 - 

2024-06-06 02:47:27,509 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:47:33,575 - Epoch: [167][  100/ 1218]    Overall Loss 0.519001    Objective Loss 0.519001                                        LR 0.000250    Time 0.060639    
2024-06-06 02:47:38,370 - Epoch: [167][  200/ 1218]    Overall Loss 0.507794    Objective Loss 0.507794                                        LR 0.000250    Time 0.054284    
2024-06-06 02:47:43,205 - Epoch: [167][  300/ 1218]    Overall Loss 0.509463    Objective Loss 0.509463                                        LR 0.000250    Time 0.052301    
2024-06-06 02:47:47,813 - Epoch: [167][  400/ 1218]    Overall Loss 0.506380    Objective Loss 0.506380                                        LR 0.000250    Time 0.050740    
2024-06-06 02:47:52,369 - Epoch: [167][  500/ 1218]    Overall Loss 0.506667    Objective Loss 0.506667                                        LR 0.000250    Time 0.049700    
2024-06-06 02:47:56,954 - Epoch: [167][  600/ 1218]    Overall Loss 0.509011    Objective Loss 0.509011                                        LR 0.000250    Time 0.049056    
2024-06-06 02:48:01,731 - Epoch: [167][  700/ 1218]    Overall Loss 0.509593    Objective Loss 0.509593                                        LR 0.000250    Time 0.048870    
2024-06-06 02:48:06,492 - Epoch: [167][  800/ 1218]    Overall Loss 0.510614    Objective Loss 0.510614                                        LR 0.000250    Time 0.048711    
2024-06-06 02:48:11,376 - Epoch: [167][  900/ 1218]    Overall Loss 0.509349    Objective Loss 0.509349                                        LR 0.000250    Time 0.048723    
2024-06-06 02:48:16,320 - Epoch: [167][ 1000/ 1218]    Overall Loss 0.509217    Objective Loss 0.509217                                        LR 0.000250    Time 0.048794    
2024-06-06 02:48:21,030 - Epoch: [167][ 1100/ 1218]    Overall Loss 0.508920    Objective Loss 0.508920                                        LR 0.000250    Time 0.048637    
2024-06-06 02:48:25,589 - Epoch: [167][ 1200/ 1218]    Overall Loss 0.509955    Objective Loss 0.509955                                        LR 0.000250    Time 0.048382    
2024-06-06 02:48:26,470 - Epoch: [167][ 1218/ 1218]    Overall Loss 0.509885    Objective Loss 0.509885    Top1 74.327628    Top5 95.843521    LR 0.000250    Time 0.048390    
2024-06-06 02:48:26,668 - --- validate (epoch=167)-----------
2024-06-06 02:48:26,668 - 34633 samples (256 per mini-batch)
2024-06-06 02:48:32,226 - Epoch: [167][  100/  136]    Loss 0.478178    Top1 77.484375    Top5 95.898438    
2024-06-06 02:48:33,898 - Epoch: [167][  136/  136]    Loss 0.477886    Top1 77.501227    Top5 95.868103    
2024-06-06 02:48:34,075 - ==> Top1: 77.501    Top5: 95.868    Loss: 0.478

2024-06-06 02:48:34,076 - ==> Confusion:
[[ 790    2    5    1   10    2    1    1    7   79    1    2    2    3    8    1    2    3    4    0    7]
 [   2  935    4    1   18   23    1   12    7    0    2    6    1    2    9    2    4    3   16    4   11]
 [   5    3  845    8    4    3   26   10    0    9    9    5    4    3    2    9    4    0    5    4   12]
 [   7    5   24  860    1    5    6    3    1    0   16    3    4    2   44    1    1    7   11    4   11]
 [  24   11    8    1  933    3    2    3    4   15    1    2    3    5   13    6    4    2    4    4    6]
 [   6   43    7    7   20  822    3   29    3   10    4   17    7   20    4    5    4    5    6    9   12]
 [   2    3   25    4    3    2  987    8    2    0    2    2    3    2    0    6    3    3    5   15    9]
 [   3   19   21    2    2   36    5  887    2    0    9   13    6    0    0    0    1    2   43   20    6]
 [  14    6    0    1    1    0    0    2  831   48   18    4    2   12   35    1    3    3   12    2    7]
 [  76    1    0    0    8    1    0    1   63  802    4    2    0   13   11    3    1    1    4    0   10]
 [   0    4    6   12    2    1    7    6   19    0  953    3    2    9   12    0    0    1   14    5    8]
 [   3    2    4    0    0    8    4    3    0    1    2  863   30   10    1   16    5   29    4   19    7]
 [   0    2    1    7    2    5    3    3    2    0    1   68  809    4    5   11    3   41   10    8   10]
 [   5    2    1    2    6    3    1    3   29   22   17   12    5  849    8    2    5    9    5    8    7]
 [   4    5    3   16   14    2    2    0   34    7    4    3    3    8  965    0    2    0   16    1    9]
 [   3    2    5    0    2    0    9    0    2    1    0   23   14    3    0  968    8   17    0    1    8]
 [   1   17    6    0    8    4    2    1    4    4    2    8    4    7    6   13  957    4    4   10   10]
 [   0    6    1    3    1    1    1    2    2    2    0   23   32    6    4   11    2  900    1    0    7]
 [   1   15   10   13    3    3    1   16    7    1    8    2    6    1   21    2    0    3  933    4    8]
 [   2    5    3    1    0    4   15    9    0    1    3   21   16    6    0    8    7    3    5  965   14]
 [ 246  280  317  142  326  156  151  186  189  135  298  223  414  304  348  206  263  167  237  357 8987]]

2024-06-06 02:48:34,078 - ==> Best [Top1: 77.825   Top5: 96.128   Sparsity:0.00   Params: 169472 on epoch: 166]
2024-06-06 02:48:34,078 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:48:34,085 - 

2024-06-06 02:48:34,086 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:48:40,223 - Epoch: [168][  100/ 1218]    Overall Loss 0.499343    Objective Loss 0.499343                                        LR 0.000250    Time 0.061357    
2024-06-06 02:48:44,816 - Epoch: [168][  200/ 1218]    Overall Loss 0.501553    Objective Loss 0.501553                                        LR 0.000250    Time 0.053633    
2024-06-06 02:48:49,421 - Epoch: [168][  300/ 1218]    Overall Loss 0.506705    Objective Loss 0.506705                                        LR 0.000250    Time 0.051099    
2024-06-06 02:48:54,020 - Epoch: [168][  400/ 1218]    Overall Loss 0.510036    Objective Loss 0.510036                                        LR 0.000250    Time 0.049818    
2024-06-06 02:48:58,573 - Epoch: [168][  500/ 1218]    Overall Loss 0.512554    Objective Loss 0.512554                                        LR 0.000250    Time 0.048955    
2024-06-06 02:49:03,260 - Epoch: [168][  600/ 1218]    Overall Loss 0.511947    Objective Loss 0.511947                                        LR 0.000250    Time 0.048605    
2024-06-06 02:49:07,894 - Epoch: [168][  700/ 1218]    Overall Loss 0.509834    Objective Loss 0.509834                                        LR 0.000250    Time 0.048279    
2024-06-06 02:49:12,724 - Epoch: [168][  800/ 1218]    Overall Loss 0.510149    Objective Loss 0.510149                                        LR 0.000250    Time 0.048279    
2024-06-06 02:49:17,651 - Epoch: [168][  900/ 1218]    Overall Loss 0.508940    Objective Loss 0.508940                                        LR 0.000250    Time 0.048387    
2024-06-06 02:49:22,172 - Epoch: [168][ 1000/ 1218]    Overall Loss 0.510011    Objective Loss 0.510011                                        LR 0.000250    Time 0.048067    
2024-06-06 02:49:26,750 - Epoch: [168][ 1100/ 1218]    Overall Loss 0.510931    Objective Loss 0.510931                                        LR 0.000250    Time 0.047858    
2024-06-06 02:49:31,372 - Epoch: [168][ 1200/ 1218]    Overall Loss 0.510555    Objective Loss 0.510555                                        LR 0.000250    Time 0.047719    
2024-06-06 02:49:32,170 - Epoch: [168][ 1218/ 1218]    Overall Loss 0.510611    Objective Loss 0.510611    Top1 77.261614    Top5 97.310513    LR 0.000250    Time 0.047670    
2024-06-06 02:49:32,332 - --- validate (epoch=168)-----------
2024-06-06 02:49:32,332 - 34633 samples (256 per mini-batch)
2024-06-06 02:49:37,792 - Epoch: [168][  100/  136]    Loss 0.467809    Top1 76.921875    Top5 96.066406    
2024-06-06 02:49:39,544 - Epoch: [168][  136/  136]    Loss 0.466500    Top1 77.091214    Top5 95.992262    
2024-06-06 02:49:39,741 - ==> Top1: 77.091    Top5: 95.992    Loss: 0.466

2024-06-06 02:49:39,742 - ==> Confusion:
[[ 799    0    0    1   13    1    0    2   10   78    0    2    4    2    5    3    3    3    1    0    4]
 [   2  944    3    2   17   21    4   13    1    3    2    5    4    0    3    2   12    1    9    1   14]
 [   4    6  841    8    3    2   28   17    1    7    5    5    5    4    1    3    5    1    8    5   11]
 [   3    4   17  860    3    5    6    5    4    3   18    0   10    3   27    1    0    9   24    1   13]
 [  25    9    2    1  930   10    0    1    2   20    1    3    2    6   10    7   10    1   11    0    3]
 [   5   36    3    3   18  834    2   40    5    2    2   15    9   23    4    6    8    1    4   11   12]
 [   1    4   22    2    2    7  996    8    1    0    3    4    2    1    1    6    2    1    2   11   10]
 [   3   12   11    4    4   43    3  913    2    1    6   10    5    3    2    3    1    2   28   14    7]
 [   9    4    1    2    0    0    0    1  835   52   21    2    4   16   32    0    2    3   12    1    5]
 [  74    0    2    0    5    3    1    4   64  804    1    1    0   22    5    0    1    2    3    1    8]
 [   3    3   10   16    3    4    3    6   17    2  943    1    2   13   12    0    1    1   14    1    9]
 [   2    2    1    0    0   12    4    9    2    1    0  857   38    5    0   13    4   19    6   25   11]
 [   3    2    2    5    1    4    2    9    4    1    2   72  808    5    3    7    5   31   10    7   12]
 [   3    0    3    6    4    9    2    6   18   24   10    7    7  866    9    1    2    7    0    6   11]
 [   9    9    2   14    9    4    1    2   28    7    8    2    5    5  961    0    0    2   19    2    9]
 [   0    1    6    1    5    1    9    2    0    1    0   23   10    1    0  969   10   11    1    4   11]
 [   6   10    5    2    7    7    1    4    5    0    2   10    6    3    0   11  962    6    1    9   15]
 [   0    0    4    0    1    1    1    2    1    3    2   16   29    4    4   10    3  915    0    3    6]
 [   3    6    6   11    3    2    7   27    5    1    2    3    4    0   22    0    1    0  938    5   12]
 [   1    4    5    1    4    9   11   20    1    1    0   19    7    7    0    5    4    3    5  972    9]
 [ 217  330  246  147  318  242  139  279  163  193  218  218  435  375  309  190  323  157  275  406 8752]]

2024-06-06 02:49:39,744 - ==> Best [Top1: 77.825   Top5: 96.128   Sparsity:0.00   Params: 169472 on epoch: 166]
2024-06-06 02:49:39,744 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:49:39,752 - 

2024-06-06 02:49:39,752 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:49:45,908 - Epoch: [169][  100/ 1218]    Overall Loss 0.500871    Objective Loss 0.500871                                        LR 0.000250    Time 0.061535    
2024-06-06 02:49:50,626 - Epoch: [169][  200/ 1218]    Overall Loss 0.500948    Objective Loss 0.500948                                        LR 0.000250    Time 0.054351    
2024-06-06 02:49:55,190 - Epoch: [169][  300/ 1218]    Overall Loss 0.505202    Objective Loss 0.505202                                        LR 0.000250    Time 0.051440    
2024-06-06 02:49:59,883 - Epoch: [169][  400/ 1218]    Overall Loss 0.509197    Objective Loss 0.509197                                        LR 0.000250    Time 0.050309    
2024-06-06 02:50:04,438 - Epoch: [169][  500/ 1218]    Overall Loss 0.510095    Objective Loss 0.510095                                        LR 0.000250    Time 0.049354    
2024-06-06 02:50:09,248 - Epoch: [169][  600/ 1218]    Overall Loss 0.510073    Objective Loss 0.510073                                        LR 0.000250    Time 0.049142    
2024-06-06 02:50:13,928 - Epoch: [169][  700/ 1218]    Overall Loss 0.510367    Objective Loss 0.510367                                        LR 0.000250    Time 0.048804    
2024-06-06 02:50:18,496 - Epoch: [169][  800/ 1218]    Overall Loss 0.512339    Objective Loss 0.512339                                        LR 0.000250    Time 0.048412    
2024-06-06 02:50:23,131 - Epoch: [169][  900/ 1218]    Overall Loss 0.511604    Objective Loss 0.511604                                        LR 0.000250    Time 0.048181    
2024-06-06 02:50:27,788 - Epoch: [169][ 1000/ 1218]    Overall Loss 0.510596    Objective Loss 0.510596                                        LR 0.000250    Time 0.048018    
2024-06-06 02:50:32,370 - Epoch: [169][ 1100/ 1218]    Overall Loss 0.509975    Objective Loss 0.509975                                        LR 0.000250    Time 0.047816    
2024-06-06 02:50:36,984 - Epoch: [169][ 1200/ 1218]    Overall Loss 0.509768    Objective Loss 0.509768                                        LR 0.000250    Time 0.047676    
2024-06-06 02:50:37,830 - Epoch: [169][ 1218/ 1218]    Overall Loss 0.509785    Objective Loss 0.509785    Top1 76.528117    Top5 96.088020    LR 0.000250    Time 0.047665    
2024-06-06 02:50:38,032 - --- validate (epoch=169)-----------
2024-06-06 02:50:38,033 - 34633 samples (256 per mini-batch)
2024-06-06 02:50:43,622 - Epoch: [169][  100/  136]    Loss 0.467761    Top1 77.136719    Top5 96.007812    
2024-06-06 02:50:45,304 - Epoch: [169][  136/  136]    Loss 0.466767    Top1 77.174949    Top5 96.052898    
2024-06-06 02:50:45,502 - ==> Top1: 77.175    Top5: 96.053    Loss: 0.467

2024-06-06 02:50:45,503 - ==> Confusion:
[[ 798    0    0    0   22    2    2    3    5   67    0    3    2    5    3    4    2    1    4    0    8]
 [   2  922    3    2   20   28    2   24    0    2    7    5    1    0    5    1    9    0   18    3    9]
 [   2    7  835   12   11    1   21   13    0    6    4    3    6    4    6    5    5    1    8    6   14]
 [   5    1   11  875    5    6    3    5    2    2   20    1    7    5   34    3    1    8   15    0    7]
 [  13    5    1    3  957   10    0    1    2   10    3    5    0    0    9    5    8    1   10    3    8]
 [   3   37    3    1   19  855    3   39    6    2    0   24    6   11    3    3    5    2    7    6    8]
 [   5    8   17    1    5    9  988    7    0    0    3    4    1    1    0    8    2    6    3   13    5]
 [   3   17   15    1    1   31    4  905    2    5    6   13    2    1    2    2    2    3   37   20    5]
 [  11    7    1    2    4    6    1    2  805   61   15    2    6   25   29    2    0    0   14    0    9]
 [  80    1    3    1   11    4    1    3   38  820    1    1    1   15    5    3    0    3    4    2    4]
 [   3    4   13   25    2    6    5    8   15    3  924    0    3   11   10    0    1    1   21    3    6]
 [   2    4    3    1    2   10    3    3    0    0    1  877   29    6    0   21    3   22    1   17    6]
 [   0    2    3    5    0    4    2    5    2    0    4   66  820    2    2    6    2   47   13    4    6]
 [   5    1    3    0    9   17    1    7   13   24    9    7    9  860    7    6    2    2    2    7   10]
 [  10    4    2   12   17    0    0    1   28    9    5    1    3    8  959    1    2    3   26    0    7]
 [   0    1    4    0    5    2   11    0    0    0    0   16   12    1    0  977   10   13    3    5    6]
 [   2   15    3    2    6    6    4    1    5    1    2    9    4    1    4   13  959    2    4    8   21]
 [   3    2    1    1    0    2    1    0    1    3    0   22   25    4    3    9    1  913    4    1    9]
 [   5    6    6   17    5    2    1   19    1    0    2    3    5    0   14    0    2    0  961    4    5]
 [   0    6    1    0    1   10   14   12    2    3    0   20   11    4    1   12   11    3    9  958   10]
 [ 240  307  246  153  341  219  125  254  122  165  180  263  460  305  309  237  325  200  337  384 8760]]

2024-06-06 02:50:45,506 - ==> Best [Top1: 77.825   Top5: 96.128   Sparsity:0.00   Params: 169472 on epoch: 166]
2024-06-06 02:50:45,506 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:50:45,514 - 

2024-06-06 02:50:45,515 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:50:51,922 - Epoch: [170][  100/ 1218]    Overall Loss 0.510693    Objective Loss 0.510693                                        LR 0.000250    Time 0.064053    
2024-06-06 02:50:56,983 - Epoch: [170][  200/ 1218]    Overall Loss 0.511404    Objective Loss 0.511404                                        LR 0.000250    Time 0.057320    
2024-06-06 02:51:02,033 - Epoch: [170][  300/ 1218]    Overall Loss 0.512528    Objective Loss 0.512528                                        LR 0.000250    Time 0.055043    
2024-06-06 02:51:06,856 - Epoch: [170][  400/ 1218]    Overall Loss 0.510601    Objective Loss 0.510601                                        LR 0.000250    Time 0.053335    
2024-06-06 02:51:11,428 - Epoch: [170][  500/ 1218]    Overall Loss 0.511652    Objective Loss 0.511652                                        LR 0.000250    Time 0.051797    
2024-06-06 02:51:16,093 - Epoch: [170][  600/ 1218]    Overall Loss 0.510220    Objective Loss 0.510220                                        LR 0.000250    Time 0.050936    
2024-06-06 02:51:20,763 - Epoch: [170][  700/ 1218]    Overall Loss 0.510922    Objective Loss 0.510922                                        LR 0.000250    Time 0.050328    
2024-06-06 02:51:25,500 - Epoch: [170][  800/ 1218]    Overall Loss 0.511076    Objective Loss 0.511076                                        LR 0.000250    Time 0.049956    
2024-06-06 02:51:30,157 - Epoch: [170][  900/ 1218]    Overall Loss 0.511336    Objective Loss 0.511336                                        LR 0.000250    Time 0.049577    
2024-06-06 02:51:34,917 - Epoch: [170][ 1000/ 1218]    Overall Loss 0.510016    Objective Loss 0.510016                                        LR 0.000250    Time 0.049377    
2024-06-06 02:51:39,580 - Epoch: [170][ 1100/ 1218]    Overall Loss 0.510632    Objective Loss 0.510632                                        LR 0.000250    Time 0.049126    
2024-06-06 02:51:44,217 - Epoch: [170][ 1200/ 1218]    Overall Loss 0.510123    Objective Loss 0.510123                                        LR 0.000250    Time 0.048895    
2024-06-06 02:51:45,036 - Epoch: [170][ 1218/ 1218]    Overall Loss 0.510627    Objective Loss 0.510627    Top1 74.816626    Top5 96.577017    LR 0.000250    Time 0.048845    
2024-06-06 02:51:45,221 - --- validate (epoch=170)-----------
2024-06-06 02:51:45,221 - 34633 samples (256 per mini-batch)
2024-06-06 02:51:50,676 - Epoch: [170][  100/  136]    Loss 0.472886    Top1 76.335938    Top5 95.605469    
2024-06-06 02:51:52,350 - Epoch: [170][  136/  136]    Loss 0.475298    Top1 76.256749    Top5 95.596685    
2024-06-06 02:51:52,540 - ==> Top1: 76.257    Top5: 95.597    Loss: 0.475

2024-06-06 02:51:52,542 - ==> Confusion:
[[ 812    1    2    0   15    4    0    3    5   56    0    2    3    3    8    2    5    1    2    1    6]
 [   1  904    3    1   20   43    5   23    5    2    6    5    1    2    4    4    3    2   15    5    9]
 [  10    2  830   10    4    4   23   10    1    5    4    8    4   10    6    9    7    6    8    3    6]
 [   4    1   19  851    3    6    1    8    1    4   15    2    9    8   37    5    4   17   18    0    3]
 [  21    9    5    0  949    8    0    5    1   13    1    4    0    3   13    2    6    3    4    2    5]
 [   4   14    1    5   14  877    0   27    2    3    4   26    7   22    1    5    7    5    7    9    3]
 [   2    2   23    1    2    6  983    9    2    0    1   11    3    2    1   12    5    3    4    9    5]
 [   6    5   11    2    3   45    6  912    4    1    4   17    5    4    1    0    4    3   25   16    3]
 [  17    3    1    0    2    1    0    1  785   59   13    5    3   31   47    0    5    5   15    0    9]
 [  99    3    5    1    8    4    1    2   36  773    1    2    3   33   18    1    1    2    4    0    4]
 [   1    6   13   10    1    8    6    7   16    1  923    0    2   22   18    0    2    0   18    4    6]
 [   2    1    2    0    3    9    1    7    0    1    0  895   26   10    3   12    3   21    1   10    4]
 [   2    1    0    2    0    3    1    2    0    0    1   77  833    5    6    8    3   33    8    2    8]
 [   0    0    2    0    2   18    1    5    5   17    4   21    2  895    9    3    2    4    1    3    7]
 [  13    2    1    8   12    3    0    3   12    6    7    3    5    6  985    1    4    4   12    3    8]
 [   4    0    3    0    1    0    9    1    0    1    0   19    8    2    0  978   13   12    2    3   10]
 [   5   10    6    0    9   12    1    1    5    0    2   13   10    8    1   13  961    2    2    5    6]
 [   0    1    3    3    0    1    2    2    2    2    0   20   36    3    6   10    2  905    1    3    3]
 [   1    8    7   11    4    7    0   21    5    2    3    2    8    1   30    0    0    4  936    5    3]
 [   3    3    6    0    1   17   21   10    1    0    1   44    9    7    2    9    5    4    7  931    7]
 [ 267  220  287  142  328  322  149  265  108  128  187  318  464  459  410  229  350  197  280  330 8492]]

2024-06-06 02:51:52,543 - ==> Best [Top1: 77.825   Top5: 96.128   Sparsity:0.00   Params: 169472 on epoch: 166]
2024-06-06 02:51:52,544 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:51:52,551 - 

2024-06-06 02:51:52,551 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:51:59,022 - Epoch: [171][  100/ 1218]    Overall Loss 0.517568    Objective Loss 0.517568                                        LR 0.000250    Time 0.064684    
2024-06-06 02:52:03,656 - Epoch: [171][  200/ 1218]    Overall Loss 0.512878    Objective Loss 0.512878                                        LR 0.000250    Time 0.055506    
2024-06-06 02:52:08,334 - Epoch: [171][  300/ 1218]    Overall Loss 0.518349    Objective Loss 0.518349                                        LR 0.000250    Time 0.052591    
2024-06-06 02:52:12,896 - Epoch: [171][  400/ 1218]    Overall Loss 0.518070    Objective Loss 0.518070                                        LR 0.000250    Time 0.050828    
2024-06-06 02:52:17,415 - Epoch: [171][  500/ 1218]    Overall Loss 0.516627    Objective Loss 0.516627                                        LR 0.000250    Time 0.049698    
2024-06-06 02:52:22,165 - Epoch: [171][  600/ 1218]    Overall Loss 0.517168    Objective Loss 0.517168                                        LR 0.000250    Time 0.049328    
2024-06-06 02:52:26,907 - Epoch: [171][  700/ 1218]    Overall Loss 0.515409    Objective Loss 0.515409                                        LR 0.000250    Time 0.049053    
2024-06-06 02:52:31,459 - Epoch: [171][  800/ 1218]    Overall Loss 0.515674    Objective Loss 0.515674                                        LR 0.000250    Time 0.048609    
2024-06-06 02:52:36,237 - Epoch: [171][  900/ 1218]    Overall Loss 0.515037    Objective Loss 0.515037                                        LR 0.000250    Time 0.048515    
2024-06-06 02:52:40,893 - Epoch: [171][ 1000/ 1218]    Overall Loss 0.515692    Objective Loss 0.515692                                        LR 0.000250    Time 0.048318    
2024-06-06 02:52:45,462 - Epoch: [171][ 1100/ 1218]    Overall Loss 0.515368    Objective Loss 0.515368                                        LR 0.000250    Time 0.048077    
2024-06-06 02:52:50,196 - Epoch: [171][ 1200/ 1218]    Overall Loss 0.514428    Objective Loss 0.514428                                        LR 0.000250    Time 0.048014    
2024-06-06 02:52:50,998 - Epoch: [171][ 1218/ 1218]    Overall Loss 0.513930    Objective Loss 0.513930    Top1 78.728606    Top5 93.643032    LR 0.000250    Time 0.047962    
2024-06-06 02:52:51,178 - --- validate (epoch=171)-----------
2024-06-06 02:52:51,178 - 34633 samples (256 per mini-batch)
2024-06-06 02:52:56,750 - Epoch: [171][  100/  136]    Loss 0.467630    Top1 77.667969    Top5 96.148438    
2024-06-06 02:52:58,530 - Epoch: [171][  136/  136]    Loss 0.469160    Top1 77.720671    Top5 96.208818    
2024-06-06 02:52:58,719 - ==> Top1: 77.721    Top5: 96.209    Loss: 0.469

2024-06-06 02:52:58,720 - ==> Confusion:
[[ 815    1    3    1   11    0    1    6    6   63    0    0    3    3    5    4    2    0    2    1    4]
 [   4  943    2    6   16   18    2   16    2    3    3    0    4    4    2    2    8    1   11    3   13]
 [   7    3  858    1    8    1   21    8    2    4   12    3    5    1    0    5    9    0    4    8   10]
 [   4    5   17  877    8    7    1    6    1    2   16    1    5    4   19    3    5    7   17    1   10]
 [  22   14    4    1  946    5    1    2    0   15    1    2    1    4    7    3   11    1    3    2    9]
 [   9   34    4    2   13  852    2   35    1   10    1   12    8   22    0    2    7    2    3   16    8]
 [   6    3   32    1    3    4  988    9    2    2    4    0    1    3    1    9    4    0    0   10    4]
 [   6   21   11    3    3   34    2  913    3    1    6    5    3    3    1    0    6    4   28   16    8]
 [   9    3    2    0    3    0    0    2  818   79   15    3    7   13   23    1    5    2    9    0    8]
 [  86    2    3    0   10    1    1    3   45  812    2    0    0   14    7    0    1    3    2    1    8]
 [   0    1   11   17    2    6    6    5   16    4  944    2    2   11   11    0    2    1   10    1   12]
 [   4    2    3    0    2   16    5   10    0    3    2  830   42   12    0   15    8   23    4   22    8]
 [   0    0    1    3    0    4    0   10    1    0    2   54  831    6    0    7   13   38    5    7   13]
 [   3    0    3    1    7   13    3    4   19   25    6    9    3  866    6    5    3    1    3   13    8]
 [  18    5    2   22   15    1    1    2   40   10    7    2    1    9  915    1    5    4   27    1   10]
 [   2    3    1    1    3    0    8    6    0    3    0   13    8    1    0  981   15    8    2    5    6]
 [   4   14    2    2   10    9    1    2    5    0    3    7    7    4    1   13  956    1    4    6   21]
 [   3    0    2    5    1    4    1    5    3    2    0   15   34    3    2   15    5  884    3    8   10]
 [   5    9    9   14    5    2    0   30    7    2    5    3    4    2   14    2    1    1  931    4    8]
 [   0    7    5    2    3    8   15   15    2    3    3   22    6    8    0    3   10    4    1  965    6]
 [ 287  311  311  133  319  198  125  234  167  179  223  187  419  337  190  156  406  128  221  409 8992]]

2024-06-06 02:52:58,722 - ==> Best [Top1: 77.825   Top5: 96.128   Sparsity:0.00   Params: 169472 on epoch: 166]
2024-06-06 02:52:58,722 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:52:58,730 - 

2024-06-06 02:52:58,730 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:53:05,142 - Epoch: [172][  100/ 1218]    Overall Loss 0.509078    Objective Loss 0.509078                                        LR 0.000250    Time 0.064106    
2024-06-06 02:53:10,018 - Epoch: [172][  200/ 1218]    Overall Loss 0.500830    Objective Loss 0.500830                                        LR 0.000250    Time 0.056416    
2024-06-06 02:53:14,625 - Epoch: [172][  300/ 1218]    Overall Loss 0.505427    Objective Loss 0.505427                                        LR 0.000250    Time 0.052960    
2024-06-06 02:53:19,633 - Epoch: [172][  400/ 1218]    Overall Loss 0.508674    Objective Loss 0.508674                                        LR 0.000250    Time 0.052237    
2024-06-06 02:53:24,618 - Epoch: [172][  500/ 1218]    Overall Loss 0.509856    Objective Loss 0.509856                                        LR 0.000250    Time 0.051755    
2024-06-06 02:53:29,482 - Epoch: [172][  600/ 1218]    Overall Loss 0.508911    Objective Loss 0.508911                                        LR 0.000250    Time 0.051234    
2024-06-06 02:53:34,322 - Epoch: [172][  700/ 1218]    Overall Loss 0.507229    Objective Loss 0.507229                                        LR 0.000250    Time 0.050827    
2024-06-06 02:53:39,157 - Epoch: [172][  800/ 1218]    Overall Loss 0.508371    Objective Loss 0.508371                                        LR 0.000250    Time 0.050515    
2024-06-06 02:53:44,126 - Epoch: [172][  900/ 1218]    Overall Loss 0.510722    Objective Loss 0.510722                                        LR 0.000250    Time 0.050421    
2024-06-06 02:53:48,852 - Epoch: [172][ 1000/ 1218]    Overall Loss 0.511406    Objective Loss 0.511406                                        LR 0.000250    Time 0.050103    
2024-06-06 02:53:53,438 - Epoch: [172][ 1100/ 1218]    Overall Loss 0.512016    Objective Loss 0.512016                                        LR 0.000250    Time 0.049716    
2024-06-06 02:53:58,122 - Epoch: [172][ 1200/ 1218]    Overall Loss 0.510794    Objective Loss 0.510794                                        LR 0.000250    Time 0.049474    
2024-06-06 02:53:58,906 - Epoch: [172][ 1218/ 1218]    Overall Loss 0.510787    Objective Loss 0.510787    Top1 74.816626    Top5 94.865526    LR 0.000250    Time 0.049386    
2024-06-06 02:53:59,074 - --- validate (epoch=172)-----------
2024-06-06 02:53:59,074 - 34633 samples (256 per mini-batch)
2024-06-06 02:54:04,615 - Epoch: [172][  100/  136]    Loss 0.466140    Top1 78.000000    Top5 96.074219    
2024-06-06 02:54:06,431 - Epoch: [172][  136/  136]    Loss 0.465416    Top1 77.870817    Top5 96.081772    
2024-06-06 02:54:06,624 - ==> Top1: 77.871    Top5: 96.082    Loss: 0.465

2024-06-06 02:54:06,626 - ==> Confusion:
[[ 787    2    5    0    6    2    1    2    7   81    1    1    2    5    6    3    6    2    0    4    8]
 [   1  929    2    0   16   28    4   18    3    1    2    5    5    2   16    2    8    2   10    2    7]
 [   8    3  848    7    1    1   25   12    0    8    4    5    1    6    9    3    9    1    9    3    7]
 [   4    4   14  857    8    4    5    2    2    0   19    2    8    2   46    2    2    8   20    2    5]
 [  29   15    4    0  918    9    2    1    3   12    2    4    0    7   15    7   16    1    4    1    4]
 [   8   22    4    3   17  850    3   43    2    5    3    9    8   32    6    1    4    3    8    6    6]
 [   2    1   16    2    3    8 1005    8    1    3    3   11    1    0    0    5    2    1    3    4    7]
 [   7   13   10    4    3   37    4  912    3    2    4    9    5    8    0    0    2    3   33    8   10]
 [  13    5    0    2    5    1    0    0  817   66    8    4    8   24   28    2    4    2    8    0    5]
 [  73    0    5    0    4    3    0    2   40  834    1    2    1   16    8    1    1    2    2    0    6]
 [   3    3    6   10    3    2    1    8   19    4  943    3    1   15   20    0    2    1   12    1    7]
 [   4    2    0    0    1   10    2    8    1    1    2  846   46   17    0   14   11   20    0   16   10]
 [   1    3    1    4    0    8    2    2    4    0    2   61  823    5    4    4    5   41    6    7   12]
 [   4    1    3    1    3   10    0    1   17   26    8   14    6  876    8    3    2    1    4    4    9]
 [  13    5    1    6    8    2    2    2   21    9    4    2    5    4  988    0    0    1   14    0   11]
 [   2    3    6    1    7    0    7    2    1    1    0   20   12    4    1  954   16   13    3    3   10]
 [   6    9    6    0    8    9    1    2    8    0    2    6    5    6    5   12  959    2    2    5   19]
 [   2    0    0    1    0    0    1    2    0    1    1   10   35    4    6   11    2  911    3    1   14]
 [   2    9    9   10    4    5    1   19    7    2    7    2    6    2   25    1    1    1  937    1    7]
 [   1    4    3    2    1   10   19   19    2    1    2   29   12   11    2   12   12    4    0  927   15]
 [ 255  254  278  137  261  220  125  210  145  182  208  190  452  416  387  158  339  158  222  287 9048]]

2024-06-06 02:54:06,627 - ==> Best [Top1: 77.871   Top5: 96.082   Sparsity:0.00   Params: 169472 on epoch: 172]
2024-06-06 02:54:06,628 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:54:06,637 - 

2024-06-06 02:54:06,637 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:54:12,679 - Epoch: [173][  100/ 1218]    Overall Loss 0.509141    Objective Loss 0.509141                                        LR 0.000250    Time 0.060403    
2024-06-06 02:54:17,282 - Epoch: [173][  200/ 1218]    Overall Loss 0.513456    Objective Loss 0.513456                                        LR 0.000250    Time 0.053204    
2024-06-06 02:54:22,018 - Epoch: [173][  300/ 1218]    Overall Loss 0.512918    Objective Loss 0.512918                                        LR 0.000250    Time 0.051250    
2024-06-06 02:54:26,626 - Epoch: [173][  400/ 1218]    Overall Loss 0.514509    Objective Loss 0.514509                                        LR 0.000250    Time 0.049953    
2024-06-06 02:54:31,283 - Epoch: [173][  500/ 1218]    Overall Loss 0.513695    Objective Loss 0.513695                                        LR 0.000250    Time 0.049273    
2024-06-06 02:54:36,124 - Epoch: [173][  600/ 1218]    Overall Loss 0.512706    Objective Loss 0.512706                                        LR 0.000250    Time 0.049126    
2024-06-06 02:54:41,135 - Epoch: [173][  700/ 1218]    Overall Loss 0.512095    Objective Loss 0.512095                                        LR 0.000250    Time 0.049264    
2024-06-06 02:54:46,074 - Epoch: [173][  800/ 1218]    Overall Loss 0.510911    Objective Loss 0.510911                                        LR 0.000250    Time 0.049279    
2024-06-06 02:54:50,911 - Epoch: [173][  900/ 1218]    Overall Loss 0.510915    Objective Loss 0.510915                                        LR 0.000250    Time 0.049175    
2024-06-06 02:54:55,875 - Epoch: [173][ 1000/ 1218]    Overall Loss 0.511985    Objective Loss 0.511985                                        LR 0.000250    Time 0.049220    
2024-06-06 02:55:00,649 - Epoch: [173][ 1100/ 1218]    Overall Loss 0.511540    Objective Loss 0.511540                                        LR 0.000250    Time 0.049083    
2024-06-06 02:55:05,391 - Epoch: [173][ 1200/ 1218]    Overall Loss 0.511376    Objective Loss 0.511376                                        LR 0.000250    Time 0.048944    
2024-06-06 02:55:06,153 - Epoch: [173][ 1218/ 1218]    Overall Loss 0.511482    Objective Loss 0.511482    Top1 75.061125    Top5 95.843521    LR 0.000250    Time 0.048845    
2024-06-06 02:55:06,342 - --- validate (epoch=173)-----------
2024-06-06 02:55:06,342 - 34633 samples (256 per mini-batch)
2024-06-06 02:55:11,792 - Epoch: [173][  100/  136]    Loss 0.479742    Top1 77.347656    Top5 96.152344    
2024-06-06 02:55:13,485 - Epoch: [173][  136/  136]    Loss 0.476854    Top1 77.336644    Top5 96.151070    
2024-06-06 02:55:13,673 - ==> Top1: 77.337    Top5: 96.151    Loss: 0.477

2024-06-06 02:55:13,675 - ==> Confusion:
[[ 811    0    1    0   15    2    0    1    7   66    0    2    0    2    8    1    2    3    4    1    5]
 [   1  931    4    3   17   25    2   18    6    2    5    5    1    3   11    1   10    3   12    0    3]
 [  11    2  840   15    8    4   19    8    3    5    6    5    3    4    5    6    7    3    5    3    8]
 [   1    3   14  862    2    7    4    1    1    2   27    3   10    2   34    1    3    8   18    0   13]
 [  21   12    1    0  951    8    0    1    3   12    0    3    1    3   17    3    5    1    6    0    6]
 [   5   27    1    4   15  872    2   27    5    2    2   15    6   23    5    2    4    0    7    3   16]
 [   0    7   23    4    2    7  975   10    3    0    5    6    1    0    1   16    5    3    3    9    6]
 [   4   19   16    2    6   51    5  879    3    2    3    9    4    4    0    0    4    4   37   13   12]
 [  13    2    1    0    1    2    1    0  818   56   14    3    3   21   42    2    5    2    8    0    8]
 [  83    1    1    1    9    5    1    2   53  809    2    1    2   15    8    2    0    1    3    0    2]
 [   4    3    8   13    1    6    5    5   24    1  948    1    1   13   10    0    3    0   13    1    4]
 [   0    1    1    2    2   17    2    8    4    2    1  859   34   15    1   10    5   22    3   14    8]
 [   1    2    2    6    0    9    1    4    3    0    3   71  792    6    6    7    9   51    9    2   11]
 [   0    4    1    1    8   19    0    4   18   22    9   20    2  864    5    4    2    3    2    3   10]
 [  10    2    2   12   16    2    1    2   29    6    5    2    2    4  976    0    0    3   14    1    9]
 [   2    2    5    1    9    3    6    0    1    0    0   18    7    3    0  973   15   13    2    1    5]
 [   4    8    2    2    8    8    1    1    9    1    0    8    1    4    5    7  978    4    1    5   15]
 [   5    1    1    2    0    2    2    2    1    2    0   17   31    9    5   12    3  895    6    1    8]
 [   2   14   10   12    1    3    0   17   12    1    8    2    2    4   26    1    3    0  934    0    6]
 [   0    9    4    1    1   13   18   15    3    0    3   35   10   11    1    7   10    6    5  928    8]
 [ 279  275  276  123  321  237  125  205  205  143  229  230  403  321  333  194  431  151  276  286 8889]]

2024-06-06 02:55:13,676 - ==> Best [Top1: 77.871   Top5: 96.082   Sparsity:0.00   Params: 169472 on epoch: 172]
2024-06-06 02:55:13,676 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:55:13,684 - 

2024-06-06 02:55:13,684 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:55:19,956 - Epoch: [174][  100/ 1218]    Overall Loss 0.502722    Objective Loss 0.502722                                        LR 0.000250    Time 0.062700    
2024-06-06 02:55:24,893 - Epoch: [174][  200/ 1218]    Overall Loss 0.508894    Objective Loss 0.508894                                        LR 0.000250    Time 0.056027    
2024-06-06 02:55:29,819 - Epoch: [174][  300/ 1218]    Overall Loss 0.507242    Objective Loss 0.507242                                        LR 0.000250    Time 0.053765    
2024-06-06 02:55:34,574 - Epoch: [174][  400/ 1218]    Overall Loss 0.505918    Objective Loss 0.505918                                        LR 0.000250    Time 0.052206    
2024-06-06 02:55:39,114 - Epoch: [174][  500/ 1218]    Overall Loss 0.506134    Objective Loss 0.506134                                        LR 0.000250    Time 0.050841    
2024-06-06 02:55:43,762 - Epoch: [174][  600/ 1218]    Overall Loss 0.505415    Objective Loss 0.505415                                        LR 0.000250    Time 0.050112    
2024-06-06 02:55:48,510 - Epoch: [174][  700/ 1218]    Overall Loss 0.507984    Objective Loss 0.507984                                        LR 0.000250    Time 0.049733    
2024-06-06 02:55:53,537 - Epoch: [174][  800/ 1218]    Overall Loss 0.509767    Objective Loss 0.509767                                        LR 0.000250    Time 0.049796    
2024-06-06 02:55:58,313 - Epoch: [174][  900/ 1218]    Overall Loss 0.509966    Objective Loss 0.509966                                        LR 0.000250    Time 0.049567    
2024-06-06 02:56:02,958 - Epoch: [174][ 1000/ 1218]    Overall Loss 0.509758    Objective Loss 0.509758                                        LR 0.000250    Time 0.049254    
2024-06-06 02:56:07,626 - Epoch: [174][ 1100/ 1218]    Overall Loss 0.511120    Objective Loss 0.511120                                        LR 0.000250    Time 0.049018    
2024-06-06 02:56:12,306 - Epoch: [174][ 1200/ 1218]    Overall Loss 0.510675    Objective Loss 0.510675                                        LR 0.000250    Time 0.048827    
2024-06-06 02:56:13,107 - Epoch: [174][ 1218/ 1218]    Overall Loss 0.511077    Objective Loss 0.511077    Top1 77.017115    Top5 95.354523    LR 0.000250    Time 0.048763    
2024-06-06 02:56:13,307 - --- validate (epoch=174)-----------
2024-06-06 02:56:13,307 - 34633 samples (256 per mini-batch)
2024-06-06 02:56:18,841 - Epoch: [174][  100/  136]    Loss 0.475137    Top1 77.738281    Top5 96.285156    
2024-06-06 02:56:20,539 - Epoch: [174][  136/  136]    Loss 0.470976    Top1 77.665810    Top5 96.191494    
2024-06-06 02:56:20,717 - ==> Top1: 77.666    Top5: 96.191    Loss: 0.471

2024-06-06 02:56:20,718 - ==> Confusion:
[[ 791    0    4    1   15    2    1    4    9   73    0    3    0    1    6    2    2    1    2    2   12]
 [   3  938    1    1   27   17    4   19    6    1    2    3    1    0    8    1    6    2   16    2    5]
 [   7    3  845   13    5    4   25   15    1    2    7    6    2    4    4    6    1    1    6    1   12]
 [   4    2    9  880    2    6    5    4    4    2   12    1    7    4   27    1    4   14   22    1    5]
 [  29   13    4    2  937    8    1    2    1   10    0    3    1    6   12    4    8    2    6    0    5]
 [   1   37    3    2   21  839    5   42    1    2    6   21    5   13    3    4    3    3    7   11   14]
 [   2    5   30    0    2    3  984    7    3    1    5    6    0    0    2    7    2    2    1   16    8]
 [   4   20   10    2    4   44   10  896    4    1    3   12    4    4    2    0    1    3   37   13    3]
 [  12    9    1    1    1    4    0    3  828   43   16    2    3   24   24    0    8    1   15    1    6]
 [  79    0    6    0   11    1    2    2   64  787    1    1    2   24    6    1    1    2    4    0    7]
 [   0    7   10   14    3    5    3    9   23    1  936    1    3   14   13    0    0    0   15    0    7]
 [   1    2    3    2    2    9    4    6    1    0    2  886   22    9    1   12    6   18    1   19    5]
 [   1    2    3    6    1   10    2    7    2    1    0   85  796    2    2   10    3   41    7    4   10]
 [   3    1    1    2    7   11    2    6   15   14    5   13    5  877    5    1    4    7    3    9   10]
 [   4    6    4   13   11    1    1    1   44    5    2    3    4    8  958    0    0    5   16    0   12]
 [   2    1    2    0    3    3   10    0    0    0    0   20   12    4    0  964    9   20    5    5    6]
 [   4   16    2    4   10   12    2    3    6    0    1   11    3    4    2   16  943    1    3    9   20]
 [   2    1    2    3    1    2    3    0    0    1    1   31   26    2    6    7    1  900    3    6    7]
 [   1    7   10   12    4    3    0   23    2    2    3    2    3    3   16    0    3    0  958    2    4]
 [   3    8    7    1    3   13   13   19    1    2    1   24    7    1    0    5    9    1    3  962    5]
 [ 294  308  277  158  339  226  126  249  148  126  172  279  381  317  314  183  306  144  260  332 8993]]

2024-06-06 02:56:20,720 - ==> Best [Top1: 77.871   Top5: 96.082   Sparsity:0.00   Params: 169472 on epoch: 172]
2024-06-06 02:56:20,720 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:56:20,729 - 

2024-06-06 02:56:20,729 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:56:26,876 - Epoch: [175][  100/ 1218]    Overall Loss 0.505022    Objective Loss 0.505022                                        LR 0.000250    Time 0.061453    
2024-06-06 02:56:31,413 - Epoch: [175][  200/ 1218]    Overall Loss 0.499868    Objective Loss 0.499868                                        LR 0.000250    Time 0.053399    
2024-06-06 02:56:35,964 - Epoch: [175][  300/ 1218]    Overall Loss 0.503630    Objective Loss 0.503630                                        LR 0.000250    Time 0.050763    
2024-06-06 02:56:40,512 - Epoch: [175][  400/ 1218]    Overall Loss 0.505774    Objective Loss 0.505774                                        LR 0.000250    Time 0.049438    
2024-06-06 02:56:45,307 - Epoch: [175][  500/ 1218]    Overall Loss 0.507915    Objective Loss 0.507915                                        LR 0.000250    Time 0.049138    
2024-06-06 02:56:50,039 - Epoch: [175][  600/ 1218]    Overall Loss 0.506956    Objective Loss 0.506956                                        LR 0.000250    Time 0.048832    
2024-06-06 02:56:54,902 - Epoch: [175][  700/ 1218]    Overall Loss 0.508482    Objective Loss 0.508482                                        LR 0.000250    Time 0.048800    
2024-06-06 02:56:59,673 - Epoch: [175][  800/ 1218]    Overall Loss 0.507477    Objective Loss 0.507477                                        LR 0.000250    Time 0.048661    
2024-06-06 02:57:04,264 - Epoch: [175][  900/ 1218]    Overall Loss 0.507815    Objective Loss 0.507815                                        LR 0.000250    Time 0.048354    
2024-06-06 02:57:09,072 - Epoch: [175][ 1000/ 1218]    Overall Loss 0.508177    Objective Loss 0.508177                                        LR 0.000250    Time 0.048324    
2024-06-06 02:57:13,738 - Epoch: [175][ 1100/ 1218]    Overall Loss 0.509650    Objective Loss 0.509650                                        LR 0.000250    Time 0.048171    
2024-06-06 02:57:18,386 - Epoch: [175][ 1200/ 1218]    Overall Loss 0.510016    Objective Loss 0.510016                                        LR 0.000250    Time 0.048029    
2024-06-06 02:57:19,163 - Epoch: [175][ 1218/ 1218]    Overall Loss 0.509433    Objective Loss 0.509433    Top1 76.772616    Top5 97.799511    LR 0.000250    Time 0.047957    
2024-06-06 02:57:19,363 - --- validate (epoch=175)-----------
2024-06-06 02:57:19,363 - 34633 samples (256 per mini-batch)
2024-06-06 02:57:24,832 - Epoch: [175][  100/  136]    Loss 0.477034    Top1 78.035156    Top5 96.253906    
2024-06-06 02:57:26,559 - Epoch: [175][  136/  136]    Loss 0.474883    Top1 77.945890    Top5 96.292553    
2024-06-06 02:57:26,751 - ==> Top1: 77.946    Top5: 96.293    Loss: 0.475

2024-06-06 02:57:26,752 - ==> Confusion:
[[ 829    1    2    0    6    1    0    3   10   48    0    4    5    5    5    0    0    3    4    1    4]
 [   6  936    2    3   17   24    2   17    6    6    0    7    2    2    4    2    6    2    8    4    7]
 [  11    3  837   12    1    2   35   14    2    5    8    4    3    6    0    4    6    2    3    1   11]
 [   2    0   15  889    1    5    3    5    3    1   16    2    7    3   28    2    4    8   12    2    8]
 [  37    9    3    0  936   10    2    2    1   12    0    5    1    3   10    4    4    0    4    1   10]
 [   6   37    8    4    9  836    5   49    1    4    3   20    4   20    2    0    5    3    5    8   14]
 [   2    8   28    2    3    6  980    8    1    0    6    6    2    2    0    4    2    5    3   11    7]
 [   7   14   10    3    3   29   13  906    6    1    8    9    3    7    0    2    1    4   35   11    5]
 [  18    6    0    3    2    0    0    2  825   55   15    1    8   20   23    1    3    4    6    2    8]
 [ 115    3    2    0    8    2    0    2   32  800    0    0    1   15    5    2    1    2    0    2    9]
 [   3    7   12   13    0    4    3    5   22    3  933    0    3   17   12    0    0    0   19    2    6]
 [   1    4    3    0    0    8    4   11    3    2    0  862   28   11    0   12    5   29    2   20    6]
 [   0    1    4    9    0    5    1    4    2    0    2   86  789    6    1    7    4   53    6    3   12]
 [   8    1    1    3    6   10    5    2   19   30    6   11    3  859    5    1    3    5    1    6   16]
 [  15    2    2   11    8    3    0    1   39   11    8    2    3    4  952    1    3    3   21    2    7]
 [   3    3    6    1    5    2    9    0    1    2    0   20    9    1    0  953   11   27    1    4    8]
 [   2   10    8    2    4   11    2    2    5    3    3    9    6    5    0   15  950    1    3   12   19]
 [   2    3    0    2    1    3    1    3    3    1    0   17   20    2    3   11    4  919    3    2    5]
 [   2    8    8   19    4    7    2   21    7    4    4    4    4    2    7    3    0    2  941    0    9]
 [   1    6    2    1    2    7   20   17    1    0    1   30    8    7    0    8    5   11    4  950    7]
 [ 276  279  254  159  255  222  116  243  151  156  202  250  387  337  251  182  286  193  270  350 9113]]

2024-06-06 02:57:26,754 - ==> Best [Top1: 77.946   Top5: 96.293   Sparsity:0.00   Params: 169472 on epoch: 175]
2024-06-06 02:57:26,754 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:57:26,770 - 

2024-06-06 02:57:26,770 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:57:32,817 - Epoch: [176][  100/ 1218]    Overall Loss 0.509761    Objective Loss 0.509761                                        LR 0.000250    Time 0.060455    
2024-06-06 02:57:37,501 - Epoch: [176][  200/ 1218]    Overall Loss 0.505786    Objective Loss 0.505786                                        LR 0.000250    Time 0.053634    
2024-06-06 02:57:42,295 - Epoch: [176][  300/ 1218]    Overall Loss 0.506585    Objective Loss 0.506585                                        LR 0.000250    Time 0.051731    
2024-06-06 02:57:46,993 - Epoch: [176][  400/ 1218]    Overall Loss 0.508464    Objective Loss 0.508464                                        LR 0.000250    Time 0.050538    
2024-06-06 02:57:51,715 - Epoch: [176][  500/ 1218]    Overall Loss 0.508142    Objective Loss 0.508142                                        LR 0.000250    Time 0.049871    
2024-06-06 02:57:56,453 - Epoch: [176][  600/ 1218]    Overall Loss 0.510208    Objective Loss 0.510208                                        LR 0.000250    Time 0.049454    
2024-06-06 02:58:01,134 - Epoch: [176][  700/ 1218]    Overall Loss 0.509985    Objective Loss 0.509985                                        LR 0.000250    Time 0.049074    
2024-06-06 02:58:05,702 - Epoch: [176][  800/ 1218]    Overall Loss 0.509887    Objective Loss 0.509887                                        LR 0.000250    Time 0.048647    
2024-06-06 02:58:10,258 - Epoch: [176][  900/ 1218]    Overall Loss 0.508055    Objective Loss 0.508055                                        LR 0.000250    Time 0.048302    
2024-06-06 02:58:14,906 - Epoch: [176][ 1000/ 1218]    Overall Loss 0.508703    Objective Loss 0.508703                                        LR 0.000250    Time 0.048118    
2024-06-06 02:58:19,634 - Epoch: [176][ 1100/ 1218]    Overall Loss 0.509567    Objective Loss 0.509567                                        LR 0.000250    Time 0.048041    
2024-06-06 02:58:24,256 - Epoch: [176][ 1200/ 1218]    Overall Loss 0.509274    Objective Loss 0.509274                                        LR 0.000250    Time 0.047887    
2024-06-06 02:58:25,015 - Epoch: [176][ 1218/ 1218]    Overall Loss 0.509147    Objective Loss 0.509147    Top1 75.794621    Top5 96.332518    LR 0.000250    Time 0.047803    
2024-06-06 02:58:25,179 - --- validate (epoch=176)-----------
2024-06-06 02:58:25,179 - 34633 samples (256 per mini-batch)
2024-06-06 02:58:30,675 - Epoch: [176][  100/  136]    Loss 0.485788    Top1 77.070312    Top5 95.898438    
2024-06-06 02:58:32,390 - Epoch: [176][  136/  136]    Loss 0.481075    Top1 77.198048    Top5 96.006699    
2024-06-06 02:58:32,562 - ==> Top1: 77.198    Top5: 96.007    Loss: 0.481

2024-06-06 02:58:32,563 - ==> Confusion:
[[ 762    1    3    0   19    1    0    4    8   98    0    1    2    6    4    2    3    2    2    2   11]
 [   2  918    3    2   26   21    3   17    3    5    2   10    4    2    9    2   12    1    8    5    8]
 [   6    2  849    7    4    3   19   14    0    4    7    6    3   11    2   12    7    1    1    1   11]
 [   4    2   21  865    1    8    1    5    6    2   15    6   11    6   32    4    1    4    8    4   10]
 [  20    9    1    0  942    9    0    2    1   17    1    5    1    6   12    6    7    0    3    0   12]
 [   5   26    2    1   16  854    2   36    2    5    2   22    5   26    5    4    5    3    3    7   12]
 [   3    5   22    1    4    8  968   17    2    1    6    5    4    2    0   15    1    2    1   10    9]
 [   3   12   11    1    3   36    5  926    4    5    4   14    5    5    1    0    0    2   22   10    8]
 [   7    2    0    0    1    3    1    2  821   68    7    6    2   39   19    2    3    2    8    2    7]
 [  55    2    1    0    6    3    0    2   37  839    1    3    2   28    9    0    1    1    3    1    7]
 [   1    1    4   12    2    5    4    7   24    3  943    1    3   24    9    1    2    0   11    2    5]
 [   4    1    3    0    0   13    3    3    1    1    0  902   16   13    1   12    3   10    1   13   11]
 [   0    2    2    6    1    8    2    6    1    2    1  129  768    2    2    9    4   22    5    4   19]
 [   2    0    0    0    6    9    0    6    9   15    7   13    3  909    3    3    1    2    1    5    7]
 [  14    5    0   15    8    4    0    0   29   10    4    2    7   14  960    0    2    2   15    0    7]
 [   2    1    2    1    5    1    3    0    0    0    0   26    7    3    3  971   15   10    1    4   11]
 [  10    9    4    1    6    9    2    0    7    2    2   13    5    3    5    8  954    2    3    7   20]
 [   1    0    0    3    1    4    1    3    1    8    0   47   32    6    3   15    4  865    0    2    9]
 [   1   14    7   15    5    4    2   31   10    2    8    3    7    0   23    1    1    1  914    5    4]
 [   2    6    7    2    2   11   14   10    1    1    1   32    7   10    0    6   11    6    3  945   11]
 [ 250  246  251  137  324  248  108  253  142  210  206  337  364  496  282  231  334  102  208  342 8861]]

2024-06-06 02:58:32,565 - ==> Best [Top1: 77.946   Top5: 96.293   Sparsity:0.00   Params: 169472 on epoch: 175]
2024-06-06 02:58:32,565 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:58:32,572 - 

2024-06-06 02:58:32,573 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:58:38,663 - Epoch: [177][  100/ 1218]    Overall Loss 0.503187    Objective Loss 0.503187                                        LR 0.000250    Time 0.060887    
2024-06-06 02:58:43,386 - Epoch: [177][  200/ 1218]    Overall Loss 0.507887    Objective Loss 0.507887                                        LR 0.000250    Time 0.054047    
2024-06-06 02:58:47,989 - Epoch: [177][  300/ 1218]    Overall Loss 0.503668    Objective Loss 0.503668                                        LR 0.000250    Time 0.051368    
2024-06-06 02:58:52,692 - Epoch: [177][  400/ 1218]    Overall Loss 0.504584    Objective Loss 0.504584                                        LR 0.000250    Time 0.050279    
2024-06-06 02:58:57,456 - Epoch: [177][  500/ 1218]    Overall Loss 0.503401    Objective Loss 0.503401                                        LR 0.000250    Time 0.049747    
2024-06-06 02:59:02,079 - Epoch: [177][  600/ 1218]    Overall Loss 0.504147    Objective Loss 0.504147                                        LR 0.000250    Time 0.049159    
2024-06-06 02:59:06,771 - Epoch: [177][  700/ 1218]    Overall Loss 0.505564    Objective Loss 0.505564                                        LR 0.000250    Time 0.048836    
2024-06-06 02:59:11,363 - Epoch: [177][  800/ 1218]    Overall Loss 0.506132    Objective Loss 0.506132                                        LR 0.000250    Time 0.048469    
2024-06-06 02:59:15,921 - Epoch: [177][  900/ 1218]    Overall Loss 0.505403    Objective Loss 0.505403                                        LR 0.000250    Time 0.048146    
2024-06-06 02:59:20,607 - Epoch: [177][ 1000/ 1218]    Overall Loss 0.506223    Objective Loss 0.506223                                        LR 0.000250    Time 0.048016    
2024-06-06 02:59:25,294 - Epoch: [177][ 1100/ 1218]    Overall Loss 0.506118    Objective Loss 0.506118                                        LR 0.000250    Time 0.047910    
2024-06-06 02:59:29,954 - Epoch: [177][ 1200/ 1218]    Overall Loss 0.508684    Objective Loss 0.508684                                        LR 0.000250    Time 0.047799    
2024-06-06 02:59:30,730 - Epoch: [177][ 1218/ 1218]    Overall Loss 0.509266    Objective Loss 0.509266    Top1 75.794621    Top5 96.821516    LR 0.000250    Time 0.047729    
2024-06-06 02:59:30,904 - --- validate (epoch=177)-----------
2024-06-06 02:59:30,904 - 34633 samples (256 per mini-batch)
2024-06-06 02:59:36,565 - Epoch: [177][  100/  136]    Loss 0.467019    Top1 77.871094    Top5 95.996094    
2024-06-06 02:59:38,391 - Epoch: [177][  136/  136]    Loss 0.468361    Top1 77.994976    Top5 95.943176    
2024-06-06 02:59:38,562 - ==> Top1: 77.995    Top5: 95.943    Loss: 0.468

2024-06-06 02:59:38,563 - ==> Confusion:
[[ 799    0    1    0   11    1    0    4   12   70    1    0    2    3    5    4    1    2    2    0   13]
 [   2  927    2    2   21   32    5   19    4    4    4    4    6    1    5    2    5    0    9    2    7]
 [   9    5  828   13    2    1   38   12    3    5    5    6    4    8    4    2    6    1    5    4    9]
 [   5    2   20  875    4    7    1    3    1    5   18    2    9    4   24    2    0   11   12    3    8]
 [  28   13    4    3  923    7    4    1    5   20    2    3    2    3    9    7    6    2    4    1    7]
 [   4   30    4    3   16  856    4   37    6    7    3   11   12   19    4    1    4    2    4    6   10]
 [   5    1   14    2    5    5 1005    6    2    0    2    3    2    1    1    7    3    3    4   12    3]
 [   3   13   10    1    1   42    3  902    9    1    7   12    8    3    2    1    0    2   31   17    9]
 [  15    5    1    1    2    5    2    1  840   53   18    2    6   11   26    0    1    0    7    0    6]
 [  77    1    2    0    5    2    1    1   52  809    1    1    2   17   13    2    2    3    4    0    6]
 [   3    6    6    9    3    3    2    8   22    1  947    0    3   18    8    0    1    0   16    2    6]
 [   2    1    3    0    0   16    3    5    2    0    0  843   51    6    2   17    2   35    1   17    5]
 [   1    1    6    4    0    5    0    6    2    1    3   68  816    3    5    5    4   47    7    5    6]
 [   3    0    3    0    1   13    0    3   21   24    9    5    9  876    4    0    1    9    0   11    9]
 [  10    6    3   11    7    1    0    1   39   16    7    1    4    4  941    4    2    5   27    0    9]
 [   4    1    3    1    3    1    5    1    0    2    0   17   11    2    1  972   15   15    0    5    7]
 [   2   11    5    4    6    9    2    3    5    1    1    4    3    6    3   13  964    7    4    9   10]
 [   2    2    1    2    0    3    0    0    0    3    1   16   26    4    3    5    3  921    3    2    8]
 [   4    5    5   18    4    4    1   19    5    2    4    1   10    3   19    1    0    1  941    5    6]
 [   2    3    4    2    2   17   13    9    1    0    4   25   11    4    0    5   12    4    8  951   11]
 [ 249  241  253  174  259  235  150  207  186  162  223  209  430  349  264  181  321  176  247  340 9076]]

2024-06-06 02:59:38,565 - ==> Best [Top1: 77.995   Top5: 95.943   Sparsity:0.00   Params: 169472 on epoch: 177]
2024-06-06 02:59:38,566 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 02:59:38,584 - 

2024-06-06 02:59:38,584 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 02:59:44,797 - Epoch: [178][  100/ 1218]    Overall Loss 0.515388    Objective Loss 0.515388                                        LR 0.000250    Time 0.062107    
2024-06-06 02:59:49,810 - Epoch: [178][  200/ 1218]    Overall Loss 0.509308    Objective Loss 0.509308                                        LR 0.000250    Time 0.056109    
2024-06-06 02:59:54,688 - Epoch: [178][  300/ 1218]    Overall Loss 0.508056    Objective Loss 0.508056                                        LR 0.000250    Time 0.053659    
2024-06-06 02:59:59,256 - Epoch: [178][  400/ 1218]    Overall Loss 0.508040    Objective Loss 0.508040                                        LR 0.000250    Time 0.051660    
2024-06-06 03:00:04,308 - Epoch: [178][  500/ 1218]    Overall Loss 0.506807    Objective Loss 0.506807                                        LR 0.000250    Time 0.051427    
2024-06-06 03:00:09,143 - Epoch: [178][  600/ 1218]    Overall Loss 0.506813    Objective Loss 0.506813                                        LR 0.000250    Time 0.050912    
2024-06-06 03:00:13,829 - Epoch: [178][  700/ 1218]    Overall Loss 0.507091    Objective Loss 0.507091                                        LR 0.000250    Time 0.050331    
2024-06-06 03:00:18,509 - Epoch: [178][  800/ 1218]    Overall Loss 0.508877    Objective Loss 0.508877                                        LR 0.000250    Time 0.049885    
2024-06-06 03:00:23,081 - Epoch: [178][  900/ 1218]    Overall Loss 0.507812    Objective Loss 0.507812                                        LR 0.000250    Time 0.049420    
2024-06-06 03:00:27,833 - Epoch: [178][ 1000/ 1218]    Overall Loss 0.508289    Objective Loss 0.508289                                        LR 0.000250    Time 0.049229    
2024-06-06 03:00:32,512 - Epoch: [178][ 1100/ 1218]    Overall Loss 0.508942    Objective Loss 0.508942                                        LR 0.000250    Time 0.049005    
2024-06-06 03:00:37,108 - Epoch: [178][ 1200/ 1218]    Overall Loss 0.508689    Objective Loss 0.508689                                        LR 0.000250    Time 0.048750    
2024-06-06 03:00:37,876 - Epoch: [178][ 1218/ 1218]    Overall Loss 0.508843    Objective Loss 0.508843    Top1 80.440098    Top5 97.310513    LR 0.000250    Time 0.048660    
2024-06-06 03:00:38,057 - --- validate (epoch=178)-----------
2024-06-06 03:00:38,057 - 34633 samples (256 per mini-batch)
2024-06-06 03:00:43,728 - Epoch: [178][  100/  136]    Loss 0.471113    Top1 77.175781    Top5 95.843750    
2024-06-06 03:00:45,421 - Epoch: [178][  136/  136]    Loss 0.474275    Top1 77.111426    Top5 95.735281    
2024-06-06 03:00:45,603 - ==> Top1: 77.111    Top5: 95.735    Loss: 0.474

2024-06-06 03:00:45,604 - ==> Confusion:
[[ 814    0    6    1    2    2    0    0   11   59    0    2    5    6    8    1    2    0    2    1    9]
 [   2  928    5    2   16   27    5   12    5    2    6    2    5    2    4    2   10    2   16    4    6]
 [  10    4  828   14    3    5   24    9    1    3   11    7    7    9    5    6    7    0    8    1    8]
 [   5    7   11  881    0    6    4    3    3    4   20    1    9    3   26    2    1    7   15    1    7]
 [  35   17    3    0  908   16    0    3    4   22    0    4    3    5    8    5   11    3    4    1    2]
 [   7   29    2    3    9  870    5   27    5    7    4   20    9   22    3    3    4    2    0    7    5]
 [   2    4   26    3    3    8  968    8    2    3    5    6    2    1    0    8    6    6    5   12    8]
 [   4   13   11    3    3   48    6  900    3    2    6    9    8    5    1    1    5    1   28   15    5]
 [  16    5    0    3    0    2    1    2  843   56   16    7    2   10   16    1    6    1   10    0    5]
 [  88    2    3    0    3    4    0    1   61  800    0    1    0   16   11    1    1    2    1    1    5]
 [   2    3    5   11    1    6    4    5   19    1  955    0    1   14   14    1    3    1   12    3    3]
 [   3    2    2    0    1   11    2    7    1    4    1  864   28   15    4   10    5   20    3   18   10]
 [   0    2    2    4    0    7    1    1    4    0    2   78  828    2    3    7    3   29    6    2   14]
 [   2    2    1    1    6   18    1    5   19   21   10   12    7  863    7    4    4    3    2    8    5]
 [  12   10    0   14    6    2    1    2   39   11    5    3    5   11  947    0    5    6   13    0    6]
 [   4    2    1    0    5    2    9    0    2    2    0   20   15    6    1  961   13   12    3    2    6]
 [   9   11    2    5   14    8    3    0   11    3    1    8    3    5    0    9  957    2    4    6   11]
 [   5    2    1    8    2    2    1    3    1    0    2   27   33    2    2   12    4  886    3    3    6]
 [   2   10    6   19    1    8    2   15   13    1    7    3    3    0   24    2    1    2  929    6    4]
 [   1    5    6    0    1   11   15    9    2    0    1   24   16    6    0    7    7    1    5  962    9]
 [ 331  289  252  170  218  251  148  211  213  136  251  238  430  343  293  199  380  126  262  377 8814]]

2024-06-06 03:00:45,606 - ==> Best [Top1: 77.995   Top5: 95.943   Sparsity:0.00   Params: 169472 on epoch: 177]
2024-06-06 03:00:45,606 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 03:00:45,613 - 

2024-06-06 03:00:45,613 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 03:00:51,635 - Epoch: [179][  100/ 1218]    Overall Loss 0.516399    Objective Loss 0.516399                                        LR 0.000250    Time 0.060194    
2024-06-06 03:00:56,320 - Epoch: [179][  200/ 1218]    Overall Loss 0.517089    Objective Loss 0.517089                                        LR 0.000250    Time 0.053514    
2024-06-06 03:01:00,941 - Epoch: [179][  300/ 1218]    Overall Loss 0.516099    Objective Loss 0.516099                                        LR 0.000250    Time 0.051072    
2024-06-06 03:01:05,551 - Epoch: [179][  400/ 1218]    Overall Loss 0.513605    Objective Loss 0.513605                                        LR 0.000250    Time 0.049826    
2024-06-06 03:01:10,100 - Epoch: [179][  500/ 1218]    Overall Loss 0.512867    Objective Loss 0.512867                                        LR 0.000250    Time 0.048954    
2024-06-06 03:01:14,732 - Epoch: [179][  600/ 1218]    Overall Loss 0.512460    Objective Loss 0.512460                                        LR 0.000250    Time 0.048512    
2024-06-06 03:01:19,474 - Epoch: [179][  700/ 1218]    Overall Loss 0.510221    Objective Loss 0.510221                                        LR 0.000250    Time 0.048354    
2024-06-06 03:01:24,073 - Epoch: [179][  800/ 1218]    Overall Loss 0.509917    Objective Loss 0.509917                                        LR 0.000250    Time 0.048056    
2024-06-06 03:01:28,941 - Epoch: [179][  900/ 1218]    Overall Loss 0.510046    Objective Loss 0.510046                                        LR 0.000250    Time 0.048124    
2024-06-06 03:01:33,655 - Epoch: [179][ 1000/ 1218]    Overall Loss 0.509085    Objective Loss 0.509085                                        LR 0.000250    Time 0.048023    
2024-06-06 03:01:38,433 - Epoch: [179][ 1100/ 1218]    Overall Loss 0.507990    Objective Loss 0.507990                                        LR 0.000250    Time 0.047999    
2024-06-06 03:01:43,124 - Epoch: [179][ 1200/ 1218]    Overall Loss 0.507641    Objective Loss 0.507641                                        LR 0.000250    Time 0.047908    
2024-06-06 03:01:43,948 - Epoch: [179][ 1218/ 1218]    Overall Loss 0.507034    Objective Loss 0.507034    Top1 80.195599    Top5 97.555012    LR 0.000250    Time 0.047876    
2024-06-06 03:01:44,120 - --- validate (epoch=179)-----------
2024-06-06 03:01:44,120 - 34633 samples (256 per mini-batch)
2024-06-06 03:01:49,639 - Epoch: [179][  100/  136]    Loss 0.455053    Top1 77.773438    Top5 96.285156    
2024-06-06 03:01:51,298 - Epoch: [179][  136/  136]    Loss 0.460741    Top1 77.509889    Top5 96.292553    
2024-06-06 03:01:51,498 - ==> Top1: 77.510    Top5: 96.293    Loss: 0.461

2024-06-06 03:01:51,500 - ==> Confusion:
[[ 804    0    2    0    8    4    0    3    7   68    0    3    1    5    9    0    3    5    4    2    3]
 [   4  943    1    5   16   20    5   16    3    1    3    2    4    2    4    0    7    1   14    2   10]
 [   2    4  829   18    3    5   31   13    1    5    3    4    4    5    5    7    8    1    9    6    7]
 [   6    3   12  889    1    5    4    4    1    2   11    2   11    4   26    3    2    6   16    4    4]
 [  31   11    1    1  944   12    1    3    4   10    2    2    2    2    9    1    5    1    5    2    5]
 [   5   34    1    2   15  863    2   45    4    1    1   10    7   18    5    0    9    2    2    8    9]
 [   2    4   22    3    3    7  976    8    1    0    4    4    3    2    0   11    4    3    3   15   11]
 [   1   12   11    2    4   35    5  922    2    5    3    6    5    2    3    0    1    6   33   15    4]
 [  12    2    0    4    1    2    0    1  844   48   10    4    5   19   29    0    1    4    7    2    7]
 [  77    0    1    0   11    4    0    3   47  798    0    1    2   29   10    0    2    4    6    1    5]
 [   1    6   11   15    1    2    6    9   16    2  936    0    1   12   15    0    6    0   13    4    8]
 [   2    0    1    0    1   16    5    7    0    1    0  854   35    9    2   10    6   24    3   30    5]
 [   1    1    2    6    0   13    4    4    1    0    2   63  802    6    1    7    4   43   15   10   10]
 [   2    0    2    1    5   18    0    5   18   14    8    9    3  881    7    2    0    5    2    5   14]
 [  10    4    2   19    9    1    0    2   31    8    1    1    3    9  963    0    1    5   18    1   10]
 [   2    2    4    1    7    3    6    1    0    0    0   19    7    1    0  968    9   19    1    8    8]
 [   2    7    5    1    4   10    3    0    6    1    1    9    5    2    4   14  964    6    2    8   18]
 [   2    0    1    1    1    2    0    4    0    3    1   15   31    1    4    7    1  913    2    2   14]
 [   0    8    6   11    5    5    2   26    3    1    1    1    3    3   23    0    2    1  944    5    8]
 [   1    6    1    1    0   13   16   15    1    0    1   19   11    3    1    7    8    3    1  966   14]
 [ 276  287  258  143  307  243  136  263  136  131  207  203  427  316  310  176  415  158  268  431 8841]]

2024-06-06 03:01:51,501 - ==> Best [Top1: 77.995   Top5: 95.943   Sparsity:0.00   Params: 169472 on epoch: 177]
2024-06-06 03:01:51,501 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 03:01:51,509 - 

2024-06-06 03:01:51,509 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 03:01:57,636 - Epoch: [180][  100/ 1218]    Overall Loss 0.501272    Objective Loss 0.501272                                        LR 0.000125    Time 0.061244    
2024-06-06 03:02:02,373 - Epoch: [180][  200/ 1218]    Overall Loss 0.507859    Objective Loss 0.507859                                        LR 0.000125    Time 0.054301    
2024-06-06 03:02:06,997 - Epoch: [180][  300/ 1218]    Overall Loss 0.504244    Objective Loss 0.504244                                        LR 0.000125    Time 0.051609    
2024-06-06 03:02:11,709 - Epoch: [180][  400/ 1218]    Overall Loss 0.501729    Objective Loss 0.501729                                        LR 0.000125    Time 0.050482    
2024-06-06 03:02:16,324 - Epoch: [180][  500/ 1218]    Overall Loss 0.501668    Objective Loss 0.501668                                        LR 0.000125    Time 0.049612    
2024-06-06 03:02:21,084 - Epoch: [180][  600/ 1218]    Overall Loss 0.500672    Objective Loss 0.500672                                        LR 0.000125    Time 0.049274    
2024-06-06 03:02:25,716 - Epoch: [180][  700/ 1218]    Overall Loss 0.499705    Objective Loss 0.499705                                        LR 0.000125    Time 0.048850    
2024-06-06 03:02:30,607 - Epoch: [180][  800/ 1218]    Overall Loss 0.499423    Objective Loss 0.499423                                        LR 0.000125    Time 0.048854    
2024-06-06 03:02:35,485 - Epoch: [180][  900/ 1218]    Overall Loss 0.501090    Objective Loss 0.501090                                        LR 0.000125    Time 0.048844    
2024-06-06 03:02:40,285 - Epoch: [180][ 1000/ 1218]    Overall Loss 0.501331    Objective Loss 0.501331                                        LR 0.000125    Time 0.048758    
2024-06-06 03:02:44,900 - Epoch: [180][ 1100/ 1218]    Overall Loss 0.501287    Objective Loss 0.501287                                        LR 0.000125    Time 0.048519    
2024-06-06 03:02:49,620 - Epoch: [180][ 1200/ 1218]    Overall Loss 0.500844    Objective Loss 0.500844                                        LR 0.000125    Time 0.048408    
2024-06-06 03:02:50,470 - Epoch: [180][ 1218/ 1218]    Overall Loss 0.500748    Objective Loss 0.500748    Top1 80.684597    Top5 96.577017    LR 0.000125    Time 0.048390    
2024-06-06 03:02:50,665 - --- validate (epoch=180)-----------
2024-06-06 03:02:50,665 - 34633 samples (256 per mini-batch)
2024-06-06 03:02:56,380 - Epoch: [180][  100/  136]    Loss 0.454559    Top1 77.503906    Top5 96.058594    
2024-06-06 03:02:58,034 - Epoch: [180][  136/  136]    Loss 0.456255    Top1 77.550313    Top5 96.044235    
2024-06-06 03:02:58,204 - ==> Top1: 77.550    Top5: 96.044    Loss: 0.456

2024-06-06 03:02:58,206 - ==> Confusion:
[[ 799    1    3    0   12    2    0    2    7   74    0    3    2    7    3    1    5    1    3    1    5]
 [   4  924    2    0   26   23    5   15    3    0    3    5    3    1    8    1    4    1   18    6   11]
 [   3    5  852    8    3    4   22   12    0    6   10    2    5    2    4    7    4    1    3    7   10]
 [   6    2   11  858    2    9    5    4    2    1   20    0    6    6   40    2    2    6   19    1   14]
 [  25    6    3    0  935   10    2    4    2   18    1    4    2    5    9    5    8    0    8    2    5]
 [   4   25    5    0   12  838    3   40    6    5    3   21   12   22    4    4    1    5   10   12   11]
 [   2    3   26    2    2    5  978    8    0    0    6   10    6    2    1    9    2    1    3   11    9]
 [   2   14   14    1    5   37    3  909    6    0    5   16    3    3    2    2    1    3   30   11   10]
 [  15    2    0    2    0    3    0    1  829   55   14    5    4   21   26    1    2    4   12    0    6]
 [  72    2    1    0    2    2    0    2   37  836    2    0    1   19   12    2    1    3    2    0    5]
 [   0    4    8    9    1    3    4    5   14    2  954    1    1   14   20    0    1    0   15    1    7]
 [   2    2    4    1    0   10    2    3    3    0    3  860   41    9    2   11    5   24    3   15   11]
 [   3    0    2    4    0    5    0    5    0    0    0   74  831    6    2    6    2   35    6    3   11]
 [   0    0    3    0    3    4    0    2   19   25   11   13    3  878    6    4    3    6    2   11    8]
 [   8    1    3    8   11    0    0    0   33   10    2    5    3   11  974    1    4    4   14    0    6]
 [   2    3    0    0    5    3    8    0    0    1    1   13   15    3    1  968   15   16    2    2    8]
 [   6    5    2    3    8    7    0    1    8    0    3    9    2    8    2   12  963    6    2    9   16]
 [   4    1    0    3    1    1    0    0    0    3    0   21   26    5    5    7    4  910    5    5    4]
 [   5    9   12   13    2    2    1   21    7    1   10    1    2    0   23    1    2    3  934    2    7]
 [   3    2    7    1    0    6   17   23    0    0    0   22   16    4    0    7    9    3    5  954    9]
 [ 238  240  250  141  287  206  109  250  168  169  241  229  442  372  319  205  373  190  267  362 8874]]

2024-06-06 03:02:58,207 - ==> Best [Top1: 77.995   Top5: 95.943   Sparsity:0.00   Params: 169472 on epoch: 177]
2024-06-06 03:02:58,207 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 03:02:58,215 - 

2024-06-06 03:02:58,215 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 03:03:04,274 - Epoch: [181][  100/ 1218]    Overall Loss 0.484202    Objective Loss 0.484202                                        LR 0.000125    Time 0.060572    
2024-06-06 03:03:08,979 - Epoch: [181][  200/ 1218]    Overall Loss 0.489067    Objective Loss 0.489067                                        LR 0.000125    Time 0.053801    
2024-06-06 03:03:13,575 - Epoch: [181][  300/ 1218]    Overall Loss 0.490182    Objective Loss 0.490182                                        LR 0.000125    Time 0.051182    
2024-06-06 03:03:18,234 - Epoch: [181][  400/ 1218]    Overall Loss 0.491275    Objective Loss 0.491275                                        LR 0.000125    Time 0.050030    
2024-06-06 03:03:23,018 - Epoch: [181][  500/ 1218]    Overall Loss 0.490635    Objective Loss 0.490635                                        LR 0.000125    Time 0.049587    
2024-06-06 03:03:27,681 - Epoch: [181][  600/ 1218]    Overall Loss 0.491479    Objective Loss 0.491479                                        LR 0.000125    Time 0.049091    
2024-06-06 03:03:32,332 - Epoch: [181][  700/ 1218]    Overall Loss 0.493990    Objective Loss 0.493990                                        LR 0.000125    Time 0.048720    
2024-06-06 03:03:37,237 - Epoch: [181][  800/ 1218]    Overall Loss 0.493287    Objective Loss 0.493287                                        LR 0.000125    Time 0.048760    
2024-06-06 03:03:42,060 - Epoch: [181][  900/ 1218]    Overall Loss 0.492800    Objective Loss 0.492800                                        LR 0.000125    Time 0.048699    
2024-06-06 03:03:46,779 - Epoch: [181][ 1000/ 1218]    Overall Loss 0.492260    Objective Loss 0.492260                                        LR 0.000125    Time 0.048547    
2024-06-06 03:03:51,510 - Epoch: [181][ 1100/ 1218]    Overall Loss 0.492697    Objective Loss 0.492697                                        LR 0.000125    Time 0.048432    
2024-06-06 03:03:56,210 - Epoch: [181][ 1200/ 1218]    Overall Loss 0.492970    Objective Loss 0.492970                                        LR 0.000125    Time 0.048311    
2024-06-06 03:03:56,981 - Epoch: [181][ 1218/ 1218]    Overall Loss 0.493117    Objective Loss 0.493117    Top1 78.239609    Top5 97.310513    LR 0.000125    Time 0.048230    
2024-06-06 03:03:57,174 - --- validate (epoch=181)-----------
2024-06-06 03:03:57,174 - 34633 samples (256 per mini-batch)
2024-06-06 03:04:02,730 - Epoch: [181][  100/  136]    Loss 0.451613    Top1 78.074219    Top5 96.304688    
2024-06-06 03:04:04,465 - Epoch: [181][  136/  136]    Loss 0.452063    Top1 78.156671    Top5 96.402275    
2024-06-06 03:04:04,658 - ==> Top1: 78.157    Top5: 96.402    Loss: 0.452

2024-06-06 03:04:04,659 - ==> Confusion:
[[ 813    0    2    2   11    4    1    1    9   55    0    4    2    4    5    2    3    1    0    3    9]
 [   2  932    2    2   17   27    2   13    5    1    8    5    5    2    6    1   10    1    9    4    9]
 [   8    2  845    6    7    4   30    9    2    4    8    5    3    2    1    5   11    1    3    2   12]
 [   3    5   16  884    5    7    5    1    3    2   15    1    4    3   24    2    1   10   18    2    5]
 [  22    5    3    1  947   12    3    0    3   12    2    1    0    5    5    7    6    2    6    1   11]
 [   1   27    4    4   11  872    3   34    4    3    4   14   10   21    3    1    3    3    4    5   12]
 [   1    4   27    1    1    5 1008    8    1    1    5    2    0    1    1    1    2    2    3    4    8]
 [   4   13    9    5    1   40    5  905    1    2    8   16    3    4    2    3    1    5   29   15    6]
 [  12    6    1    3    2    2    1    0  823   47   20    0    7   17   24    1    4    5   13    2   12]
 [  74    1    3    1    4    5    2    2   50  806    1    1    4   25   10    2    3    1    1    0    5]
 [   2    3    4   11    2    2    5    8   12    0  963    0    1   12    9    1    2    0   14    2   11]
 [   3    3    2    0    1   13    4    5    1    3    0  846   40   15    2   13    8   22    0   24    6]
 [   3    1    3    6    0    4    3    5    3    0    1   65  833    2    1    7    1   31    3    6   17]
 [   2    0    4    3    6   12    0    3   11   20    8   15    5  879    8    3    4    3    1    5    9]
 [  17    2    0   20    7    2    0    1   27   11   10    1    2   13  955    0    3    3   12    2   10]
 [   2    2    3    2    3    0    8    0    1    0    0   19    7    5    0  973   14   13    0    4   10]
 [   4   13   11    1    7    5    3    0    3    0    6   12    8    4    1   13  953    4    2    7   15]
 [   1    0    3    2    0    3    1    2    1    1    0   20   25    2    3   10    2  921    1    1    6]
 [   3    9    7   17    2    3    0   19    2    0   10    1    8    1   18    0    3    1  943    5    6]
 [   2    8    5    0    0   13   22    9    1    0    3   26   18    7    1    8    9    4    1  943    8]
 [ 248  287  291  137  304  249  164  240  142  111  203  226  402  380  263  171  343  198  223  326 9024]]

2024-06-06 03:04:04,661 - ==> Best [Top1: 78.157   Top5: 96.402   Sparsity:0.00   Params: 169472 on epoch: 181]
2024-06-06 03:04:04,661 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 03:04:04,676 - 

2024-06-06 03:04:04,677 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 03:04:10,947 - Epoch: [182][  100/ 1218]    Overall Loss 0.497472    Objective Loss 0.497472                                        LR 0.000125    Time 0.062679    
2024-06-06 03:04:15,645 - Epoch: [182][  200/ 1218]    Overall Loss 0.498520    Objective Loss 0.498520                                        LR 0.000125    Time 0.054823    
2024-06-06 03:04:20,168 - Epoch: [182][  300/ 1218]    Overall Loss 0.495861    Objective Loss 0.495861                                        LR 0.000125    Time 0.051618    
2024-06-06 03:04:24,804 - Epoch: [182][  400/ 1218]    Overall Loss 0.498139    Objective Loss 0.498139                                        LR 0.000125    Time 0.050299    
2024-06-06 03:04:29,359 - Epoch: [182][  500/ 1218]    Overall Loss 0.496816    Objective Loss 0.496816                                        LR 0.000125    Time 0.049346    
2024-06-06 03:04:34,088 - Epoch: [182][  600/ 1218]    Overall Loss 0.497330    Objective Loss 0.497330                                        LR 0.000125    Time 0.049001    
2024-06-06 03:04:38,704 - Epoch: [182][  700/ 1218]    Overall Loss 0.496458    Objective Loss 0.496458                                        LR 0.000125    Time 0.048592    
2024-06-06 03:04:43,426 - Epoch: [182][  800/ 1218]    Overall Loss 0.496289    Objective Loss 0.496289                                        LR 0.000125    Time 0.048418    
2024-06-06 03:04:48,168 - Epoch: [182][  900/ 1218]    Overall Loss 0.495016    Objective Loss 0.495016                                        LR 0.000125    Time 0.048305    
2024-06-06 03:04:52,870 - Epoch: [182][ 1000/ 1218]    Overall Loss 0.495695    Objective Loss 0.495695                                        LR 0.000125    Time 0.048175    
2024-06-06 03:04:57,512 - Epoch: [182][ 1100/ 1218]    Overall Loss 0.495384    Objective Loss 0.495384                                        LR 0.000125    Time 0.048014    
2024-06-06 03:05:02,145 - Epoch: [182][ 1200/ 1218]    Overall Loss 0.495724    Objective Loss 0.495724                                        LR 0.000125    Time 0.047871    
2024-06-06 03:05:02,995 - Epoch: [182][ 1218/ 1218]    Overall Loss 0.495312    Objective Loss 0.495312    Top1 77.017115    Top5 96.577017    LR 0.000125    Time 0.047862    
2024-06-06 03:05:03,180 - --- validate (epoch=182)-----------
2024-06-06 03:05:03,180 - 34633 samples (256 per mini-batch)
2024-06-06 03:05:08,747 - Epoch: [182][  100/  136]    Loss 0.455321    Top1 77.800781    Top5 96.164062    
2024-06-06 03:05:10,404 - Epoch: [182][  136/  136]    Loss 0.454444    Top1 77.997863    Top5 96.252129    
2024-06-06 03:05:10,581 - ==> Top1: 77.998    Top5: 96.252    Loss: 0.454

2024-06-06 03:05:10,582 - ==> Confusion:
[[ 808    2    1    1    9    0    1    2    9   71    3    2    2    3    4    1    4    2    2    1    3]
 [   2  933    5    2   20   22    3   17    3    0    4    2    6    5    9    3    3    1   12    5    6]
 [   5    1  860    5    3    2   18   13    0    9    5    4    4    5    7    4    3    2    4    5   11]
 [   6    0   18  857    4    6    4    3    2    4   17    1    8    4   39    1    3    8   19    1   11]
 [  28   10    4    0  928   11    0    1    4   24    3    3    1    5   11    3    8    0    3    2    5]
 [   4   35    3    3   11  866    5   28    1    6    1    9   11   17    4    4    7    3    5   10   10]
 [   1    8   27    0    1    6  979   10    1    1    5    8    6    1    1    8    3    4    2   10    4]
 [   6   18    9    2    1   23    4  930    3    1    2   12    5    5    1    0    1    1   29   15    9]
 [  13    2    1    1    1    2    0    4  828   57   19    1    4   24   17    2    6    3    9    2    6]
 [  80    1    1    0    6    3    1    0   47  824    0    1    2   18    4    1    1    1    2    1    7]
 [   2    4   13   13    0    6    1    6   26    4  929    1    3   19   10    0    1    1   18    1    6]
 [   4    2    1    0    1    9    3    5    1    3    0  875   38   11    2   10    4   18    2   14    8]
 [   1    1    1    5    1    2    0    7    2    0    0   74  834    7    4    7    6   21    2    8   12]
 [   4    1    1    0    6    9    0    5   10   21    2   15    5  897    3    3    3    2    4    3    7]
 [  11    0    0   14   11    1    1    5   32   12    6    1    4   11  970    0    1    5    9    0    4]
 [   1    2    7    0    2    1    5    2    1    1    1   22   12    2    0  977   11    8    1    2    8]
 [   3    8    5    4    7    9    0    3    7    1    2   13    4    4    3   12  963    1    2    4   17]
 [   1    2    1    1    1    2    0    1    1    1    0   22   36    5    4   13    0  906    0    4    4]
 [   7    5    9   13    1    2    0   22   11    0    4    3    5    1   16    0    1    0  947    2    9]
 [   4    7    3    1    4    9   14   14    2    0    0   25   10    7    1    8    9    2    3  957    8]
 [ 263  271  313  135  289  232  117  274  160  144  186  220  451  391  274  198  318  156  239  356 8945]]

2024-06-06 03:05:10,584 - ==> Best [Top1: 78.157   Top5: 96.402   Sparsity:0.00   Params: 169472 on epoch: 181]
2024-06-06 03:05:10,584 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 03:05:10,592 - 

2024-06-06 03:05:10,592 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 03:05:16,833 - Epoch: [183][  100/ 1218]    Overall Loss 0.511520    Objective Loss 0.511520                                        LR 0.000125    Time 0.062391    
2024-06-06 03:05:21,451 - Epoch: [183][  200/ 1218]    Overall Loss 0.501420    Objective Loss 0.501420                                        LR 0.000125    Time 0.054276    
2024-06-06 03:05:26,072 - Epoch: [183][  300/ 1218]    Overall Loss 0.498592    Objective Loss 0.498592                                        LR 0.000125    Time 0.051581    
2024-06-06 03:05:30,755 - Epoch: [183][  400/ 1218]    Overall Loss 0.498870    Objective Loss 0.498870                                        LR 0.000125    Time 0.050390    
2024-06-06 03:05:35,483 - Epoch: [183][  500/ 1218]    Overall Loss 0.498582    Objective Loss 0.498582                                        LR 0.000125    Time 0.049763    
2024-06-06 03:05:40,280 - Epoch: [183][  600/ 1218]    Overall Loss 0.498026    Objective Loss 0.498026                                        LR 0.000125    Time 0.049460    
2024-06-06 03:05:44,981 - Epoch: [183][  700/ 1218]    Overall Loss 0.497154    Objective Loss 0.497154                                        LR 0.000125    Time 0.049107    
2024-06-06 03:05:50,205 - Epoch: [183][  800/ 1218]    Overall Loss 0.498170    Objective Loss 0.498170                                        LR 0.000125    Time 0.049496    
2024-06-06 03:05:55,052 - Epoch: [183][  900/ 1218]    Overall Loss 0.496563    Objective Loss 0.496563                                        LR 0.000125    Time 0.049379    
2024-06-06 03:05:59,734 - Epoch: [183][ 1000/ 1218]    Overall Loss 0.496796    Objective Loss 0.496796                                        LR 0.000125    Time 0.049122    
2024-06-06 03:06:04,389 - Epoch: [183][ 1100/ 1218]    Overall Loss 0.495315    Objective Loss 0.495315                                        LR 0.000125    Time 0.048887    
2024-06-06 03:06:08,973 - Epoch: [183][ 1200/ 1218]    Overall Loss 0.496042    Objective Loss 0.496042                                        LR 0.000125    Time 0.048629    
2024-06-06 03:06:09,764 - Epoch: [183][ 1218/ 1218]    Overall Loss 0.495658    Objective Loss 0.495658    Top1 75.550122    Top5 93.887531    LR 0.000125    Time 0.048560    
2024-06-06 03:06:09,959 - --- validate (epoch=183)-----------
2024-06-06 03:06:09,959 - 34633 samples (256 per mini-batch)
2024-06-06 03:06:15,526 - Epoch: [183][  100/  136]    Loss 0.449620    Top1 78.019531    Top5 96.445312    
2024-06-06 03:06:17,188 - Epoch: [183][  136/  136]    Loss 0.447926    Top1 78.009413    Top5 96.347414    
2024-06-06 03:06:17,389 - ==> Top1: 78.009    Top5: 96.347    Loss: 0.448

2024-06-06 03:06:17,390 - ==> Confusion:
[[ 801    2    3    0   11    1    1    1    9   67    2    1    3    2    7    5    6    0    2    1    6]
 [   4  947    4    2   16   21    3   12    5    1    3    4    7    0    7    2    6    2    9    1    7]
 [   7    3  846   12    4    3   29   13    2    6    6    4    7    2    1    4    7    2    2    1    9]
 [   5    2   17  874    7    9    5    6    3    5   17    1    4    2   27    4    0    6    9    4    9]
 [  23   13    6    3  938    7    1    1    5   11    0    4    3    4   11    5   10    1    4    1    3]
 [   4   35    2    1   14  843    7   44    3    3    2   14    5   23    5    1   13    4    5    6    9]
 [   2    7   23    1    4    5  992    5    2    3    1    1    5    1    0    6    3    2    2   13    8]
 [   4   13   13    6    3   32    4  910    4    0    7   10    6    4    0    1    3    2   34   16    5]
 [  10    5    0    2    3    3    0    1  841   51   14    4    5   18   22    0    4    1    9    1    8]
 [  91    2    4    0    7    2    1    0   49  801    0    0    1   19   11    3    0    3    1    0    6]
 [   1    6    5   11    0    1    3    2   16    3  963    0    3   18    8    0    3    1   10    3    7]
 [   2    2    0    0    0   15    3    1    1    2    4  868   33    4    2   14    4   23    3   23    7]
 [   0    2    1    8    1    5    2    8    4    0    0   76  823    1    1    7    6   34    6    5    5]
 [   3    2    1    1    9    8    1    3   22   22   10   11    5  874    4    3    2    4    0    6   10]
 [  12    2    5   19    6    0    0    0   33   10    6    3    2    8  965    1    2    1   14    0    9]
 [   0    0    6    1    1    0    6    1    2    0    1   15    9    1    1  981   14   15    3    6    3]
 [   2   11    3    2   11    6    2    0    4    0    3   11    8    3    2   14  963    3    2    5   17]
 [   0    3    3    3    1    2    0    1    0    3    2   16   30    4    2   11    3  910    4    2    5]
 [   1    5    8   18    4    4    2   24    6    0    6    1    5    1   18    0    0    1  943    3    8]
 [   1    8    4    0    1   10   18    9    0    0    1   26   13    5    1    7    4    4    5  960   11]
 [ 252  289  257  146  289  183  132  224  174  170  210  243  452  342  284  195  344  159  244  369 8974]]

2024-06-06 03:06:17,392 - ==> Best [Top1: 78.157   Top5: 96.402   Sparsity:0.00   Params: 169472 on epoch: 181]
2024-06-06 03:06:17,392 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 03:06:17,399 - 

2024-06-06 03:06:17,399 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 03:06:23,830 - Epoch: [184][  100/ 1218]    Overall Loss 0.498338    Objective Loss 0.498338                                        LR 0.000125    Time 0.064282    
2024-06-06 03:06:28,656 - Epoch: [184][  200/ 1218]    Overall Loss 0.487964    Objective Loss 0.487964                                        LR 0.000125    Time 0.056261    
2024-06-06 03:06:33,351 - Epoch: [184][  300/ 1218]    Overall Loss 0.485504    Objective Loss 0.485504                                        LR 0.000125    Time 0.053152    
2024-06-06 03:06:37,996 - Epoch: [184][  400/ 1218]    Overall Loss 0.484581    Objective Loss 0.484581                                        LR 0.000125    Time 0.051473    
2024-06-06 03:06:42,775 - Epoch: [184][  500/ 1218]    Overall Loss 0.490041    Objective Loss 0.490041                                        LR 0.000125    Time 0.050732    
2024-06-06 03:06:47,614 - Epoch: [184][  600/ 1218]    Overall Loss 0.490276    Objective Loss 0.490276                                        LR 0.000125    Time 0.050338    
2024-06-06 03:06:52,335 - Epoch: [184][  700/ 1218]    Overall Loss 0.490344    Objective Loss 0.490344                                        LR 0.000125    Time 0.049886    
2024-06-06 03:06:57,152 - Epoch: [184][  800/ 1218]    Overall Loss 0.488335    Objective Loss 0.488335                                        LR 0.000125    Time 0.049669    
2024-06-06 03:07:01,834 - Epoch: [184][  900/ 1218]    Overall Loss 0.489833    Objective Loss 0.489833                                        LR 0.000125    Time 0.049351    
2024-06-06 03:07:06,530 - Epoch: [184][ 1000/ 1218]    Overall Loss 0.491238    Objective Loss 0.491238                                        LR 0.000125    Time 0.049109    
2024-06-06 03:07:11,267 - Epoch: [184][ 1100/ 1218]    Overall Loss 0.492692    Objective Loss 0.492692                                        LR 0.000125    Time 0.048950    
2024-06-06 03:07:15,828 - Epoch: [184][ 1200/ 1218]    Overall Loss 0.491817    Objective Loss 0.491817                                        LR 0.000125    Time 0.048670    
2024-06-06 03:07:16,597 - Epoch: [184][ 1218/ 1218]    Overall Loss 0.491989    Objective Loss 0.491989    Top1 76.772616    Top5 95.354523    LR 0.000125    Time 0.048582    
2024-06-06 03:07:16,803 - --- validate (epoch=184)-----------
2024-06-06 03:07:16,803 - 34633 samples (256 per mini-batch)
2024-06-06 03:07:22,420 - Epoch: [184][  100/  136]    Loss 0.451448    Top1 77.992188    Top5 96.031250    
2024-06-06 03:07:24,144 - Epoch: [184][  136/  136]    Loss 0.461444    Top1 77.634048    Top5 95.951838    
2024-06-06 03:07:24,322 - ==> Top1: 77.634    Top5: 95.952    Loss: 0.461

2024-06-06 03:07:24,324 - ==> Confusion:
[[ 824    3    5    1    4    6    0    1    7   46    2    2    3    5    7    1    0    2    5    0    7]
 [   4  929    2    0   17   27    6   19    4    1    2    2    1    4    6    4    7    3   12    5    8]
 [   7    3  849    9    4    2   26   14    1    4    5    1    2    4    5   11    4    1    6    6    6]
 [   3    2   16  853    3    7    5    5    5    2   12    5   11    4   43    2    0   11   18    2    7]
 [  23    7    2    2  937   10    1    3    2    8    1    2    2    4   12    4    8    2   10    3   11]
 [   3   31    1    5   13  859    3   36    2    6    1   18    3   23    4    3    9    3    4    8    8]
 [   4    8   32    4    1    4  976    7    2    0    6    2    5    1    0    6    5    6    2    9    6]
 [   3   15   14    0    3   38    5  916    4    4    0   16    6    3    1    1    1    4   30   12    1]
 [  15    5    1    0    1    2    0    2  849   48    6    3    5   20   23    0    1    2   13    0    6]
 [ 100    1    1    1    3    3    0    2   46  789    2    5    2   18    9    2    1    4    2    1    9]
 [   1    9   14   17    2    5    9    7   29    1  911    0    4   14   15    0    1    0   17    2    6]
 [   3    0    2    0    0   13    1    3    0    1    0  869   44   12    2   12    9   18    2   17    3]
 [   2    2    3    4    0   10    0    6    3    1    1   60  831    4    2    8    2   30    6   10   10]
 [   6    0    3    0    5   16    0    6   19   16    7   11   11  869    2    2    3    4    0   13    8]
 [  14    1    2   18   11    0    0    2   32    8    5    1    3    6  963    0    3    3   18    1    7]
 [   3    2    8    0    6    1    1    1    1    0    0   22   13    4    0  960   14   24    0    4    2]
 [   3    7    5    1   12   11    1    0    6    1    3   10    1    4    1   10  969    2    4    8   13]
 [   5    0    0    1    3    4    2    2    0    1    1   22   38    1    2    7    0  901    6    3    6]
 [   2    3    5   14    5    4    2   21    9    1    6    3    6    1   17    0    3    1  947    2    6]
 [   1    9    2    0    3   11   13   15    2    2    1   24   10    5    0    4   11    7    3  958    7]
 [ 307  264  281  163  276  235  115  260  152  145  171  223  439  377  289  180  355  164  273  335 8928]]

2024-06-06 03:07:24,325 - ==> Best [Top1: 78.157   Top5: 96.402   Sparsity:0.00   Params: 169472 on epoch: 181]
2024-06-06 03:07:24,325 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 03:07:24,333 - 

2024-06-06 03:07:24,333 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 03:07:30,643 - Epoch: [185][  100/ 1218]    Overall Loss 0.504631    Objective Loss 0.504631                                        LR 0.000125    Time 0.063073    
2024-06-06 03:07:35,426 - Epoch: [185][  200/ 1218]    Overall Loss 0.501030    Objective Loss 0.501030                                        LR 0.000125    Time 0.055444    
2024-06-06 03:07:39,994 - Epoch: [185][  300/ 1218]    Overall Loss 0.494098    Objective Loss 0.494098                                        LR 0.000125    Time 0.052186    
2024-06-06 03:07:44,603 - Epoch: [185][  400/ 1218]    Overall Loss 0.493550    Objective Loss 0.493550                                        LR 0.000125    Time 0.050657    
2024-06-06 03:07:49,506 - Epoch: [185][  500/ 1218]    Overall Loss 0.493118    Objective Loss 0.493118                                        LR 0.000125    Time 0.050328    
2024-06-06 03:07:54,459 - Epoch: [185][  600/ 1218]    Overall Loss 0.493050    Objective Loss 0.493050                                        LR 0.000125    Time 0.050192    
2024-06-06 03:07:59,481 - Epoch: [185][  700/ 1218]    Overall Loss 0.493670    Objective Loss 0.493670                                        LR 0.000125    Time 0.050192    
2024-06-06 03:08:04,202 - Epoch: [185][  800/ 1218]    Overall Loss 0.493961    Objective Loss 0.493961                                        LR 0.000125    Time 0.049818    
2024-06-06 03:08:08,759 - Epoch: [185][  900/ 1218]    Overall Loss 0.493230    Objective Loss 0.493230                                        LR 0.000125    Time 0.049344    
2024-06-06 03:08:13,341 - Epoch: [185][ 1000/ 1218]    Overall Loss 0.493313    Objective Loss 0.493313                                        LR 0.000125    Time 0.048990    
2024-06-06 03:08:18,144 - Epoch: [185][ 1100/ 1218]    Overall Loss 0.492538    Objective Loss 0.492538                                        LR 0.000125    Time 0.048902    
2024-06-06 03:08:22,814 - Epoch: [185][ 1200/ 1218]    Overall Loss 0.491857    Objective Loss 0.491857                                        LR 0.000125    Time 0.048717    
2024-06-06 03:08:23,614 - Epoch: [185][ 1218/ 1218]    Overall Loss 0.491846    Objective Loss 0.491846    Top1 78.239609    Top5 96.577017    LR 0.000125    Time 0.048653    
2024-06-06 03:08:23,817 - --- validate (epoch=185)-----------
2024-06-06 03:08:23,817 - 34633 samples (256 per mini-batch)
2024-06-06 03:08:29,266 - Epoch: [185][  100/  136]    Loss 0.464050    Top1 78.359375    Top5 96.144531    
2024-06-06 03:08:31,021 - Epoch: [185][  136/  136]    Loss 0.463487    Top1 78.454076    Top5 96.278116    
2024-06-06 03:08:31,193 - ==> Top1: 78.454    Top5: 96.278    Loss: 0.463

2024-06-06 03:08:31,195 - ==> Confusion:
[[ 797    3    3    0   11    3    0    2   11   72    0    2    1    6    3    2    2    2    3    1    7]
 [   6  951    0    0   17   18    2   16    4    2    3    1    3    1    4    3   14    0    7    4    7]
 [   9    1  855    9    5    2   25    5    2    6    5    9    4    2    1    4    7    1    5    3   10]
 [   3    2   20  878    3    5    2    4    3    1   15    1    6    3   28    3    2    9   12    3   13]
 [  26   13    1    3  940    9    2    2    0   10    3    1    1    5    8    5   11    0    3    3    8]
 [   2   41    4    4   15  858    3   39    2   11    1   10    6   15    6    0    5    1    5    3   12]
 [   2    7   28    0    1    6  975   11    1    5    2    4    2    0    1   12    2    2    3   13    9]
 [   2   15   10    1    0   27    5  929    7    2    6    8    5    1    1    1    1    1   27   17   11]
 [  19    7    1    1    0    2    1    0  844   58   10    2    6   14   20    0    4    1    8    0    4]
 [  71    0    2    1   11    1    0    0   59  806    2    5    0   17    8    1    1    2    3    0   11]
 [   4    5   12   12    0    4    5    3   24    3  932    0    2   17   11    0    2    2   16    3    7]
 [   3    2    5    0    2   13    2    6    1    2    0  861   32   10    3   14   10   18    4   18    5]
 [   2    2    2    4    1    9    0    3    5    0    5   60  817    4    3    8    4   40    5    5   16]
 [   7    3    1    0    7   12    2    4   22   16    7    9    3  864   13    1    3    4    1    8   14]
 [   8    3    2    8    6    2    0    1   23   13    3    3    4    8  980    0    2    1   22    0    9]
 [   2    4    3    0    5    1    5    3    0    1    0   15    9    4    0  968   14   15    2    3   12]
 [   3   10    1    1   14    9    0    1    9    0    1    6    2    4    1   10  968    2    5    9   16]
 [   2    1    1    0    1    1    0    4    3    5    0   20   37    4    3   16    2  891    2    4    8]
 [   2    9    6   11    3    1    3   17   10    3   14    1    6    1   23    0    1    1  938    1    7]
 [   2    2    3    0    0   12   16   14    4    1    1   25    4    7    3    8   14    4    8  950   10]
 [ 252  275  279  151  298  237  125  222  150  138  200  197  376  352  289  171  367  135  250  299 9169]]

2024-06-06 03:08:31,197 - ==> Best [Top1: 78.454   Top5: 96.278   Sparsity:0.00   Params: 169472 on epoch: 185]
2024-06-06 03:08:31,197 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 03:08:31,210 - 

2024-06-06 03:08:31,210 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 03:08:37,225 - Epoch: [186][  100/ 1218]    Overall Loss 0.491668    Objective Loss 0.491668                                        LR 0.000125    Time 0.060123    
2024-06-06 03:08:41,884 - Epoch: [186][  200/ 1218]    Overall Loss 0.495440    Objective Loss 0.495440                                        LR 0.000125    Time 0.053350    
2024-06-06 03:08:46,510 - Epoch: [186][  300/ 1218]    Overall Loss 0.494313    Objective Loss 0.494313                                        LR 0.000125    Time 0.050980    
2024-06-06 03:08:51,263 - Epoch: [186][  400/ 1218]    Overall Loss 0.495192    Objective Loss 0.495192                                        LR 0.000125    Time 0.050097    
2024-06-06 03:08:55,991 - Epoch: [186][  500/ 1218]    Overall Loss 0.493588    Objective Loss 0.493588                                        LR 0.000125    Time 0.049530    
2024-06-06 03:09:00,901 - Epoch: [186][  600/ 1218]    Overall Loss 0.494619    Objective Loss 0.494619                                        LR 0.000125    Time 0.049455    
2024-06-06 03:09:05,726 - Epoch: [186][  700/ 1218]    Overall Loss 0.495769    Objective Loss 0.495769                                        LR 0.000125    Time 0.049280    
2024-06-06 03:09:10,433 - Epoch: [186][  800/ 1218]    Overall Loss 0.495739    Objective Loss 0.495739                                        LR 0.000125    Time 0.049002    
2024-06-06 03:09:14,987 - Epoch: [186][  900/ 1218]    Overall Loss 0.495821    Objective Loss 0.495821                                        LR 0.000125    Time 0.048616    
2024-06-06 03:09:19,565 - Epoch: [186][ 1000/ 1218]    Overall Loss 0.495397    Objective Loss 0.495397                                        LR 0.000125    Time 0.048331    
2024-06-06 03:09:24,320 - Epoch: [186][ 1100/ 1218]    Overall Loss 0.495365    Objective Loss 0.495365                                        LR 0.000125    Time 0.048258    
2024-06-06 03:09:28,909 - Epoch: [186][ 1200/ 1218]    Overall Loss 0.494877    Objective Loss 0.494877                                        LR 0.000125    Time 0.048059    
2024-06-06 03:09:29,771 - Epoch: [186][ 1218/ 1218]    Overall Loss 0.495070    Objective Loss 0.495070    Top1 77.506112    Top5 96.577017    LR 0.000125    Time 0.048057    
2024-06-06 03:09:29,961 - --- validate (epoch=186)-----------
2024-06-06 03:09:29,961 - 34633 samples (256 per mini-batch)
2024-06-06 03:09:35,472 - Epoch: [186][  100/  136]    Loss 0.461578    Top1 77.679688    Top5 96.011719    
2024-06-06 03:09:37,157 - Epoch: [186][  136/  136]    Loss 0.464287    Top1 77.602287    Top5 96.006699    
2024-06-06 03:09:37,327 - ==> Top1: 77.602    Top5: 96.007    Loss: 0.464

2024-06-06 03:09:37,328 - ==> Confusion:
[[ 821    3    3    2    8    3    0    2    3   53    0    3    1    6    3    4    2    4    1    2    7]
 [   4  912    2    2   32   23    7   21    7    1    5    5    3    0    5    3    8    3    9    4    7]
 [   8    4  860   10    6    2   16    8    2    6    4    5    4    8    1    9    3    1    1    3    9]
 [   3    1   16  876    4    9    4    2    1    1   13    2    9    4   30    3    2   11   15    0   10]
 [  27   11    5    1  928    8    1    3    2   11    1    1    3    3   10    8    7    3   12    2    7]
 [   7   22    4    4   15  859    5   27    5    3    4   16    8   18    2    3   11    5    4    7   14]
 [   3    4   31    3    3    6  975    4    1    0    1    3    6    4    1   16    5    2    2    9    7]
 [   3   25   16    3    3   34    7  890    2    0    3   15    7    5    2    1    0    2   31   18   10]
 [  17    4    0    1    2    1    0    0  844   56   12    1    7   10   21    0    4    5   10    2    5]
 [  97    0    2    1    7    2    1    1   47  802    1    1    2   17    7    1    3    2    1    0    6]
 [   1    4   12   21    1    6    9    6   20    2  927    1    2   19   15    1    4    1    9    1    2]
 [   6    2    0    0    2    5    2    3    1    4    0  867   40    7    2   12    8   18    4   19    9]
 [   1    2    3    3    0    2    1    4    5    1    3   61  831    2    4   10    4   40    2    7    9]
 [   4    1    3    0    3   13    1    4   21   22    8   13    9  862    6    6    4    4    2    8    7]
 [  11    2    2   16   14    3    0    3   39   10    6    2    3    7  950    0    1    5   15    0    9]
 [   3    1    4    0    3    0    2    0    0    1    0   19    6    0    0  981   15   15    3    5    8]
 [   7    9    3    0    9    7    3    0    4    0    2    7    6    3    1   15  965    3    3    6   19]
 [   3    0    2    4    1    1    0    1    2    1    0   16   31    2    3   14    1  910    1    3    9]
 [   3    7   12    9    5    4    2   24    2    2    3    3    3    2   26    1    2    2  935    4    7]
 [   3    5    4    2    1   10   12    8    1    1    0   29   13    9    1    8   13    5    7  948    8]
 [ 374  226  299  135  292  175  138  203  161  146  170  217  427  327  312  224  427  188  221  337 8933]]

2024-06-06 03:09:37,330 - ==> Best [Top1: 78.454   Top5: 96.278   Sparsity:0.00   Params: 169472 on epoch: 185]
2024-06-06 03:09:37,330 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 03:09:37,337 - 

2024-06-06 03:09:37,337 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 03:09:43,791 - Epoch: [187][  100/ 1218]    Overall Loss 0.500574    Objective Loss 0.500574                                        LR 0.000125    Time 0.064520    
2024-06-06 03:09:48,502 - Epoch: [187][  200/ 1218]    Overall Loss 0.493388    Objective Loss 0.493388                                        LR 0.000125    Time 0.055801    
2024-06-06 03:09:53,349 - Epoch: [187][  300/ 1218]    Overall Loss 0.493522    Objective Loss 0.493522                                        LR 0.000125    Time 0.053353    
2024-06-06 03:09:58,000 - Epoch: [187][  400/ 1218]    Overall Loss 0.490327    Objective Loss 0.490327                                        LR 0.000125    Time 0.051636    
2024-06-06 03:10:02,566 - Epoch: [187][  500/ 1218]    Overall Loss 0.493885    Objective Loss 0.493885                                        LR 0.000125    Time 0.050439    
2024-06-06 03:10:07,228 - Epoch: [187][  600/ 1218]    Overall Loss 0.492668    Objective Loss 0.492668                                        LR 0.000125    Time 0.049799    
2024-06-06 03:10:11,888 - Epoch: [187][  700/ 1218]    Overall Loss 0.492038    Objective Loss 0.492038                                        LR 0.000125    Time 0.049338    
2024-06-06 03:10:16,487 - Epoch: [187][  800/ 1218]    Overall Loss 0.492598    Objective Loss 0.492598                                        LR 0.000125    Time 0.048918    
2024-06-06 03:10:21,534 - Epoch: [187][  900/ 1218]    Overall Loss 0.492455    Objective Loss 0.492455                                        LR 0.000125    Time 0.049089    
2024-06-06 03:10:26,068 - Epoch: [187][ 1000/ 1218]    Overall Loss 0.491855    Objective Loss 0.491855                                        LR 0.000125    Time 0.048712    
2024-06-06 03:10:30,693 - Epoch: [187][ 1100/ 1218]    Overall Loss 0.490985    Objective Loss 0.490985                                        LR 0.000125    Time 0.048487    
2024-06-06 03:10:35,266 - Epoch: [187][ 1200/ 1218]    Overall Loss 0.490901    Objective Loss 0.490901                                        LR 0.000125    Time 0.048255    
2024-06-06 03:10:36,093 - Epoch: [187][ 1218/ 1218]    Overall Loss 0.490964    Objective Loss 0.490964    Top1 77.017115    Top5 95.354523    LR 0.000125    Time 0.048221    
2024-06-06 03:10:36,274 - --- validate (epoch=187)-----------
2024-06-06 03:10:36,275 - 34633 samples (256 per mini-batch)
2024-06-06 03:10:42,060 - Epoch: [187][  100/  136]    Loss 0.452246    Top1 77.296875    Top5 95.945312    
2024-06-06 03:10:43,750 - Epoch: [187][  136/  136]    Loss 0.449218    Top1 77.495452    Top5 95.957613    
2024-06-06 03:10:43,914 - ==> Top1: 77.495    Top5: 95.958    Loss: 0.449

2024-06-06 03:10:43,915 - ==> Confusion:
[[ 805    0    2    1   14    0    0    1    9   67    0    1    4    4    6    0    2    1    6    1    7]
 [   1  944    4    4   21   24    2   11    2    6    7    5    6    0    4    2    1    1   12    4    2]
 [   4    3  862    9    2    3   22    9    1    5    8    2    6    3    3    3    5    1    5    5    9]
 [   2    3   13  892    3    3    3    3    6    0   16    1    8    3   27    2    5    5   15    2    4]
 [  24   12    2    2  942    6    1    1    4   15    2    1    5    2    5    6    7    0    7    2    8]
 [   5   33    2    5   16  852    3   27    5    9    3   16   11   18    4    2    3    2    6    7   14]
 [   1    5   22    1    2    4  990    6    1    3    6    6    2    1    0    4    2    4    4   14    8]
 [   4   14    9    3    2   34    4  905    3    2    6    9    9    3    2    2    0    3   41   18    4]
 [  12    6    0    3    1    0    0    0  841   52   21    4    5   10   17    0    2    2   18    4    4]
 [  77    1    0    1    3    2    0    2   53  817    1    1    4   19    6    0    3    4    3    0    4]
 [   2    4    6   11    0    3    4    4   20    1  961    3    2    8   10    0    0    0   14    2    9]
 [   1    7    2    1    0   15    3    5    0    1    2  847   43   12    1   16    3   16    1   25   10]
 [   2    1    1    7    1    7    5    3    4    1    3   59  827    2    4    7    1   33   11    8    8]
 [   4    1    0    3    6    9    0    4   17   25   10    8    6  870    6    3    3    3    6    7   10]
 [  12    6    2   15    6    3    0    0   40   10    5    2    5    6  943    0    4    3   21    3   12]
 [   0    0    3    2    5    2   10    2    1    0    2   16   13    8    0  972    9   12    4    4    1]
 [   1   12   10    0   11    6    2    1    4    2    2    8    4    3    1   12  958    2    3    7   23]
 [   2    2    1    3    1    1    1    2    3    3    1   12   39    4    5   16    0  899    1    4    5]
 [   4   10   11   15    1    4    2   16    8    5    5    2    5    2   18    2    4    1  937    1    5]
 [   1    2    3    3    1    8   15    9    1    0    1   21   12    5    0    5   11    4    5  967   14]
 [ 278  285  262  183  320  211  134  202  174  187  239  197  466  321  320  176  319  150  285  415 8808]]

2024-06-06 03:10:43,917 - ==> Best [Top1: 78.454   Top5: 96.278   Sparsity:0.00   Params: 169472 on epoch: 185]
2024-06-06 03:10:43,917 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 03:10:43,925 - 

2024-06-06 03:10:43,925 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 03:10:49,990 - Epoch: [188][  100/ 1218]    Overall Loss 0.484336    Objective Loss 0.484336                                        LR 0.000125    Time 0.060627    
2024-06-06 03:10:54,780 - Epoch: [188][  200/ 1218]    Overall Loss 0.492980    Objective Loss 0.492980                                        LR 0.000125    Time 0.054256    
2024-06-06 03:10:59,611 - Epoch: [188][  300/ 1218]    Overall Loss 0.496561    Objective Loss 0.496561                                        LR 0.000125    Time 0.052268    
2024-06-06 03:11:04,166 - Epoch: [188][  400/ 1218]    Overall Loss 0.496944    Objective Loss 0.496944                                        LR 0.000125    Time 0.050585    
2024-06-06 03:11:08,732 - Epoch: [188][  500/ 1218]    Overall Loss 0.494857    Objective Loss 0.494857                                        LR 0.000125    Time 0.049596    
2024-06-06 03:11:13,529 - Epoch: [188][  600/ 1218]    Overall Loss 0.493800    Objective Loss 0.493800                                        LR 0.000125    Time 0.049321    
2024-06-06 03:11:18,346 - Epoch: [188][  700/ 1218]    Overall Loss 0.491976    Objective Loss 0.491976                                        LR 0.000125    Time 0.049145    
2024-06-06 03:11:23,126 - Epoch: [188][  800/ 1218]    Overall Loss 0.490174    Objective Loss 0.490174                                        LR 0.000125    Time 0.048976    
2024-06-06 03:11:28,086 - Epoch: [188][  900/ 1218]    Overall Loss 0.491097    Objective Loss 0.491097                                        LR 0.000125    Time 0.049043    
2024-06-06 03:11:32,820 - Epoch: [188][ 1000/ 1218]    Overall Loss 0.491547    Objective Loss 0.491547                                        LR 0.000125    Time 0.048871    
2024-06-06 03:11:37,619 - Epoch: [188][ 1100/ 1218]    Overall Loss 0.491254    Objective Loss 0.491254                                        LR 0.000125    Time 0.048788    
2024-06-06 03:11:42,289 - Epoch: [188][ 1200/ 1218]    Overall Loss 0.491834    Objective Loss 0.491834                                        LR 0.000125    Time 0.048613    
2024-06-06 03:11:43,160 - Epoch: [188][ 1218/ 1218]    Overall Loss 0.491832    Objective Loss 0.491832    Top1 79.462103    Top5 97.066015    LR 0.000125    Time 0.048609    
2024-06-06 03:11:43,333 - --- validate (epoch=188)-----------
2024-06-06 03:11:43,333 - 34633 samples (256 per mini-batch)
2024-06-06 03:11:49,065 - Epoch: [188][  100/  136]    Loss 0.459696    Top1 78.078125    Top5 96.359375    
2024-06-06 03:11:50,747 - Epoch: [188][  136/  136]    Loss 0.459179    Top1 78.096036    Top5 96.387838    
2024-06-06 03:11:50,942 - ==> Top1: 78.096    Top5: 96.388    Loss: 0.459

2024-06-06 03:11:50,943 - ==> Confusion:
[[ 799    1    9    0   16    2    0    3    8   70    1    3    0    3    3    1    3    0    2    0    7]
 [   3  922    3    2   35   19    2   17    3    1    9    4    4    0    4    0    9    1   12    3   10]
 [   7    4  855   10    6    1   18    6    1    4    5    3    5    3    6    7    5    1    7    6   10]
 [   5    2   17  891    2    4    3    1    1    2   11    1    7    5   24    3    4    5   19    2    7]
 [  22    7    3    2  947    5    0    0    3   15    2    3    2    6    9    4    7    1    7    1    8]
 [   9   32    2    2   26  829    4   35    2    4    2   20    8   23    2    3    9    4    5   11   11]
 [   2    6   25    3    2    3  986    6    1    0    2    5    2    1    0   10    5    4    4   14    5]
 [   2   18   17    3    3   33    6  892    5    0    4   15    3    4    0    0    5    1   40   18    8]
 [  12    2    0    0    1    3    1    1  808   67   17    3    6   27   29    0    2    1   14    1    7]
 [  86    1    0    0   10    3    1    0   45  801    0    2    0   23   10    0    2    2    3    4    8]
 [   3    4   10   13    3    3    2    8   17    1  953    1    0   12   12    1    0    1   14    2    4]
 [   3    2    3    0    1   10    1    7    1    1    2  869   22    9    1   18    8   18    4   22    9]
 [   1    1    3    5    0    2    3    6    2    0    2   77  809    1    2   14    4   40    8   10    5]
 [   4    1    3    3    4   12    1    4   14   19   13   18    4  857    5    5    7    4    5    8   10]
 [  10    0    2   20    6    2    0    0   25   10    7    1    3    6  972    0    2    4   15    2   11]
 [   2    2    5    0    5    1    8    1    0    4    0   18    7    2    0  976   11   13    2    6    3]
 [   3   11   11    0    5    8    0    1    2    1    4   10    5    1    2   18  963    1    2    5   19]
 [   4    4    0    5    0    0    0    3    0    2    0   18   25    2    4   17    2  907    3    2    7]
 [   6    4    9   15    2    1    2   14    7    1    5    4    4    3   21    1    1    0  946    3    9]
 [   1    4    2    0    1   11   21    9    1    1    3   25    6    6    0   12    3    5    7  962    8]
 [ 276  255  317  163  320  179  125  182  114  144  197  196  407  332  298  223  332  134  278  357 9103]]

2024-06-06 03:11:50,944 - ==> Best [Top1: 78.454   Top5: 96.278   Sparsity:0.00   Params: 169472 on epoch: 185]
2024-06-06 03:11:50,944 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 03:11:50,952 - 

2024-06-06 03:11:50,952 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 03:11:57,204 - Epoch: [189][  100/ 1218]    Overall Loss 0.489295    Objective Loss 0.489295                                        LR 0.000125    Time 0.062492    
2024-06-06 03:12:02,126 - Epoch: [189][  200/ 1218]    Overall Loss 0.493924    Objective Loss 0.493924                                        LR 0.000125    Time 0.055848    
2024-06-06 03:12:07,169 - Epoch: [189][  300/ 1218]    Overall Loss 0.495134    Objective Loss 0.495134                                        LR 0.000125    Time 0.054037    
2024-06-06 03:12:12,193 - Epoch: [189][  400/ 1218]    Overall Loss 0.495446    Objective Loss 0.495446                                        LR 0.000125    Time 0.053069    
2024-06-06 03:12:16,989 - Epoch: [189][  500/ 1218]    Overall Loss 0.493441    Objective Loss 0.493441                                        LR 0.000125    Time 0.052042    
2024-06-06 03:12:21,623 - Epoch: [189][  600/ 1218]    Overall Loss 0.490992    Objective Loss 0.490992                                        LR 0.000125    Time 0.051089    
2024-06-06 03:12:26,465 - Epoch: [189][  700/ 1218]    Overall Loss 0.489424    Objective Loss 0.489424                                        LR 0.000125    Time 0.050706    
2024-06-06 03:12:31,405 - Epoch: [189][  800/ 1218]    Overall Loss 0.489425    Objective Loss 0.489425                                        LR 0.000125    Time 0.050541    
2024-06-06 03:12:36,056 - Epoch: [189][  900/ 1218]    Overall Loss 0.491135    Objective Loss 0.491135                                        LR 0.000125    Time 0.050091    
2024-06-06 03:12:40,703 - Epoch: [189][ 1000/ 1218]    Overall Loss 0.493223    Objective Loss 0.493223                                        LR 0.000125    Time 0.049727    
2024-06-06 03:12:45,397 - Epoch: [189][ 1100/ 1218]    Overall Loss 0.492393    Objective Loss 0.492393                                        LR 0.000125    Time 0.049471    
2024-06-06 03:12:50,065 - Epoch: [189][ 1200/ 1218]    Overall Loss 0.492098    Objective Loss 0.492098                                        LR 0.000125    Time 0.049238    
2024-06-06 03:12:50,872 - Epoch: [189][ 1218/ 1218]    Overall Loss 0.492134    Objective Loss 0.492134    Top1 74.572127    Top5 95.843521    LR 0.000125    Time 0.049172    
2024-06-06 03:12:51,036 - --- validate (epoch=189)-----------
2024-06-06 03:12:51,036 - 34633 samples (256 per mini-batch)
2024-06-06 03:12:56,618 - Epoch: [189][  100/  136]    Loss 0.448412    Top1 78.359375    Top5 96.132812    
2024-06-06 03:12:58,303 - Epoch: [189][  136/  136]    Loss 0.454269    Top1 78.237519    Top5 96.148182    
2024-06-06 03:12:58,500 - ==> Top1: 78.238    Top5: 96.148    Loss: 0.454

2024-06-06 03:12:58,501 - ==> Confusion:
[[ 797    2    4    1   11    2    0    2   12   65    1    2    4    6    6    2    1    2    2    1    8]
 [   2  940    3    2   16   21    2   14    7    1    2    5    3    0    5    2    6    3   11    7   11]
 [   4    3  839   17    8    1   19   15    1    3    9    3    3    5    2   12    6    0    5    5   10]
 [   1    1   12  875    6    4    2    3    0    2   24    2    7    5   22    4    6    7   19    1   13]
 [  26   11    3    2  933   10    3    2    3   14    2    3    2    5    8    4    6    0    3    3   11]
 [   8   29    3    2   18  839    4   34    1    4    2   22    7   21    6    4    8    5    5   14    7]
 [   2    5   21    1    3    3 1005    7    1    2    1    7    0    2    0    5    4    1    4    8    4]
 [   1   17    8    2    2   29   11  912    0    1    4   15    7    1    1    2    1    1   31   22    9]
 [   5    6    1    1    1    3    0    1  829   60   19    4    5   15   31    0    2    2   11    0    6]
 [  69    1    3    2    7    3    3    3   54  806    0    2    1   22    8    3    0    4    2    1    7]
 [   0    4   11   13    0    5    3    6   17    2  948    3    5   14   10    1    3    1    9    3    6]
 [   1    3    0    1    2    7    5    6    1    0    1  881   38    7    1    9    5   18    1   18    6]
 [   2    2    1    6    0    4    1    6    2    0    2   65  835    3    1    6    8   27    5    3   16]
 [   4    1    2    0    5   11    0    6   11   18    9   17    4  884    4    3    1    3    1   10    7]
 [  12    6    2   24   15    1    0    0   35    7    3    0    3    5  954    2    2    1   18    0    8]
 [   1    0    2    1    3    0    4    1    0    2    0   24    8    1    0  980   13   12    2    6    6]
 [   3    7    7    0    8   10    1    2    5    2    1    8    3    3    2   12  969    1    2   11   15]
 [   1    0    2    2    1    1    1    0    1    2    0   21   37    3    0   14    2  904    3    5    5]
 [   2   13    9   13    3    4    3   26    7    1    7    2    6    1   23    0    3    2  925    2    6]
 [   0    3    5    0    1    8   16   14    0    0    0   27    8    3    1    8    8    3    6  970    7]
 [ 241  253  276  149  301  242  159  205  140  122  200  257  392  342  268  218  359  128  245  364 9071]]

2024-06-06 03:12:58,503 - ==> Best [Top1: 78.454   Top5: 96.278   Sparsity:0.00   Params: 169472 on epoch: 185]
2024-06-06 03:12:58,503 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 03:12:58,511 - 

2024-06-06 03:12:58,511 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 03:13:04,616 - Epoch: [190][  100/ 1218]    Overall Loss 0.490461    Objective Loss 0.490461                                        LR 0.000063    Time 0.061028    
2024-06-06 03:13:09,221 - Epoch: [190][  200/ 1218]    Overall Loss 0.492429    Objective Loss 0.492429                                        LR 0.000063    Time 0.053532    
2024-06-06 03:13:13,943 - Epoch: [190][  300/ 1218]    Overall Loss 0.491113    Objective Loss 0.491113                                        LR 0.000063    Time 0.051419    
2024-06-06 03:13:18,680 - Epoch: [190][  400/ 1218]    Overall Loss 0.488109    Objective Loss 0.488109                                        LR 0.000063    Time 0.050404    
2024-06-06 03:13:23,414 - Epoch: [190][  500/ 1218]    Overall Loss 0.487667    Objective Loss 0.487667                                        LR 0.000063    Time 0.049786    
2024-06-06 03:13:28,116 - Epoch: [190][  600/ 1218]    Overall Loss 0.489031    Objective Loss 0.489031                                        LR 0.000063    Time 0.049323    
2024-06-06 03:13:32,962 - Epoch: [190][  700/ 1218]    Overall Loss 0.488764    Objective Loss 0.488764                                        LR 0.000063    Time 0.049197    
2024-06-06 03:13:37,718 - Epoch: [190][  800/ 1218]    Overall Loss 0.488187    Objective Loss 0.488187                                        LR 0.000063    Time 0.048990    
2024-06-06 03:13:42,287 - Epoch: [190][  900/ 1218]    Overall Loss 0.488178    Objective Loss 0.488178                                        LR 0.000063    Time 0.048621    
2024-06-06 03:13:46,898 - Epoch: [190][ 1000/ 1218]    Overall Loss 0.488263    Objective Loss 0.488263                                        LR 0.000063    Time 0.048368    
2024-06-06 03:13:51,645 - Epoch: [190][ 1100/ 1218]    Overall Loss 0.488822    Objective Loss 0.488822                                        LR 0.000063    Time 0.048285    
2024-06-06 03:13:56,235 - Epoch: [190][ 1200/ 1218]    Overall Loss 0.488408    Objective Loss 0.488408                                        LR 0.000063    Time 0.048085    
2024-06-06 03:13:57,001 - Epoch: [190][ 1218/ 1218]    Overall Loss 0.488600    Objective Loss 0.488600    Top1 78.728606    Top5 96.577017    LR 0.000063    Time 0.048003    
2024-06-06 03:13:57,161 - --- validate (epoch=190)-----------
2024-06-06 03:13:57,161 - 34633 samples (256 per mini-batch)
2024-06-06 03:14:02,408 - Epoch: [190][  100/  136]    Loss 0.447496    Top1 78.355469    Top5 96.191406    
2024-06-06 03:14:04,017 - Epoch: [190][  136/  136]    Loss 0.449380    Top1 78.194208    Top5 96.116421    
2024-06-06 03:14:04,178 - ==> Top1: 78.194    Top5: 96.116    Loss: 0.449

2024-06-06 03:14:04,179 - ==> Confusion:
[[ 791    1    4    3   13    3    0    4   11   69    1    3    1    5    4    1    2    6    2    2    5]
 [   2  945    3    1   11   19    3   12    1    2    8    4    6    2    6    3    7    0   15    6    7]
 [   4    3  854   15    3    2   15   14    0    3    8    5    5    7    6    5    4    1    4    3    9]
 [   2    2   13  881    5    6    3    0    2    1   23    3    9    4   28    3    1    6   18    1    5]
 [  18   16    5    4  932    8    1    5    3   13    4    3    2    6    8    6    6    1    9    0    4]
 [   3   47    5    4   12  849    0   31    1    4    2   12    6   23    3    2    3    4    7   14   11]
 [   2    9   28    3    4    3  968    7    2    1    5    7    5    1    0    9    5    5    1   11   10]
 [   2   17   11    3    3   29    5  913    3    2    7    8    5    2    0    2    0    4   37   16    8]
 [  17    5    1    2    0    1    0    1  831   53   21    3    3   17   26    1    1    4   10    2    3]
 [  85    1    5    1    7    4    0    4   56  797    3    0    1   10   13    3    1    2    0    2    6]
 [   2    4    7   15    1    4    1    8   12    2  968    1    2   10   12    0    1    1    6    5    2]
 [   2    2    6    0    0   10    1    9    0    1    5  863   38   12    0   10    4   20    0   25    3]
 [   1    2    1    7    1    7    0    4    4    1    3   59  836    2    2    9    1   41    1    3   10]
 [   6    0    1    0    8   14    0    1   19   19   15   10    8  872    2    2    2    3    3    7    9]
 [  11    1    3   10   11    2    0    2   29    8    4    1    5    9  978    3    1    4   14    1    1]
 [   3    1    3    0    9    1    3    1    0    1    0   21   11    4    1  970   13   14    2    3    5]
 [   1   11    3    3    8    8    1    1    8    1    2   11    6    4    2   10  964    3    5    3   17]
 [   0    1    0    5    1    0    3    4    2    1    0   16   31    2    6    7    1  909    1    6    9]
 [   0    5   10    9    5    1    1   18   11    3    7    2    2    1   19    1    1    0  953    4    5]
 [   0    9    2    1    1   10   10   12    1    0    1   26    8    5    0    7    6    3    3  975    8]
 [ 238  314  274  156  252  214  107  214  142  129  266  216  448  343  298  175  343  147  287  337 9032]]

2024-06-06 03:14:04,180 - ==> Best [Top1: 78.454   Top5: 96.278   Sparsity:0.00   Params: 169472 on epoch: 185]
2024-06-06 03:14:04,180 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 03:14:04,187 - 

2024-06-06 03:14:04,188 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 03:14:09,911 - Epoch: [191][  100/ 1218]    Overall Loss 0.503857    Objective Loss 0.503857                                        LR 0.000063    Time 0.057216    
2024-06-06 03:14:14,355 - Epoch: [191][  200/ 1218]    Overall Loss 0.499099    Objective Loss 0.499099                                        LR 0.000063    Time 0.050824    
2024-06-06 03:14:18,789 - Epoch: [191][  300/ 1218]    Overall Loss 0.491326    Objective Loss 0.491326                                        LR 0.000063    Time 0.048658    
2024-06-06 03:14:23,238 - Epoch: [191][  400/ 1218]    Overall Loss 0.489147    Objective Loss 0.489147                                        LR 0.000063    Time 0.047612    
2024-06-06 03:14:27,665 - Epoch: [191][  500/ 1218]    Overall Loss 0.485774    Objective Loss 0.485774                                        LR 0.000063    Time 0.046941    
2024-06-06 03:14:32,077 - Epoch: [191][  600/ 1218]    Overall Loss 0.486906    Objective Loss 0.486906                                        LR 0.000063    Time 0.046470    
2024-06-06 03:14:36,503 - Epoch: [191][  700/ 1218]    Overall Loss 0.487338    Objective Loss 0.487338                                        LR 0.000063    Time 0.046152    
2024-06-06 03:14:41,080 - Epoch: [191][  800/ 1218]    Overall Loss 0.488721    Objective Loss 0.488721                                        LR 0.000063    Time 0.046103    
2024-06-06 03:14:45,735 - Epoch: [191][  900/ 1218]    Overall Loss 0.488030    Objective Loss 0.488030                                        LR 0.000063    Time 0.046150    
2024-06-06 03:14:50,376 - Epoch: [191][ 1000/ 1218]    Overall Loss 0.487404    Objective Loss 0.487404                                        LR 0.000063    Time 0.046175    
2024-06-06 03:14:54,767 - Epoch: [191][ 1100/ 1218]    Overall Loss 0.486723    Objective Loss 0.486723                                        LR 0.000063    Time 0.045968    
2024-06-06 03:14:59,177 - Epoch: [191][ 1200/ 1218]    Overall Loss 0.485955    Objective Loss 0.485955                                        LR 0.000063    Time 0.045810    
2024-06-06 03:14:59,999 - Epoch: [191][ 1218/ 1218]    Overall Loss 0.485628    Objective Loss 0.485628    Top1 75.305623    Top5 95.843521    LR 0.000063    Time 0.045808    
2024-06-06 03:15:00,156 - --- validate (epoch=191)-----------
2024-06-06 03:15:00,157 - 34633 samples (256 per mini-batch)
2024-06-06 03:15:05,406 - Epoch: [191][  100/  136]    Loss 0.452993    Top1 78.589844    Top5 96.269531    
2024-06-06 03:15:07,025 - Epoch: [191][  136/  136]    Loss 0.450805    Top1 78.731268    Top5 96.286779    
2024-06-06 03:15:07,210 - ==> Top1: 78.731    Top5: 96.287    Loss: 0.451

2024-06-06 03:15:07,211 - ==> Confusion:
[[ 808    0    5    2   16    0    1    0    4   59    0    2    4    4    8    1    2    4    3    1    7]
 [   0  948    3    1   20   23    0    9    1    1    3    3    3    2    9    1    9    2   17    1    7]
 [   5    1  857    7    5    3   18   11    4    5    6    2    2    4   10    7    5    0    8    0   10]
 [   2    4   17  875    2    5    3    3    4    4   16    1    6    1   29    3    4    5   17    1   14]
 [  14   11    3    1  950   10    1    1    1    9    1    7    2    3   16    4    7    1    5    0    7]
 [   7   29    4    1   19  856    5   35    1    3    2   17    5   17    4    5    5    1    4   12   11]
 [   0    6   27    1    4    3  991    6    2    0    3    5    4    0    0    9    3    3    1   11    7]
 [   4   21   13    2    2   36    3  902    4    2    4   12    0    5    0    0    2    5   35   15   10]
 [  18    2    1    1    1    0    0    2  832   51   18    2    4   16   24    0    3    1    9    5   12]
 [  74    2    4    0   10    3    0    1   52  803    3    2    2   18   12    1    2    3    0    2    7]
 [   1    6   14   10    2    3    3    4   16    1  951    2    1   10   12    0    3    0   15    2    8]
 [   2    4    0    0    1   18    2    8    0    1    1  867   31   12    1   18    3   15    3   17    7]
 [   1    3    2    6    1    3    0    6    6    0    2   80  801    2    5    9    6   31    8    9   14]
 [   3    1    1    1    5    8    1    4   20   17   10    9    4  876    8    4    4    4    4    5   12]
 [  18    3    3   15    7    4    0    2   42    5    5    2    4    5  951    1    1    4   13    2   11]
 [   1    2    5    0    3    1    8    2    0    3    0   15   11    4    0  962   14   11    5    5   14]
 [   2    6    6    4   13    9    2    3    7    1    3    7    5    3    4    9  966    0    4    8   10]
 [   2    1    0    1    1    1    0    1    1    1    0   22   31    4    4   14    0  904    2    5   10]
 [   5    5    6   17    2    2    1   20    7    0    7    0    5    0   16    2    2    2  942    3   14]
 [   1    8    6    2    2   11   14   10    0    0    0   27   14    6    2    7    9    3    4  949   13]
 [ 258  260  267  141  298  210  104  237  138  116  169  210  389  323  272  187  348  136  235  358 9276]]

2024-06-06 03:15:07,212 - ==> Best [Top1: 78.731   Top5: 96.287   Sparsity:0.00   Params: 169472 on epoch: 191]
2024-06-06 03:15:07,213 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 03:15:07,227 - 

2024-06-06 03:15:07,227 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 03:15:13,126 - Epoch: [192][  100/ 1218]    Overall Loss 0.497093    Objective Loss 0.497093                                        LR 0.000063    Time 0.058969    
2024-06-06 03:15:17,587 - Epoch: [192][  200/ 1218]    Overall Loss 0.489632    Objective Loss 0.489632                                        LR 0.000063    Time 0.051785    
2024-06-06 03:15:22,024 - Epoch: [192][  300/ 1218]    Overall Loss 0.485760    Objective Loss 0.485760                                        LR 0.000063    Time 0.049307    
2024-06-06 03:15:26,486 - Epoch: [192][  400/ 1218]    Overall Loss 0.486683    Objective Loss 0.486683                                        LR 0.000063    Time 0.048133    
2024-06-06 03:15:30,916 - Epoch: [192][  500/ 1218]    Overall Loss 0.485822    Objective Loss 0.485822                                        LR 0.000063    Time 0.047364    
2024-06-06 03:15:35,366 - Epoch: [192][  600/ 1218]    Overall Loss 0.484855    Objective Loss 0.484855                                        LR 0.000063    Time 0.046884    
2024-06-06 03:15:39,806 - Epoch: [192][  700/ 1218]    Overall Loss 0.484103    Objective Loss 0.484103                                        LR 0.000063    Time 0.046524    
2024-06-06 03:15:44,361 - Epoch: [192][  800/ 1218]    Overall Loss 0.484999    Objective Loss 0.484999                                        LR 0.000063    Time 0.046400    
2024-06-06 03:15:49,027 - Epoch: [192][  900/ 1218]    Overall Loss 0.485390    Objective Loss 0.485390                                        LR 0.000063    Time 0.046428    
2024-06-06 03:15:53,706 - Epoch: [192][ 1000/ 1218]    Overall Loss 0.484791    Objective Loss 0.484791                                        LR 0.000063    Time 0.046463    
2024-06-06 03:15:58,391 - Epoch: [192][ 1100/ 1218]    Overall Loss 0.483547    Objective Loss 0.483547                                        LR 0.000063    Time 0.046497    
2024-06-06 03:16:03,075 - Epoch: [192][ 1200/ 1218]    Overall Loss 0.484235    Objective Loss 0.484235                                        LR 0.000063    Time 0.046524    
2024-06-06 03:16:03,854 - Epoch: [192][ 1218/ 1218]    Overall Loss 0.483949    Objective Loss 0.483949    Top1 76.039120    Top5 97.310513    LR 0.000063    Time 0.046476    
2024-06-06 03:16:04,041 - --- validate (epoch=192)-----------
2024-06-06 03:16:04,041 - 34633 samples (256 per mini-batch)
2024-06-06 03:16:09,272 - Epoch: [192][  100/  136]    Loss 0.449812    Top1 78.218750    Top5 96.187500    
2024-06-06 03:16:10,870 - Epoch: [192][  136/  136]    Loss 0.452222    Top1 78.026738    Top5 96.162619    
2024-06-06 03:16:11,027 - ==> Top1: 78.027    Top5: 96.163    Loss: 0.452

2024-06-06 03:16:11,028 - ==> Confusion:
[[ 802    2    2    0   13    2    0    1    6   62    0    4    2    7   12    2    2    1    3    1    7]
 [   3  938    2    3   25   20    3   12    8    1    2    1    3    0    7    3   12    1   12    3    4]
 [  10    3  844    8    4    0   28   11    1    4    9    2    2    3    3    7    6    1    3   11   10]
 [   1    3   22  869    5    8    3    1    4    1   22    2    7    3   30    2    2    9   13    4    5]
 [  25    9    4    1  948    6    1    4    2   11    2    2    4    3   12    1    8    0    2    2    7]
 [   7   43    2    4   16  841    7   30    3    7    3   14    7   21    5    0    6    4    3    9   11]
 [   1    9   21    1    3    4  995    4    2    1    1    6    1    1    1   10    4    2    2   13    4]
 [   3   15   13    0    1   40    6  913    3    3    7   11    5    3    4    2    2    2   27   15    2]
 [  11    1    1    1    4    1    0    1  829   49   26    5    7   19   26    1    0    4   11    0    5]
 [  76    2    4    1    4    3    1    2   49  809    1    0    1   26    7    1    0    3    3    1    7]
 [   0    1    6   12    1    2    2    7   14    1  968    3    3   11   11    0    4    0   12    0    6]
 [   4    6    1    1    0    9    4    9    2    0    1  884   14   12    3   10    4   18    2   21    6]
 [   0    4    2    3    0    5    2    8    1    0    5   81  813    5    6    9    2   27    4    8   10]
 [   3    1    3    1    5   11    1    2   17   15    8   12    4  887    3    2    3    1    5    6   11]
 [   7    4    1   15    7    1    0    2   28    8    4    2    2    7  979    1    7    1   17    0    5]
 [   2    4    6    0    1    0    7    1    1    0    0   15   12    0    0  973   10   13    2    7   12]
 [   3    9   11    4    9    9    0    1    9    3    2    9    5    5    2   10  959    1    2    6   13]
 [   1    2    1    4    0    3    2    1    1    1    0   23   32    1    5   13    4  900    0    1   10]
 [   4    8    9   13    4    4    3   22    6    3    5    4    4    3   16    0    2    1  937    4    6]
 [   2    4    0    1    3   10   16   10    2    0    1   29   11    4    2   14    6    2    5  955   11]
 [ 256  282  313  140  344  220  139  203  132  144  213  220  427  351  295  213  348  155  216  341 8980]]

2024-06-06 03:16:11,030 - ==> Best [Top1: 78.731   Top5: 96.287   Sparsity:0.00   Params: 169472 on epoch: 191]
2024-06-06 03:16:11,030 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 03:16:11,037 - 

2024-06-06 03:16:11,037 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 03:16:16,811 - Epoch: [193][  100/ 1218]    Overall Loss 0.503451    Objective Loss 0.503451                                        LR 0.000063    Time 0.057719    
2024-06-06 03:16:21,252 - Epoch: [193][  200/ 1218]    Overall Loss 0.492671    Objective Loss 0.492671                                        LR 0.000063    Time 0.051058    
2024-06-06 03:16:25,705 - Epoch: [193][  300/ 1218]    Overall Loss 0.493454    Objective Loss 0.493454                                        LR 0.000063    Time 0.048877    
2024-06-06 03:16:30,140 - Epoch: [193][  400/ 1218]    Overall Loss 0.491614    Objective Loss 0.491614                                        LR 0.000063    Time 0.047742    
2024-06-06 03:16:34,600 - Epoch: [193][  500/ 1218]    Overall Loss 0.489521    Objective Loss 0.489521                                        LR 0.000063    Time 0.047111    
2024-06-06 03:16:39,058 - Epoch: [193][  600/ 1218]    Overall Loss 0.485946    Objective Loss 0.485946                                        LR 0.000063    Time 0.046689    
2024-06-06 03:16:43,508 - Epoch: [193][  700/ 1218]    Overall Loss 0.485407    Objective Loss 0.485407                                        LR 0.000063    Time 0.046374    
2024-06-06 03:16:47,986 - Epoch: [193][  800/ 1218]    Overall Loss 0.485137    Objective Loss 0.485137                                        LR 0.000063    Time 0.046172    
2024-06-06 03:16:52,422 - Epoch: [193][  900/ 1218]    Overall Loss 0.483267    Objective Loss 0.483267                                        LR 0.000063    Time 0.045969    
2024-06-06 03:16:56,861 - Epoch: [193][ 1000/ 1218]    Overall Loss 0.483034    Objective Loss 0.483034                                        LR 0.000063    Time 0.045810    
2024-06-06 03:17:01,333 - Epoch: [193][ 1100/ 1218]    Overall Loss 0.482340    Objective Loss 0.482340                                        LR 0.000063    Time 0.045710    
2024-06-06 03:17:05,784 - Epoch: [193][ 1200/ 1218]    Overall Loss 0.482101    Objective Loss 0.482101                                        LR 0.000063    Time 0.045608    
2024-06-06 03:17:06,536 - Epoch: [193][ 1218/ 1218]    Overall Loss 0.481941    Objective Loss 0.481941    Top1 78.728606    Top5 95.843521    LR 0.000063    Time 0.045551    
2024-06-06 03:17:06,713 - --- validate (epoch=193)-----------
2024-06-06 03:17:06,713 - 34633 samples (256 per mini-batch)
2024-06-06 03:17:11,975 - Epoch: [193][  100/  136]    Loss 0.447719    Top1 78.308594    Top5 96.355469    
2024-06-06 03:17:13,595 - Epoch: [193][  136/  136]    Loss 0.455217    Top1 78.093148    Top5 96.315653    
2024-06-06 03:17:13,776 - ==> Top1: 78.093    Top5: 96.316    Loss: 0.455

2024-06-06 03:17:13,777 - ==> Confusion:
[[ 804    0    1    0    8    3    0    2    7   69    0    3    3    4    5    1    1    1    5    1   13]
 [   3  937    2    2   28   21    2   15    3    3    5    1    7    1    5    3    5    1    7    2   10]
 [   8    0  856   11    4    1   24   10    2    5    2    3    5    6    4    7    5    0    4    5    8]
 [   6    0   21  880    3    5    4    1    1    2   14    2   12    3   37    2    2    6    9    0    6]
 [  29   16    5    0  921   12    2    4    3   13    0    4    2    7    8    5    9    1    3    2    8]
 [   4   25    5    2   15  851    1   43    4    6    3   23    5   19    4    3    8    3    7    4    8]
 [   2    6   29    0    4    3  979    8    0    0    7    6    4    1    1    6    5    4    1   12    8]
 [   3   16   11    2    4   38    3  918    2    1    3   17    6    4    3    0    1    1   28    8    8]
 [  16    5    1    3    1    2    0    1  828   60   14    3    8   14   20    1    2    4   10    2    7]
 [  82    1    2    0    6    3    0    2   46  817    2    1    1   14    9    3    2    1    2    0    7]
 [   1    7    8   13    1    4    5    5   24    5  934    0    2   18   16    0    1    1   13    2    4]
 [   2    2    5    0    1   10    1    4    0    2    0  892   27    9    1   13    2   17    1   15    7]
 [   0    0    4    3    1    0    1    4    2    1    2   80  825    3    3    8    4   38    9    4    3]
 [  10    0    4    0    5   16    2    2   16   23   13    8    3  875    8    2    2    4    1    2    5]
 [  19    3    3    7    8    0    0    1   30    6    3    0    4    6  985    0    3    1   15    0    4]
 [   3    1    8    3    3    0    7    0    0    0    0   15    8    3    0  982   17   10    1    3    2]
 [  10    5    8    2    8    9    2    2    4    3    2   11    2    3    0    7  955    3    4   10   22]
 [   3    0    0    3    2    1    1    1    2    2    0   21   31    2    3   11    2  905    1    3   11]
 [   1    7   10   13    3    4    0   26   10    2    5    2    7    1   22    1    0    0  930    4   10]
 [   2    4    7    2    2   12   13   11    1    0    0   23    9    3    1   13    9    6    9  948   13]
 [ 312  239  310  137  318  209  101  229  158  157  205  262  392  364  315  207  300  146  216  331 9024]]

2024-06-06 03:17:13,779 - ==> Best [Top1: 78.731   Top5: 96.287   Sparsity:0.00   Params: 169472 on epoch: 191]
2024-06-06 03:17:13,779 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 03:17:13,786 - 

2024-06-06 03:17:13,786 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 03:17:19,510 - Epoch: [194][  100/ 1218]    Overall Loss 0.485969    Objective Loss 0.485969                                        LR 0.000063    Time 0.057223    
2024-06-06 03:17:23,903 - Epoch: [194][  200/ 1218]    Overall Loss 0.488869    Objective Loss 0.488869                                        LR 0.000063    Time 0.050568    
2024-06-06 03:17:28,318 - Epoch: [194][  300/ 1218]    Overall Loss 0.489988    Objective Loss 0.489988                                        LR 0.000063    Time 0.048421    
2024-06-06 03:17:32,721 - Epoch: [194][  400/ 1218]    Overall Loss 0.489523    Objective Loss 0.489523                                        LR 0.000063    Time 0.047321    
2024-06-06 03:17:37,120 - Epoch: [194][  500/ 1218]    Overall Loss 0.485370    Objective Loss 0.485370                                        LR 0.000063    Time 0.046651    
2024-06-06 03:17:41,533 - Epoch: [194][  600/ 1218]    Overall Loss 0.485927    Objective Loss 0.485927                                        LR 0.000063    Time 0.046229    
2024-06-06 03:17:45,938 - Epoch: [194][  700/ 1218]    Overall Loss 0.485324    Objective Loss 0.485324                                        LR 0.000063    Time 0.045915    
2024-06-06 03:17:50,349 - Epoch: [194][  800/ 1218]    Overall Loss 0.484402    Objective Loss 0.484402                                        LR 0.000063    Time 0.045688    
2024-06-06 03:17:54,783 - Epoch: [194][  900/ 1218]    Overall Loss 0.482865    Objective Loss 0.482865                                        LR 0.000063    Time 0.045537    
2024-06-06 03:17:59,204 - Epoch: [194][ 1000/ 1218]    Overall Loss 0.483087    Objective Loss 0.483087                                        LR 0.000063    Time 0.045402    
2024-06-06 03:18:03,639 - Epoch: [194][ 1100/ 1218]    Overall Loss 0.484177    Objective Loss 0.484177                                        LR 0.000063    Time 0.045305    
2024-06-06 03:18:08,070 - Epoch: [194][ 1200/ 1218]    Overall Loss 0.484304    Objective Loss 0.484304                                        LR 0.000063    Time 0.045222    
2024-06-06 03:18:08,833 - Epoch: [194][ 1218/ 1218]    Overall Loss 0.484470    Objective Loss 0.484470    Top1 79.706601    Top5 96.332518    LR 0.000063    Time 0.045180    
2024-06-06 03:18:08,998 - --- validate (epoch=194)-----------
2024-06-06 03:18:08,998 - 34633 samples (256 per mini-batch)
2024-06-06 03:18:14,228 - Epoch: [194][  100/  136]    Loss 0.446270    Top1 78.457031    Top5 96.304688    
2024-06-06 03:18:15,844 - Epoch: [194][  136/  136]    Loss 0.446022    Top1 78.477175    Top5 96.281004    
2024-06-06 03:18:15,999 - ==> Top1: 78.477    Top5: 96.281    Loss: 0.446

2024-06-06 03:18:16,000 - ==> Confusion:
[[ 797    0    6    0   11    1    2    2    6   73    1    2    2    3    6    3    2    2    4    1    7]
 [   1  946    4    2   18   15    4   16    5    2    1    2    7    0    8    2    6    1   14    4    5]
 [   5    4  855    5    5    3   22   12    1    5    8    4    3    2    4    6    3    3    7    5    8]
 [   1    2   11  881    3    4    3    2    2    2   22    2    9    4   34    2    3    7   17    2    3]
 [  28   11    1    0  941   12    0    2    2    9    2    3    3    4   15    3    6    3    6    0    3]
 [   4   33    2    8   15  826    1   44    4    5    4   12    8   18    6    5   11    8    5   10   14]
 [   3    7   28    4    2    5  982    8    0    1    2    6    2    3    0    8    1    1    2   10   11]
 [   4   17   12    3    6   35    4  908    1    3    5    8    5    4    5    1    1    7   30   11    7]
 [   9    4    0    3    3    1    0    0  831   53   19    3    4   16   34    1    2    4    6    1    8]
 [  71    0    4    1    7    1    0    1   29  835    1    1    1   19   11    2    0    3    2    2   10]
 [   0    4   14    9    1    5    5    7   17    3  947    1    1    9   14    0    1    2   16    2    6]
 [   1    4    1    0    1   10    4    7    0    1    0  865   35   10    1   13    6   21    4   18    9]
 [   1    0    2    7    0    1    4    3    1    0    3   62  837    4    2    6    3   37    6    8    8]
 [   5    0    2    0    5   11    0    3   18   18   11   10    2  878   10    0    3    7    0    9    9]
 [   8    3    2    8   11    3    0    2   24    8    4    1    2    8  978    1    4    3   17    2    9]
 [   2    2    3    0    5    0    8    1    1    1    0   14    9    1    1  989   10   10    1    5    3]
 [   3   11    6    4    9    3    1    1    8    2    2    6    4    6    4   11  956    4    2   10   19]
 [   1    4    1    1    2    1    0    0    0    0    0   20   37    1    5   13    3  900    4    3    9]
 [   2   12    6   11    7    1    2   20    3    4    4    0    5    1   25    2    0    2  942    2    7]
 [   1    6    0    0    3    7   11   13    0    0    0   28    9    3    1    6    7    7    3  970   13]
 [ 263  262  287  133  291  187  100  234  148  153  213  203  417  346  316  179  333  144  250  358 9115]]

2024-06-06 03:18:16,002 - ==> Best [Top1: 78.731   Top5: 96.287   Sparsity:0.00   Params: 169472 on epoch: 191]
2024-06-06 03:18:16,002 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 03:18:16,010 - 

2024-06-06 03:18:16,010 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 03:18:21,781 - Epoch: [195][  100/ 1218]    Overall Loss 0.475901    Objective Loss 0.475901                                        LR 0.000031    Time 0.057697    
2024-06-06 03:18:26,447 - Epoch: [195][  200/ 1218]    Overall Loss 0.477652    Objective Loss 0.477652                                        LR 0.000031    Time 0.052170    
2024-06-06 03:18:31,099 - Epoch: [195][  300/ 1218]    Overall Loss 0.481868    Objective Loss 0.481868                                        LR 0.000031    Time 0.050284    
2024-06-06 03:18:35,538 - Epoch: [195][  400/ 1218]    Overall Loss 0.484053    Objective Loss 0.484053                                        LR 0.000031    Time 0.048806    
2024-06-06 03:18:39,974 - Epoch: [195][  500/ 1218]    Overall Loss 0.485343    Objective Loss 0.485343                                        LR 0.000031    Time 0.047913    
2024-06-06 03:18:44,387 - Epoch: [195][  600/ 1218]    Overall Loss 0.486164    Objective Loss 0.486164                                        LR 0.000031    Time 0.047281    
2024-06-06 03:18:48,814 - Epoch: [195][  700/ 1218]    Overall Loss 0.484194    Objective Loss 0.484194                                        LR 0.000031    Time 0.046849    
2024-06-06 03:18:53,238 - Epoch: [195][  800/ 1218]    Overall Loss 0.483609    Objective Loss 0.483609                                        LR 0.000031    Time 0.046521    
2024-06-06 03:18:57,658 - Epoch: [195][  900/ 1218]    Overall Loss 0.482954    Objective Loss 0.482954                                        LR 0.000031    Time 0.046262    
2024-06-06 03:19:02,063 - Epoch: [195][ 1000/ 1218]    Overall Loss 0.483635    Objective Loss 0.483635                                        LR 0.000031    Time 0.046040    
2024-06-06 03:19:06,485 - Epoch: [195][ 1100/ 1218]    Overall Loss 0.484210    Objective Loss 0.484210                                        LR 0.000031    Time 0.045873    
2024-06-06 03:19:10,922 - Epoch: [195][ 1200/ 1218]    Overall Loss 0.483700    Objective Loss 0.483700                                        LR 0.000031    Time 0.045746    
2024-06-06 03:19:11,667 - Epoch: [195][ 1218/ 1218]    Overall Loss 0.483584    Objective Loss 0.483584    Top1 78.239609    Top5 97.555012    LR 0.000031    Time 0.045682    
2024-06-06 03:19:11,852 - --- validate (epoch=195)-----------
2024-06-06 03:19:11,852 - 34633 samples (256 per mini-batch)
2024-06-06 03:19:17,089 - Epoch: [195][  100/  136]    Loss 0.449932    Top1 78.074219    Top5 96.246094    
2024-06-06 03:19:18,696 - Epoch: [195][  136/  136]    Loss 0.447559    Top1 78.142234    Top5 96.281004    
2024-06-06 03:19:18,862 - ==> Top1: 78.142    Top5: 96.281    Loss: 0.448

2024-06-06 03:19:18,863 - ==> Confusion:
[[ 800    0    2    1   11    4    2    2   15   68    0    2    3    3    4    3    2    2    2    0    5]
 [   0  931    2    2   24   17    2   23    3    1    1    3    3    3    6    1    7    4   21    1    8]
 [   8    1  846    5    6    3   16   11    1    8   10    6    6    5    2    8    9    0    6    5    8]
 [   3    3   23  875    1    3    4    2    3    4   12    2    5    6   31    3    2    7   18    3    6]
 [  21    8    0    1  948    9    1    2    2   13    2    3    1    6   11    3    7    4    6    0    6]
 [   8   31    2    4   18  850    5   34    3    5    1   18    8   18    2    2    5    4    4   13    8]
 [   1    4   28    2    5    5  990    6    3    0    1    0    0    2    0   11    4    2    6   11    5]
 [   5   17   13    2    2   40    6  898    3    0    4    9   10    1    1    3    0    5   35   14    9]
 [  10    3    2    2    3    2    0    0  844   49   15    1    5   19   19    0    2    2   13    2    9]
 [  78    2    0    0    5    0    0    2   56  816    1    1    2   15   10    0    2    4    1    1    5]
 [   0    2    9   16    2    3    3    4   14    1  955    1    3   10   12    0    0    3   15    4    7]
 [   5    0    1    1    0    9    4    7    1    0    0  881   33   11    0   13    5   16    2   15    7]
 [   2    0    3    7    2    5    2    7    0    0    2   67  824    7    5    3    6   25    9   10    9]
 [   1    0    2    0    5   10    0    3   17   22    8   10    6  880    7    5    3    4    0    4   14]
 [  13    1    0    9   10    1    1    2   36    7    3    0    5    5  967    1    0    5   17    0   15]
 [   3    0    3    1    4    0    4    0    0    1    0   18   12    2    0  977   15   14    1    2    9]
 [   1    7   11    2    8   10    1    1    7    2    3    7    3    6    5   13  959    7    1    8   10]
 [   1    1    0    3    2    1    2    1    1    1    1   22   39    4    4    7    3  898    2    3    9]
 [   0    6    2   10    2    4    0   20   10    1    8    2    2    1   18    2    1    2  951    5   11]
 [   1    4    8    0    2    7   17   14    1    2    0   19    9    5    0    7    8    2    7  965   10]
 [ 259  261  281  150  279  218  125  226  172  152  199  202  428  347  289  176  362  136  282  380 9008]]

2024-06-06 03:19:18,865 - ==> Best [Top1: 78.731   Top5: 96.287   Sparsity:0.00   Params: 169472 on epoch: 191]
2024-06-06 03:19:18,865 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 03:19:18,872 - 

2024-06-06 03:19:18,872 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 03:19:24,622 - Epoch: [196][  100/ 1218]    Overall Loss 0.483778    Objective Loss 0.483778                                        LR 0.000031    Time 0.057483    
2024-06-06 03:19:29,038 - Epoch: [196][  200/ 1218]    Overall Loss 0.484072    Objective Loss 0.484072                                        LR 0.000031    Time 0.050812    
2024-06-06 03:19:33,422 - Epoch: [196][  300/ 1218]    Overall Loss 0.482546    Objective Loss 0.482546                                        LR 0.000031    Time 0.048483    
2024-06-06 03:19:37,840 - Epoch: [196][  400/ 1218]    Overall Loss 0.483236    Objective Loss 0.483236                                        LR 0.000031    Time 0.047405    
2024-06-06 03:19:42,256 - Epoch: [196][  500/ 1218]    Overall Loss 0.481632    Objective Loss 0.481632                                        LR 0.000031    Time 0.046754    
2024-06-06 03:19:46,682 - Epoch: [196][  600/ 1218]    Overall Loss 0.481654    Objective Loss 0.481654                                        LR 0.000031    Time 0.046336    
2024-06-06 03:19:51,110 - Epoch: [196][  700/ 1218]    Overall Loss 0.481845    Objective Loss 0.481845                                        LR 0.000031    Time 0.046040    
2024-06-06 03:19:55,527 - Epoch: [196][  800/ 1218]    Overall Loss 0.480225    Objective Loss 0.480225                                        LR 0.000031    Time 0.045806    
2024-06-06 03:19:59,945 - Epoch: [196][  900/ 1218]    Overall Loss 0.479744    Objective Loss 0.479744                                        LR 0.000031    Time 0.045623    
2024-06-06 03:20:04,373 - Epoch: [196][ 1000/ 1218]    Overall Loss 0.480258    Objective Loss 0.480258                                        LR 0.000031    Time 0.045487    
2024-06-06 03:20:08,797 - Epoch: [196][ 1100/ 1218]    Overall Loss 0.479629    Objective Loss 0.479629                                        LR 0.000031    Time 0.045373    
2024-06-06 03:20:13,218 - Epoch: [196][ 1200/ 1218]    Overall Loss 0.480295    Objective Loss 0.480295                                        LR 0.000031    Time 0.045275    
2024-06-06 03:20:13,968 - Epoch: [196][ 1218/ 1218]    Overall Loss 0.480336    Objective Loss 0.480336    Top1 77.506112    Top5 97.310513    LR 0.000031    Time 0.045221    
2024-06-06 03:20:14,142 - --- validate (epoch=196)-----------
2024-06-06 03:20:14,142 - 34633 samples (256 per mini-batch)
2024-06-06 03:20:19,567 - Epoch: [196][  100/  136]    Loss 0.447236    Top1 78.726562    Top5 96.281250    
2024-06-06 03:20:21,195 - Epoch: [196][  136/  136]    Loss 0.445237    Top1 78.497387    Top5 96.266567    
2024-06-06 03:20:21,379 - ==> Top1: 78.497    Top5: 96.267    Loss: 0.445

2024-06-06 03:20:21,380 - ==> Confusion:
[[ 803    1    7    1   10    2    4    1    3   63    1    3    1    3    7    4    1    4    4    3    5]
 [   5  928    5    3   15   24    3   18    7    2    2    4    2    0    6    3    5    2   15    6    8]
 [   8    7  845    8    5    3   26   13    0    5    5    4    9    2    2    8    4    1    3    5    7]
 [   3    1   17  897    3    6    3    2    0    2   14    2    7    6   23    0    1    7   10    2   10]
 [  28    6    2    2  959    9    3    2    1    9    1    2    2    3    5    2    9    1    4    0    4]
 [   4   28    2    5   19  844    5   30    1    7    2   21   10   21    5    3    6    1    5   14   10]
 [   3    4   26    2    2    4  990    5    1    2    2    5    2    0    1    7    7    5    4    6    8]
 [   4   11   17    1    4   31    8  916    2    2    3   10    2    1    1    2    3    3   33   13   10]
 [  14    2    0    2    0    0    0    1  827   53   17    2    5   24   31    0    3    4   11    0    6]
 [  83    1    6    3    9    4    0    3   40  815    1    2    2   17    7    1    0    3    1    0    3]
 [   4    2   10   13    3    6    3    5   11    0  945    0    3   11   20    0    2    1   12    5    8]
 [   1    4    5    0    3    8    1    2    0    3    1  894   22    6    1   11    5   17    3   13   11]
 [   0    0    2    8    0    4    0    4    0    0    4   76  831    5    3   14    5   21    6    6    6]
 [   2    1    2    0    3    9    0    2   15   18   10   11    4  882    6    5    6    5    2   10    8]
 [  15    2    3   11    9    1    0    3   23    7    4    4    3    7  973    1    3    4   16    1    8]
 [   2    2    5    1    4    1    7    0    0    2    0   17   11    2    2  976   10   14    2    1    7]
 [   3   12    6    2    9    2    2    1    6    1    1    6    3    5    1   13  970    2    3    5   19]
 [   0    1    1    1    0    0    4    0    2    6    0   21   32    2    5   17    2  901    2    1    7]
 [   2    7    6   15    4    6    3   28    8    1    7    3    8    1   26    2    1    1  920    2    7]
 [   4    4    3    2    2   13   10   14    1    0    0   24    9    4    1    9    7    5    4  957   15]
 [ 246  240  297  145  312  216  125  205  142  137  195  234  429  365  292  204  335  150  236  314 9113]]

2024-06-06 03:20:21,382 - ==> Best [Top1: 78.731   Top5: 96.287   Sparsity:0.00   Params: 169472 on epoch: 191]
2024-06-06 03:20:21,382 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 03:20:21,389 - 

2024-06-06 03:20:21,389 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 03:20:27,133 - Epoch: [197][  100/ 1218]    Overall Loss 0.477663    Objective Loss 0.477663                                        LR 0.000031    Time 0.057424    
2024-06-06 03:20:31,585 - Epoch: [197][  200/ 1218]    Overall Loss 0.477492    Objective Loss 0.477492                                        LR 0.000031    Time 0.050965    
2024-06-06 03:20:36,040 - Epoch: [197][  300/ 1218]    Overall Loss 0.476576    Objective Loss 0.476576                                        LR 0.000031    Time 0.048822    
2024-06-06 03:20:40,487 - Epoch: [197][  400/ 1218]    Overall Loss 0.476838    Objective Loss 0.476838                                        LR 0.000031    Time 0.047732    
2024-06-06 03:20:44,911 - Epoch: [197][  500/ 1218]    Overall Loss 0.474145    Objective Loss 0.474145                                        LR 0.000031    Time 0.047030    
2024-06-06 03:20:49,347 - Epoch: [197][  600/ 1218]    Overall Loss 0.475366    Objective Loss 0.475366                                        LR 0.000031    Time 0.046583    
2024-06-06 03:20:53,780 - Epoch: [197][  700/ 1218]    Overall Loss 0.474655    Objective Loss 0.474655                                        LR 0.000031    Time 0.046260    
2024-06-06 03:20:58,205 - Epoch: [197][  800/ 1218]    Overall Loss 0.475413    Objective Loss 0.475413                                        LR 0.000031    Time 0.046007    
2024-06-06 03:21:02,655 - Epoch: [197][  900/ 1218]    Overall Loss 0.477835    Objective Loss 0.477835                                        LR 0.000031    Time 0.045837    
2024-06-06 03:21:07,074 - Epoch: [197][ 1000/ 1218]    Overall Loss 0.478088    Objective Loss 0.478088                                        LR 0.000031    Time 0.045671    
2024-06-06 03:21:11,514 - Epoch: [197][ 1100/ 1218]    Overall Loss 0.478553    Objective Loss 0.478553                                        LR 0.000031    Time 0.045555    
2024-06-06 03:21:15,946 - Epoch: [197][ 1200/ 1218]    Overall Loss 0.479339    Objective Loss 0.479339                                        LR 0.000031    Time 0.045451    
2024-06-06 03:21:16,781 - Epoch: [197][ 1218/ 1218]    Overall Loss 0.479482    Objective Loss 0.479482    Top1 77.261614    Top5 95.599022    LR 0.000031    Time 0.045465    
2024-06-06 03:21:16,967 - --- validate (epoch=197)-----------
2024-06-06 03:21:16,967 - 34633 samples (256 per mini-batch)
2024-06-06 03:21:22,211 - Epoch: [197][  100/  136]    Loss 0.445946    Top1 78.425781    Top5 96.375000    
2024-06-06 03:21:23,846 - Epoch: [197][  136/  136]    Loss 0.441681    Top1 78.491612    Top5 96.410938    
2024-06-06 03:21:24,031 - ==> Top1: 78.492    Top5: 96.411    Loss: 0.442

2024-06-06 03:21:24,032 - ==> Confusion:
[[ 788    0    1    1    8    2    1    2    7   80    1    3    3    4    3    4    2    2    4    2   13]
 [   3  940    1    1   24   21    4   12    2    2    2    3    4    2    4    0    8    1   18    2    9]
 [   7    4  839    9    3    4   25   11    1    2   12    6    3    5    3    5    6    0    5    6   14]
 [   7    3   23  865    4    8    3    2    5    1   19    1    6    2   33    2    5    6   13    1    7]
 [  19   10    3    3  942    7    3    4    2   15    2    1    0    1   11    4   13    2    5    1    6]
 [   3   34    3    3   14  854    1   34    1    5    2   17    7   21    4    2    9    4    6   11    8]
 [   1    3   24    0    3    7  993    8    4    0    5    5    1    0    0    9    6    1    3    5    8]
 [   7   19   14    6    6   29    6  899    1    2    4   14    2    2    4    2    0    3   30   13   14]
 [   8    5    1    0    1    3    0    2  846   52   14    3    2   17   29    1    4    2    8    1    3]
 [  77    4    2    1    1    4    0    3   61  803    0    1    2   24    6    0    1    2    3    0    6]
 [   2    7    5   11    1    2    5    5   14    3  968    1    1    9   12    0    1    0    9    2    6]
 [   3    1    3    0    1    6    1    3    1    1    1  892   26   16    1   10   10   11    1   13   10]
 [   0    3    5    8    0    6    1    4    4    0    1   71  820    1    1   11    4   27    6    9   13]
 [   0    2    0    1    6   19    1    0   21   24   10   13    4  865    6    0    2    5    2   11    9]
 [  10    3    3   13    4    2    1    0   29    8    4    0    2    7  987    0    1    1   14    0    9]
 [   0    1    3    1    7    0    6    2    0    1    1   12    6    1    2  977   16   16    1    3   10]
 [   3   12    6    1    8    4    1    2    6    3    4    7    3    3    5   11  972    2    3    6   10]
 [   1    1    0    5    2    1    2    1    0    1    0   20   23    4    3   10    2  913    3    4    9]
 [   3    8    6   14    5    3    1   20   11    1    3    1    4    1   28    1    2    1  935    1    9]
 [   2    4    3    0    1   11   16    9    1    1    1   29    5    4    0    7    8    5    6  963   12]
 [ 246  286  293  144  293  203  136  201  165  140  206  217  402  322  305  170  394  105  245  336 9123]]

2024-06-06 03:21:24,034 - ==> Best [Top1: 78.731   Top5: 96.287   Sparsity:0.00   Params: 169472 on epoch: 191]
2024-06-06 03:21:24,034 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 03:21:24,041 - 

2024-06-06 03:21:24,041 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 03:21:29,843 - Epoch: [198][  100/ 1218]    Overall Loss 0.477619    Objective Loss 0.477619                                        LR 0.000031    Time 0.057999    
2024-06-06 03:21:34,258 - Epoch: [198][  200/ 1218]    Overall Loss 0.482454    Objective Loss 0.482454                                        LR 0.000031    Time 0.051069    
2024-06-06 03:21:38,694 - Epoch: [198][  300/ 1218]    Overall Loss 0.480832    Objective Loss 0.480832                                        LR 0.000031    Time 0.048827    
2024-06-06 03:21:43,116 - Epoch: [198][  400/ 1218]    Overall Loss 0.482222    Objective Loss 0.482222                                        LR 0.000031    Time 0.047673    
2024-06-06 03:21:47,533 - Epoch: [198][  500/ 1218]    Overall Loss 0.481022    Objective Loss 0.481022                                        LR 0.000031    Time 0.046968    
2024-06-06 03:21:51,941 - Epoch: [198][  600/ 1218]    Overall Loss 0.481567    Objective Loss 0.481567                                        LR 0.000031    Time 0.046486    
2024-06-06 03:21:56,381 - Epoch: [198][  700/ 1218]    Overall Loss 0.480339    Objective Loss 0.480339                                        LR 0.000031    Time 0.046185    
2024-06-06 03:22:00,793 - Epoch: [198][  800/ 1218]    Overall Loss 0.479994    Objective Loss 0.479994                                        LR 0.000031    Time 0.045926    
2024-06-06 03:22:05,197 - Epoch: [198][  900/ 1218]    Overall Loss 0.480154    Objective Loss 0.480154                                        LR 0.000031    Time 0.045714    
2024-06-06 03:22:09,592 - Epoch: [198][ 1000/ 1218]    Overall Loss 0.480443    Objective Loss 0.480443                                        LR 0.000031    Time 0.045537    
2024-06-06 03:22:14,011 - Epoch: [198][ 1100/ 1218]    Overall Loss 0.480951    Objective Loss 0.480951                                        LR 0.000031    Time 0.045413    
2024-06-06 03:22:18,448 - Epoch: [198][ 1200/ 1218]    Overall Loss 0.481095    Objective Loss 0.481095                                        LR 0.000031    Time 0.045325    
2024-06-06 03:22:19,192 - Epoch: [198][ 1218/ 1218]    Overall Loss 0.481365    Objective Loss 0.481365    Top1 82.396088    Top5 97.066015    LR 0.000031    Time 0.045265    
2024-06-06 03:22:19,367 - --- validate (epoch=198)-----------
2024-06-06 03:22:19,367 - 34633 samples (256 per mini-batch)
2024-06-06 03:22:24,620 - Epoch: [198][  100/  136]    Loss 0.433681    Top1 79.097656    Top5 96.429688    
2024-06-06 03:22:26,231 - Epoch: [198][  136/  136]    Loss 0.445406    Top1 78.999798    Top5 96.384951    
2024-06-06 03:22:26,415 - ==> Top1: 79.000    Top5: 96.385    Loss: 0.445

2024-06-06 03:22:26,416 - ==> Confusion:
[[ 830    2    0    0    4    0    1    2    8   55    0    2    4    6    3    1    2    1    2    1    7]
 [   4  927    2    3   22   26    5   13    4    2    4    2    4    0    7    3   10    2   13    2    8]
 [  11    2  862    5    5    0   28    7    0    6    6    1    3    6    2    8    1    0    4    3   10]
 [   4    2   13  878    3    7    6    6    0    1   22    3    6    3   24    1    5   11   12    3    6]
 [  30   12    1    3  944   11    1    0    1   11    2    3    2    2    8    6    7    0    7    0    3]
 [   7   25    5    5    9  867    3   34    5    3    0   19    5   19    0    5    6    4    4   11    7]
 [   1    8   26    3    1    3  985    5    1    1    5    4    1    1    1   14    5    2    0    8   11]
 [   5   11   10    4    4   42    5  885    9    2    7   14    4    4    2    3    1    3   34   17   11]
 [  14    3    0    0    1    1    1    0  854   55   14    2    3   17   19    0    2    2    7    1    6]
 [  84    0    3    0    2    3    0    2   54  813    1    2    4   15    3    0    1    2    2    0   10]
 [   1    4    7   12    1    5    8    4   19    1  951    2    2   17    7    2    0    0   10    3    8]
 [   2    3    1    0    1    8    2    4    3    0    1  887   37    9    2    8    3   19    0   14    7]
 [   1    1    4    5    1    4    0    6    4    0    2   67  829    3    2    9    3   31    5    7   11]
 [   3    1    3    0    8    7    0    4   13   18    6   14    4  879    4    4    4    3    2    8   16]
 [  10    2    2   11   11    3    0    2   34    8    9    2    5    5  968    1    1    3   12    0    9]
 [   4    0    4    1    5    1    5    1    1    2    0   18   14    2    0  979   11    9    1    2    6]
 [   5    6    1    0    5    6    0    3    8    1    2    6    4    5    4   11  979    3    3    4   16]
 [   0    1    2    4    1    0    0    3    4    3    0   30   26    6    2   11    3  896    1    4    8]
 [   4   10    7   14    4    2    2   21    9    1   11    3    5    2   19    1    2    5  923    2   11]
 [   1    8    4    0    2   10   13   10    0    0    0   29    9    8    0   10    8    2    3  961   10]
 [ 254  252  261  138  259  236  119  224  163  148  204  224  389  325  264  198  340  138  212  320 9264]]

2024-06-06 03:22:26,418 - ==> Best [Top1: 79.000   Top5: 96.385   Sparsity:0.00   Params: 169472 on epoch: 198]
2024-06-06 03:22:26,418 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 03:22:26,432 - 

2024-06-06 03:22:26,433 - Training epoch: 311705 samples (256 per mini-batch)
2024-06-06 03:22:32,122 - Epoch: [199][  100/ 1218]    Overall Loss 0.483069    Objective Loss 0.483069                                        LR 0.000031    Time 0.056876    
2024-06-06 03:22:36,527 - Epoch: [199][  200/ 1218]    Overall Loss 0.479051    Objective Loss 0.479051                                        LR 0.000031    Time 0.050456    
2024-06-06 03:22:40,922 - Epoch: [199][  300/ 1218]    Overall Loss 0.475935    Objective Loss 0.475935                                        LR 0.000031    Time 0.048284    
2024-06-06 03:22:45,315 - Epoch: [199][  400/ 1218]    Overall Loss 0.476187    Objective Loss 0.476187                                        LR 0.000031    Time 0.047191    
2024-06-06 03:22:49,715 - Epoch: [199][  500/ 1218]    Overall Loss 0.477411    Objective Loss 0.477411                                        LR 0.000031    Time 0.046550    
2024-06-06 03:22:54,121 - Epoch: [199][  600/ 1218]    Overall Loss 0.475583    Objective Loss 0.475583                                        LR 0.000031    Time 0.046133    
2024-06-06 03:22:58,524 - Epoch: [199][  700/ 1218]    Overall Loss 0.475075    Objective Loss 0.475075                                        LR 0.000031    Time 0.045831    
2024-06-06 03:23:02,942 - Epoch: [199][  800/ 1218]    Overall Loss 0.475924    Objective Loss 0.475924                                        LR 0.000031    Time 0.045622    
2024-06-06 03:23:07,512 - Epoch: [199][  900/ 1218]    Overall Loss 0.476934    Objective Loss 0.476934                                        LR 0.000031    Time 0.045622    
2024-06-06 03:23:11,902 - Epoch: [199][ 1000/ 1218]    Overall Loss 0.477080    Objective Loss 0.477080                                        LR 0.000031    Time 0.045449    
2024-06-06 03:23:16,304 - Epoch: [199][ 1100/ 1218]    Overall Loss 0.478309    Objective Loss 0.478309                                        LR 0.000031    Time 0.045318    
2024-06-06 03:23:20,696 - Epoch: [199][ 1200/ 1218]    Overall Loss 0.477794    Objective Loss 0.477794                                        LR 0.000031    Time 0.045200    
2024-06-06 03:23:21,439 - Epoch: [199][ 1218/ 1218]    Overall Loss 0.477785    Objective Loss 0.477785    Top1 76.283619    Top5 96.088020    LR 0.000031    Time 0.045142    
2024-06-06 03:23:21,609 - --- validate (epoch=199)-----------
2024-06-06 03:23:21,609 - 34633 samples (256 per mini-batch)
2024-06-06 03:23:26,903 - Epoch: [199][  100/  136]    Loss 0.440208    Top1 78.792969    Top5 96.621094    
2024-06-06 03:23:28,514 - Epoch: [199][  136/  136]    Loss 0.445801    Top1 78.572460    Top5 96.529322    
2024-06-06 03:23:28,699 - ==> Top1: 78.572    Top5: 96.529    Loss: 0.446

2024-06-06 03:23:28,700 - ==> Confusion:
[[ 803    2    7    0    6    4    0    2    8   68    1    2    1    4    4    3    4    1    1    3    7]
 [   1  950    2    0   27   19    1   16    3    2    2    1    2    1    5    1    5    2   16    0    7]
 [   8    2  841    8    5    4   23   10    0    9   10    5    2    6    4    6    4    0    4    4   15]
 [   2    4   16  867    7    8    3    7    4    1   20    1    8    1   31    3    2    4   14    3   10]
 [  16   11    3    2  958    6    3    3    2   14    0    4    1    6    7    4    4    0    2    2    6]
 [   2   21    1    2   19  875    1   38    2    7    4   13    9    9    4    2    7    2    6    9   10]
 [   2    9   19    1    3    2 1005    6    1    0    4    2    3    1    2    1    1    5    4    9    6]
 [   3   11   16    3    2   36   11  902    1    2    4   12    5    1    6    0    3    1   32   17    9]
 [  13    8    1    0    0    1    0    0  814   70   20    5    4   16   27    0    3    4    7    1    8]
 [  81    0    0    0   12    6    1    1   41  817    0    1    2   15   10    0    3    1    4    0    6]
 [   0    6    5   16    0    3    5    9   18    6  952    0    1    9   13    0    0    0   14    1    6]
 [   2    4    3    0    2   13    4    4    1    4    2  871   22   11    1   14    8   21    2   14    8]
 [   2    2    3    2    1    8    2    8    1    0    0   74  818    1    1    1    4   35    8    8   16]
 [   4    4    2    1    8   10    0    3   18   21    8   12    5  861    4    2    4    6    4   11   13]
 [  12    2    5   15   14    1    1    1   25    9    6    0    5    6  963    1    5    3   12    1   11]
 [   1    2    4    1    3    1    8    0    1    0    0   20   10    0    0  978   13    9    2    6    7]
 [   4    9    3    2   10   13    2    1    7    1    3    6    1    5    3    7  976    1    0    7   11]
 [   1    1    1    0    0    3    2    2    1    2    1   21   24    7    4   14    4  905    2    3    7]
 [   0    8    9   10    2    4    0   24    8    0    8    2    3    0   16    0    1    1  953    2    7]
 [   0    6    2    0    1    9   20    8    0    0    2   27    7    9    2   10    7    3    6  960    9]
 [ 249  299  274  133  346  240  139  211  123  156  189  198  355  322  269  198  351  137  227  373 9143]]

2024-06-06 03:23:28,701 - ==> Best [Top1: 79.000   Top5: 96.385   Sparsity:0.00   Params: 169472 on epoch: 198]
2024-06-06 03:23:28,702 - Saving checkpoint to: logs/v3_original/2024.06.05-233420/qat_checkpoint.pth.tar
2024-06-06 03:23:28,709 - --- test ---------------------
2024-06-06 03:23:28,709 - 11005 samples (256 per mini-batch)
2024-06-06 03:23:30,028 - Test: [   43/   43]    Loss 0.201460    Top1 88.977737    Top5 98.736938    
2024-06-06 03:23:30,207 - ==> Top1: 88.978    Top5: 98.737    Loss: 0.201

2024-06-06 03:23:30,208 - ==> Confusion:
[[ 390    1    0    0    4    0    0    0    1   21    0    1    1    1    3    2    0    0    0    0    0]
 [   0  370    1    0    4   13    0    7    0    0    0    1    0    0    0    0    1    0    5    2    2]
 [   0    0  402    0    0    0    5    0    0    1    0    0    1    0    0    0    1    0    1    0    1]
 [   0    0    8  363    1    0    0    1    0    1    1    1    3    0   14    0    0    0    1    0    2]
 [   0    2    0    0  400    1    0    0    0    2    0    0    1    0    0    1    4    0    0    0    0]
 [   2    5    2    0    0  371    0   16    1    0    0    2    1    1    0    0    0    0    1    0    0]
 [   0    0    0    0    0    0  413    1    0    0    0    0    0    0    0    0    0    0    0    2    3]
 [   0    4    7    1    1    4    1  381    0    2    0    0    0    0    0    0    0    0    1    2    1]
 [   4    0    0    0    0    3    0    0  361   19    6    0    0    1    1    0    1    0    0    0    0]
 [  49    0    0    0    2    1    1    0    6  338    0    0    0    2    1    0    0    1    0    0    1]
 [   1    1    0    1    0    0    0    1    3    2  383    0    0    1    1    0    0    0    4    0    1]
 [   0    0    0    0    3    5    0    1    0    1    0  401    3    5    0    0    1    2    0    1    1]
 [   0    0    0    1    0    0    0    1    0    0    0    7  379    0    2    3    1    8    0    1    2]
 [   0    1    1    0    2    5    0    0    0   12    0    2    0  372    2    0    1    0    0    1    1]
 [   0    0    0    1    2    0    0    0    5    1    0    0    1    1  430    0    0    0    0    0    4]
 [   0    0    0    0    0    1    0    0    1    0    0    2    1    0    0  382    2    2    0    1    2]
 [   0    1    0    1    0    0    0    0    0    0    0    0    0    1    0    3  397    1    0    2    0]
 [   0    1    0    1    0    1    2    1    0    0    1    2    4    0    0    1    0  391    0    0    3]
 [   1    1    2    6    1    0    0    6    0    0    0    0    0    0    2    0    0    0  389    0    0]
 [   0    1    0    0    0    1    1    2    0    0    0    7    1    1    0    1    3    1    0  397    2]
 [  22   49   63   14   33   52   13   41   19   15   26   29   84   95   26   10   39   24   32   56 2082]]

2024-06-06 03:23:30,456 - 
2024-06-06 03:23:30,456 - Log file for this run: /home/merveeyuboglu/Github/ai8x-training-merve/ai8x-training/logs/v3_original/2024.06.05-233420/2024.06.05-233420.log
